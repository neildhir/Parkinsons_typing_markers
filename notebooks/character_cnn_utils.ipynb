{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "# This blokc is important if we want the memory to grow on the GPU, and not block allocate the whole thing\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.utils import class_weight \n",
    "from numpy import vstack, asarray\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparam opt\n",
    "import talos as ta\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# Set path to find modelling tools for later use\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "# Global params live here\n",
    "import haberrspd.charCNN.globals\n",
    "from haberrspd.charCNN.models_tf import char_cnn_model_talos\n",
    "from haberrspd.charCNN.data_utilities import create_training_data_keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = 'squared_hinge'\n",
    "\n",
    "if loss_func == 'hinge' or loss_func == 'squared_hinge':\n",
    "    y_train = [-1 if x==0 else x for x in y_train]\n",
    "    y_test = [-1 if x==0 else x for x in y_test]\n",
    "    \n",
    "if loss_func == 'binary_crossentropy':\n",
    "    # Check if label-space is correct\n",
    "    if (-1 in y_train) or (-1 in y_test):\n",
    "        y_train = [0 if x==-1 else x for x in y_train]\n",
    "        y_test = [0 if x==-1 else x for x in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(loss=loss_func,  # TODO: change to cosine loss, cosine_proximity, binary_crossentropy\n",
    "              optimizer='adam',            # TODO: check which is most appropriate\n",
    "              metrics=['accuracy'])        # Probs other options here which are more useful\n",
    "\n",
    "# Check if checkpoints dir exists, if not make it\n",
    "if not os.path.exists('../../keras_checkpoints'):\n",
    "    os.makedirs('../../keras_checkpoints')\n",
    "\n",
    "# Callbacks\n",
    "file_name = \"char-CNN\"\n",
    "check_cb = ModelCheckpoint(file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0,\n",
    "                           save_best_only=True,\n",
    "                           mode='min')\n",
    "\n",
    "earlystop_cb = EarlyStopping(monitor='val_loss',\n",
    "                             patience=7,\n",
    "                             verbose=0,\n",
    "                             mode='auto')\n",
    "\n",
    "# history = LossHistory()\n",
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "-Add class-weight option to take into account class-imbalance on patients and controls\n",
    "\"\"\"\n",
    "fit_hist = model.fit(X_train,\n",
    "                     y_train,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     verbose=0, # Set to zero if using live plotting of losses\n",
    "                     class_weight = class_weights,\n",
    "                     batch_size=128,\n",
    "                     epochs=40,\n",
    "                     #shuffle=True, # Our data is already shuffled during data loading\n",
    "                     callbacks=[\n",
    "                                #check_cb,\n",
    "                                #tensorboard_callback,\n",
    "                                PlotLossesCallback(),\n",
    "                                #earlystop_cb\n",
    "                               ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALOS: hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.charCNN.data_utilities import (create_training_data_keras,\n",
    "                                              create_data_objects,\n",
    "                                              english_language_qwerty_keyboard,\n",
    "                                              us_keyboard_keys_to_2d_coordinates_mrc,\n",
    "                                             )\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from collections import defaultdict\n",
    "\n",
    "import math\n",
    "def roundup(x):\n",
    "    return int(math.ceil(x / 100.0)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters used in all typed sentences: 54\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\"\n",
    "X_train, X_test, y_train, y_test, max_sentence_length, alphabet_size = create_training_data_keras(\n",
    "    DATA_ROOT, \"char_time_space\", \"SpanishData-preprocessed.csv\")\n",
    "\n",
    "# Class weights are dynamic as the data-loader is stochastic and changes with each run.\n",
    "class_weights = dict(zip([0, 1], class_weight.compute_class_weight(\"balanced\", list(set(y_train)), y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 11100, 56)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4199"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params = {\n",
    "    \"lr\": (0.1, 10, 5),  # This is a range not a tuple\n",
    "    \"conv_output_space\": [8, 16, 32],  # ,8],\n",
    "    \"number_of_large_filters\": [1, 2, 4],\n",
    "    \"number_of_small_filters\": [1, 2, 4],\n",
    "    \"large_filter_length\": [8, 16, 32],  # When time is included [20,40,80,160], when not: [10,20,40,80]\n",
    "    \"small_filter_length\": [2, 4, 8],  # [5, 10, 20],\n",
    "    \"pool_length\": [2, 4],\n",
    "    \"dense_units_layer_3\": [32, 64],\n",
    "    \"dense_units_layer_2\": [16, 32],\n",
    "    \"batch_size\": [16, 32, 64],\n",
    "    \"epochs\": [100, 200],\n",
    "    \"dropout\": (0, 0.5, 5),\n",
    "    \"conv_padding\": [\"same\"],\n",
    "    \"conv_kernel_initializer\": [\"uniform\"],\n",
    "    \"conv_bias_initializer\": [\"uniform\"],\n",
    "    \"dense_kernel_initializer\": [\"uniform\"],\n",
    "    \"dense_bias_initializer\": [\"uniform\"],\n",
    "    \"optimizer\": [Adam, Nadam],  # If used this way, these have to explicitly imported\n",
    "    \"loss\": [\"logcosh\", \"binary_crossentropy\"],  # Loss functions\n",
    "    \"conv_activation\": [\"relu\"],\n",
    "    \"dense_activation\": [\"relu\"],\n",
    "    \"last_activation\": [\"sigmoid\"],\n",
    "    # Stationary parameters, i.e. do not get optimised\n",
    "    \"max_sentence_length\": [max_sentence_length],\n",
    "    \"alphabet_size\": [alphabet_size],\n",
    "    \"control_class_weight\": [class_weights[0]],\n",
    "    \"pd_class_weight\": [class_weights[1]],\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "'sgd': SGD,\n",
    "'rmsprop': RMSprop,\n",
    "'adagrad': Adagrad,\n",
    "'adadelta': Adadelta,\n",
    "'adam': Adam,\n",
    "'adamax': Adamax,\n",
    "'nadam': Nadam\n",
    "\"\"\"\n",
    "\n",
    "def size_of_optimisation_space(params):\n",
    "    space = 1\n",
    "    for attribute in params.keys():\n",
    "        if type(attribute) == tuple:\n",
    "            space*=params[attribute][-1]\n",
    "        else:\n",
    "            space*=len(params[attribute])\n",
    "        \n",
    "    return space\n",
    "\n",
    "int(size_of_optimisation_space(opt_params) * 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 144 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.6934 - acc: 0.6372 - val_loss: 0.6892 - val_acc: 0.6389\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.6932 - acc: 0.6372 - val_loss: 0.6894 - val_acc: 0.6389\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6934 - acc: 0.6372 - val_loss: 0.6905 - val_acc: 0.6389\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6932 - acc: 0.6372 - val_loss: 0.6904 - val_acc: 0.6389\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.6933 - acc: 0.6372 - val_loss: 0.6911 - val_acc: 0.6389\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6932 - acc: 0.6372 - val_loss: 0.6907 - val_acc: 0.6389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 10%|█         | 1/10 [00:02<00:19,  2.13s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 144 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.6946 - acc: 0.3628 - val_loss: 0.6950 - val_acc: 0.3611\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.6934 - acc: 0.3628 - val_loss: 0.6942 - val_acc: 0.3611\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.6933 - acc: 0.3837 - val_loss: 0.6936 - val_acc: 0.3611\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.6934 - acc: 0.4465 - val_loss: 0.6940 - val_acc: 0.3611\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.6936 - acc: 0.3512 - val_loss: 0.6940 - val_acc: 0.3611\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.6934 - acc: 0.3628 - val_loss: 0.6940 - val_acc: 0.3611\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.6934 - acc: 0.3581 - val_loss: 0.6934 - val_acc: 0.3611\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.6933 - acc: 0.5186 - val_loss: 0.6937 - val_acc: 0.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 20%|██        | 2/10 [00:05<00:20,  2.58s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 144 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.6949 - acc: 0.6372 - val_loss: 0.6896 - val_acc: 0.6389\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.6938 - acc: 0.5767 - val_loss: 0.6928 - val_acc: 0.6389\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.6937 - acc: 0.6047 - val_loss: 0.6937 - val_acc: 0.3611\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.6940 - acc: 0.4419 - val_loss: 0.6933 - val_acc: 0.3611\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.6939 - acc: 0.3721 - val_loss: 0.6927 - val_acc: 0.6389\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.6941 - acc: 0.6000 - val_loss: 0.6924 - val_acc: 0.6389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 30%|███       | 3/10 [00:08<00:18,  2.57s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 144 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.1203 - acc: 0.3628 - val_loss: 0.1208 - val_acc: 0.3611\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.1202 - acc: 0.3628 - val_loss: 0.1212 - val_acc: 0.3611\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.1202 - acc: 0.3628 - val_loss: 0.1209 - val_acc: 0.3611\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.1201 - acc: 0.3628 - val_loss: 0.1208 - val_acc: 0.3611\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.1201 - acc: 0.3628 - val_loss: 0.1205 - val_acc: 0.3611\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.1201 - acc: 0.3628 - val_loss: 0.1205 - val_acc: 0.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 40%|████      | 4/10 [00:10<00:14,  2.41s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 144 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.6933 - acc: 0.3628 - val_loss: 0.6962 - val_acc: 0.3611\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.6933 - acc: 0.3628 - val_loss: 0.6957 - val_acc: 0.3611\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6932 - acc: 0.3884 - val_loss: 0.6935 - val_acc: 0.3611\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6939 - acc: 0.4419 - val_loss: 0.6937 - val_acc: 0.3611\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.6933 - acc: 0.3977 - val_loss: 0.6944 - val_acc: 0.3611\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6929 - acc: 0.4186 - val_loss: 0.6936 - val_acc: 0.3611\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.6930 - acc: 0.4256 - val_loss: 0.6936 - val_acc: 0.3611\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.6932 - acc: 0.4070 - val_loss: 0.6938 - val_acc: 0.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 50%|█████     | 5/10 [00:12<00:11,  2.29s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 144 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.6937 - acc: 0.4326 - val_loss: 0.6925 - val_acc: 0.6389\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.6933 - acc: 0.6395 - val_loss: 0.6926 - val_acc: 0.6389\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.6932 - acc: 0.4605 - val_loss: 0.6940 - val_acc: 0.3611\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.6934 - acc: 0.5000 - val_loss: 0.6916 - val_acc: 0.6389\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.6933 - acc: 0.5558 - val_loss: 0.6930 - val_acc: 0.6389\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.6934 - acc: 0.5884 - val_loss: 0.6910 - val_acc: 0.6389\n",
      "Epoch 7/200\n",
      " - 0s - loss: 0.6932 - acc: 0.5605 - val_loss: 0.6931 - val_acc: 0.6389\n",
      "Epoch 8/200\n",
      " - 0s - loss: 0.6934 - acc: 0.5953 - val_loss: 0.6916 - val_acc: 0.6389\n",
      "Epoch 9/200\n",
      " - 0s - loss: 0.6932 - acc: 0.5512 - val_loss: 0.6924 - val_acc: 0.6389\n",
      "Epoch 10/200\n",
      " - 0s - loss: 0.6934 - acc: 0.5279 - val_loss: 0.6921 - val_acc: 0.6389\n",
      "Epoch 11/200\n",
      " - 0s - loss: 0.6942 - acc: 0.3698 - val_loss: 0.6964 - val_acc: 0.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 60%|██████    | 6/10 [00:14<00:08,  2.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 144 samples\n",
      "Epoch 1/200\n",
      " - 0s - loss: 0.1202 - acc: 0.3628 - val_loss: 0.1209 - val_acc: 0.3611\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.1202 - acc: 0.3628 - val_loss: 0.1208 - val_acc: 0.3611\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.1202 - acc: 0.3628 - val_loss: 0.1207 - val_acc: 0.3611\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.1202 - acc: 0.3628 - val_loss: 0.1206 - val_acc: 0.3611\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.1201 - acc: 0.4512 - val_loss: 0.1204 - val_acc: 0.3611\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.1195 - acc: 0.4837 - val_loss: 0.1257 - val_acc: 0.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 70%|███████   | 7/10 [00:15<00:06,  2.01s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 144 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.6935 - acc: 0.6372 - val_loss: 0.6892 - val_acc: 0.6389\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.6934 - acc: 0.6372 - val_loss: 0.6900 - val_acc: 0.6389\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6934 - acc: 0.6372 - val_loss: 0.6913 - val_acc: 0.6389\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6934 - acc: 0.6372 - val_loss: 0.6918 - val_acc: 0.6389\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.6936 - acc: 0.5116 - val_loss: 0.6925 - val_acc: 0.6389\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6934 - acc: 0.6279 - val_loss: 0.6920 - val_acc: 0.6389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 80%|████████  | 8/10 [00:19<00:05,  2.51s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 144 samples\n",
      "Epoch 1/100\n",
      " - 1s - loss: 0.1202 - acc: 0.4791 - val_loss: 0.1202 - val_acc: 0.3611\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.1202 - acc: 0.5581 - val_loss: 0.1197 - val_acc: 0.6389\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.1201 - acc: 0.6372 - val_loss: 0.1199 - val_acc: 0.6389\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1201 - acc: 0.6372 - val_loss: 0.1200 - val_acc: 0.6389\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1201 - acc: 0.4558 - val_loss: 0.1202 - val_acc: 0.3611\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.1201 - acc: 0.3814 - val_loss: 0.1201 - val_acc: 0.6389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 90%|█████████ | 9/10 [00:21<00:02,  2.23s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 430 samples, validate on 144 samples\n",
      "Epoch 1/200\n",
      " - 1s - loss: 0.6935 - acc: 0.5349 - val_loss: 0.6922 - val_acc: 0.6389\n",
      "Epoch 2/200\n",
      " - 0s - loss: 0.6934 - acc: 0.5860 - val_loss: 0.6923 - val_acc: 0.6389\n",
      "Epoch 3/200\n",
      " - 0s - loss: 0.6934 - acc: 0.6372 - val_loss: 0.6924 - val_acc: 0.6389\n",
      "Epoch 4/200\n",
      " - 0s - loss: 0.6934 - acc: 0.4465 - val_loss: 0.6935 - val_acc: 0.3611\n",
      "Epoch 5/200\n",
      " - 0s - loss: 0.6935 - acc: 0.3860 - val_loss: 0.6940 - val_acc: 0.3611\n",
      "Epoch 6/200\n",
      " - 0s - loss: 0.6933 - acc: 0.3628 - val_loss: 0.6935 - val_acc: 0.3611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 10/10 [00:24<00:00,  2.47s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28 s, sys: 896 ms, total: 28.9 s\n",
      "Wall time: 24.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = ta.Scan(x=X_train,\n",
    "            y=asarray(y_train).reshape(-1, 1),\n",
    "            x_val=X_test, \n",
    "            y_val=asarray(y_test).reshape(-1, 1),\n",
    "            experiment_name='../results/test',\n",
    "            model=char_cnn_model_talos,\n",
    "            round_limit=10,\n",
    "#             fraction_limit=0.005,\n",
    "            disable_progress_bar=False,\n",
    "            params=opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights passed to activation function\n",
    "from talos import Deploy, Predict, Restore\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test:  144\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 "
     ]
    }
   ],
   "source": [
    "results = np.zeros((len(X_test),2))\n",
    "print(\"Size of test: \", len(X_test))\n",
    "for i,(y,x) in enumerate(zip(y_test, X_test)):\n",
    "    print(i, end=\" \")\n",
    "    results[i,:] = [y, float(p.predict(x[np.newaxis,:,:]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"foo-\" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + \".csv\", \n",
    "           results,fmt='%.15f', \n",
    "           delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy package test_deploy_MJFF_char_attempt_1 have been saved.\n",
      "data is not 2d, dummy data written instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<talos.commands.deploy.Deploy at 0x7f6c04e35b38>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Deploy(scan_object=t, model_name='test_deploy_MJFF_char_attempt_1',metric='val_acc',asc=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from talos.utils.load_model import load_model\n",
    "\n",
    "tada = load_model(\"test_deploy/test_deploy_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49789613]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tada.predict(x[np.newaxis,:,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
