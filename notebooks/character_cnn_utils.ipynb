{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "# This blokc is important if we want the memory to grow on the GPU, and not block allocate the whole thing\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.utils import class_weight \n",
    "from numpy import vstack, asarray\n",
    "from keras.optimizers import Adam, Nadam\n",
    "import pandas as pd\n",
    "\n",
    "# Hyperparam opt\n",
    "import talos as ta\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# Set path to find modelling tools for later use\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "# Global params live here\n",
    "import haberrspd.charCNN.globals\n",
    "from haberrspd.charCNN.models_tf import char_cnn_model_talos\n",
    "from haberrspd.charCNN.data_utilities import create_training_data_keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = 'squared_hinge'\n",
    "\n",
    "if loss_func == 'hinge' or loss_func == 'squared_hinge':\n",
    "    y_train = [-1 if x==0 else x for x in y_train]\n",
    "    y_test = [-1 if x==0 else x for x in y_test]\n",
    "    \n",
    "if loss_func == 'binary_crossentropy':\n",
    "    # Check if label-space is correct\n",
    "    if (-1 in y_train) or (-1 in y_test):\n",
    "        y_train = [0 if x==-1 else x for x in y_train]\n",
    "        y_test = [0 if x==-1 else x for x in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(loss=loss_func,  # TODO: change to cosine loss, cosine_proximity, binary_crossentropy\n",
    "              optimizer='adam',            # TODO: check which is most appropriate\n",
    "              metrics=['accuracy'])        # Probs other options here which are more useful\n",
    "\n",
    "# Check if checkpoints dir exists, if not make it\n",
    "if not os.path.exists('../../keras_checkpoints'):\n",
    "    os.makedirs('../../keras_checkpoints')\n",
    "\n",
    "# Callbacks\n",
    "file_name = \"char-CNN\"\n",
    "check_cb = ModelCheckpoint(file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0,\n",
    "                           save_best_only=True,\n",
    "                           mode='min')\n",
    "\n",
    "earlystop_cb = EarlyStopping(monitor='val_loss',\n",
    "                             patience=7,\n",
    "                             verbose=0,\n",
    "                             mode='auto')\n",
    "\n",
    "# history = LossHistory()\n",
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "-Add class-weight option to take into account class-imbalance on patients and controls\n",
    "\"\"\"\n",
    "fit_hist = model.fit(X_train,\n",
    "                     y_train,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     verbose=0, # Set to zero if using live plotting of losses\n",
    "                     class_weight = class_weights,\n",
    "                     batch_size=128,\n",
    "                     epochs=40,\n",
    "                     #shuffle=True, # Our data is already shuffled during data loading\n",
    "                     callbacks=[\n",
    "                                #check_cb,\n",
    "                                #tensorboard_callback,\n",
    "                                PlotLossesCallback(),\n",
    "                                #earlystop_cb\n",
    "                               ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALOS: hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.charCNN.data_utilities import (us_keyboard_keys_to_2d_coordinates_mrc, \n",
    "                                              english_language_qwerty_keyboard, \n",
    "                                              uk_standard_layout_keyboard, \n",
    "                                              uk_keyboard_keys_to_2d_coordinates_mjff,\n",
    "                                              uk_keyboard_keys_to_2d_coordinates_mjff_old)\n",
    "from haberrspd.preprocess import modifier_key_replacements\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MJFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboard_lower, keyboard_upper = english_language_qwerty_keyboard(layout=\"uk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = 'i am a little pony ^=\"ω\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  8],\n",
       "       [ 4,  3],\n",
       "       [ 2,  1],\n",
       "       [ 3,  7],\n",
       "       [ 4,  3],\n",
       "       [ 2,  1],\n",
       "       [ 4,  3],\n",
       "       [ 2,  9],\n",
       "       [ 1,  8],\n",
       "       [ 1,  5],\n",
       "       [ 1,  5],\n",
       "       [ 2,  9],\n",
       "       [ 1,  3],\n",
       "       [ 4,  3],\n",
       "       [ 1, 10],\n",
       "       [ 1,  9],\n",
       "       [ 3,  6],\n",
       "       [ 1,  6],\n",
       "       [ 4,  3],\n",
       "       [ 5,  6]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_keyboard_keys_to_2d_coordinates_mjff_old(test_sent,keyboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  8],\n",
       "       [ 4,  3],\n",
       "       [ 2,  1],\n",
       "       [ 3,  7],\n",
       "       [ 4,  3],\n",
       "       [ 2,  1],\n",
       "       [ 4,  3],\n",
       "       [ 2,  9],\n",
       "       [ 1,  8],\n",
       "       [ 1,  5],\n",
       "       [ 1,  5],\n",
       "       [ 2,  9],\n",
       "       [ 1,  3],\n",
       "       [ 4,  3],\n",
       "       [ 1, 10],\n",
       "       [ 1,  9],\n",
       "       [ 3,  6],\n",
       "       [ 1,  6],\n",
       "       [ 4,  3],\n",
       "       [ 0,  6],\n",
       "       [ 0, 12],\n",
       "       [ 0,  2],\n",
       "       [ 3,  7],\n",
       "       [ 0,  2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_keyboard_keys_to_2d_coordinates_mjff(test_sent,keyboard_lower,keyboard_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(Path(\"../data/MRC\") / 'processed_mrc_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>type</th>\n",
       "      <th>location</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>medication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>β</td>\n",
       "      <td>keydown</td>\n",
       "      <td>1</td>\n",
       "      <td>25885.055</td>\n",
       "      <td>1010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h</td>\n",
       "      <td>keydown</td>\n",
       "      <td>0</td>\n",
       "      <td>26086.840</td>\n",
       "      <td>1010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>β</td>\n",
       "      <td>keyup</td>\n",
       "      <td>1</td>\n",
       "      <td>26181.975</td>\n",
       "      <td>1010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h</td>\n",
       "      <td>keyup</td>\n",
       "      <td>0</td>\n",
       "      <td>26193.745</td>\n",
       "      <td>1010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>o</td>\n",
       "      <td>keydown</td>\n",
       "      <td>0</td>\n",
       "      <td>26321.480</td>\n",
       "      <td>1010</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  key     type  location  timestamp  participant_id  sentence_id  diagnosis  \\\n",
       "0   β  keydown         1  25885.055            1010            1          0   \n",
       "1   h  keydown         0  26086.840            1010            1          0   \n",
       "2   β    keyup         1  26181.975            1010            1          0   \n",
       "3   h    keyup         0  26193.745            1010            1          0   \n",
       "4   o  keydown         0  26321.480            1010            1          0   \n",
       "\n",
       "   medication  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = df[(df.participant_id == 1010) & (df.sentence_id == 1)].location.values\n",
    "sent = df[(df.participant_id == 1010) & (df.sentence_id == 1)].key.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(itertools.chain.from_iterable([[i]*int(n) for i,n in zip(locs[:5][:-1],[4,3,2,2,4][1:])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = us_keyboard_keys_to_2d_coordinates_mrc(sent,locs,us_lower,us_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\"\n",
    "X_train, X_test, y_train, y_test, max_sentence_length, alphabet_size = create_training_data_keras(\n",
    "    DATA_ROOT, \"char\", \"EnglishData-preprocessed_attempt_1.csv\")\n",
    "\n",
    "# Class weights are dynamic as the data-loader is stochastic and changes with each run.\n",
    "class_weights = dict(zip([0, 1], class_weight.compute_class_weight(\"balanced\", list(set(y_train)), y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 49\n",
      "{0: 0.7840059790732437, 1: 1.3802631578947369}\n"
     ]
    }
   ],
   "source": [
    "params ={\n",
    "         # Learning rate\n",
    "         'lr': (0.1, 10, 5),\n",
    "         'conv_output_space' : [4],#,8],\n",
    "         'number_of_large_filters' : [2],             \n",
    "         'number_of_small_filters' : [2],\n",
    "         'large_filter_length' : [60],\n",
    "         'small_filter_length' : [5],\n",
    "         'pool_length' : [2,4,8],\n",
    "         'dense_units_layer_3' : [32],\n",
    "         'dense_units_layer_2' : [16],\n",
    "         'batch_size': [32],\n",
    "         'epochs': [100],\n",
    "         'dropout': [0.05,0.1,0.2],\n",
    "         'conv_kernel_initializer': ['uniform'],\n",
    "         'conv_bias_initializer': ['uniform'],\n",
    "         'dense_kernel_initializer': ['uniform'],\n",
    "         'dense_bias_initializer': ['uniform'],\n",
    "         'optimizer': [Adam],\n",
    "         'loss': ['binary_crossentropy'],\n",
    "         'conv_activation':['relu'],\n",
    "         'dense_activation':['relu'],\n",
    "         'last_activation': ['sigmoid'],\n",
    "         # Stationary parameters, i.e. do not get optimised\n",
    "         'max_sentence_length':[max_sentence_length],\n",
    "         'alphabet_size':[alphabet_size],\n",
    "         'control_class_weight' : [class_weights[0]],\n",
    "         'pd_class_weight' : [class_weights[1]],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params ={\n",
    "            'lr': (0.1, 10, 2),  # Learning rate\n",
    "            'conv_output_space': [4, 8, 16],  # ,8],\n",
    "            'number_of_large_filters': [2],\n",
    "            'number_of_small_filters': [2],\n",
    "            'large_filter_length': [20, 40, 80, 160],\n",
    "            'small_filter_length': [5, 10, 20],\n",
    "            'pool_length': [2],\n",
    "            'dense_units_layer_3': [32],\n",
    "            'dense_units_layer_2': [16],\n",
    "            'batch_size': [16, 32, 64],\n",
    "            'epochs': [200],\n",
    "            'dropout': (0, 0.5, 2),\n",
    "            'conv_kernel_initializer': ['uniform'],\n",
    "            'conv_bias_initializer': ['uniform'],\n",
    "            'dense_kernel_initializer': ['uniform'],\n",
    "            'dense_bias_initializer': ['uniform'],\n",
    "            'optimizer': [Nadam],  # If used this way, these have to explicitly imported\n",
    "            'loss': ['binary_crossentropy'],  # Loss functions\n",
    "            'conv_activation': ['relu'],\n",
    "            'dense_activation': ['relu'],\n",
    "            'last_activation': ['sigmoid'],\n",
    "            # Stationary parameters, i.e. do not get optimised\n",
    "            'max_sentence_length': [max_sentence_length],\n",
    "            'alphabet_size': [alphabet_size],\n",
    "            'control_class_weight': [class_weights[0]],\n",
    "            'pd_class_weight': [class_weights[1]],\n",
    "            }\n",
    "\n",
    "\"\"\"\n",
    "'sgd': SGD,\n",
    "'rmsprop': RMSprop,\n",
    "'adagrad': Adagrad,\n",
    "'adadelta': Adadelta,\n",
    "'adam': Adam,\n",
    "'adamax': Adamax,\n",
    "'nadam': Nadam\n",
    "\"\"\"\n",
    "\n",
    "def size_of_optimisation_space(params):\n",
    "    space = 1\n",
    "    for attribute in params.keys():\n",
    "        if type(attribute) == tuple:\n",
    "            space*=params[attribute][-1]\n",
    "        else:\n",
    "            space*=len(params[attribute])\n",
    "        \n",
    "    return space\n",
    "\n",
    "int(size_of_optimisation_space(opt_params) * 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "t = 2\n",
    "if t:\n",
    "    print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1049 samples, validate on 117 samples\n",
      "Epoch 1/200\n",
      " - 7s - loss: 0.6881 - acc: 0.6120 - val_loss: 0.6766 - val_acc: 0.6410\n",
      "Epoch 2/200\n",
      " - 5s - loss: 0.6560 - acc: 0.6711 - val_loss: 0.7163 - val_acc: 0.7094\n",
      "Epoch 3/200\n",
      " - 5s - loss: 0.6085 - acc: 0.7064 - val_loss: 0.6103 - val_acc: 0.7179\n",
      "Epoch 4/200\n",
      " - 5s - loss: 0.5710 - acc: 0.7302 - val_loss: 0.6210 - val_acc: 0.7009\n",
      "Epoch 5/200\n",
      " - 5s - loss: 0.5385 - acc: 0.7455 - val_loss: 0.6034 - val_acc: 0.7179\n",
      "Epoch 6/200\n",
      " - 5s - loss: 0.6143 - acc: 0.6864 - val_loss: 0.6343 - val_acc: 0.6068\n",
      "Epoch 7/200\n",
      " - 5s - loss: 0.6177 - acc: 0.6540 - val_loss: 0.6067 - val_acc: 0.6752\n",
      "Epoch 8/200\n",
      " - 5s - loss: 0.5405 - acc: 0.7140 - val_loss: 0.6137 - val_acc: 0.6752\n",
      "Epoch 9/200\n",
      " - 5s - loss: 0.5028 - acc: 0.7331 - val_loss: 0.7406 - val_acc: 0.5470\n",
      "Epoch 10/200\n",
      " - 5s - loss: 0.4565 - acc: 0.7255 - val_loss: 0.6977 - val_acc: 0.5812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [00:51<03:26, 51.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1049 samples, validate on 117 samples\n",
      "Epoch 1/200\n",
      " - 5s - loss: 0.7527 - acc: 0.3966 - val_loss: 0.6946 - val_acc: 0.4359\n",
      "Epoch 2/200\n",
      " - 4s - loss: 0.7025 - acc: 0.3661 - val_loss: 0.6948 - val_acc: 0.3932\n",
      "Epoch 3/200\n",
      " - 4s - loss: 0.6934 - acc: 0.3622 - val_loss: 0.6936 - val_acc: 0.3932\n",
      "Epoch 4/200\n",
      " - 4s - loss: 0.6934 - acc: 0.3689 - val_loss: 0.6925 - val_acc: 0.6068\n",
      "Epoch 5/200\n",
      " - 4s - loss: 0.6933 - acc: 0.6378 - val_loss: 0.6919 - val_acc: 0.6068\n",
      "Epoch 6/200\n",
      " - 4s - loss: 0.6935 - acc: 0.6225 - val_loss: 0.6929 - val_acc: 0.6068\n",
      "Epoch 7/200\n",
      " - 4s - loss: 0.6935 - acc: 0.5929 - val_loss: 0.6938 - val_acc: 0.3932\n",
      "Epoch 8/200\n",
      " - 4s - loss: 0.6934 - acc: 0.3966 - val_loss: 0.6939 - val_acc: 0.3932\n",
      "Epoch 9/200\n",
      " - 4s - loss: 0.6934 - acc: 0.4395 - val_loss: 0.6935 - val_acc: 0.3932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 40%|████      | 2/5 [01:25<02:19, 46.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1049 samples, validate on 117 samples\n",
      "Epoch 1/200\n",
      " - 5s - loss: 0.7001 - acc: 0.4471 - val_loss: 0.6915 - val_acc: 0.6068\n",
      "Epoch 2/200\n",
      " - 4s - loss: 0.6938 - acc: 0.4948 - val_loss: 0.6928 - val_acc: 0.6068\n",
      "Epoch 3/200\n",
      " - 4s - loss: 0.6936 - acc: 0.4671 - val_loss: 0.6927 - val_acc: 0.6068\n",
      "Epoch 4/200\n",
      " - 4s - loss: 0.6935 - acc: 0.6263 - val_loss: 0.6931 - val_acc: 0.6068\n",
      "Epoch 5/200\n",
      " - 4s - loss: 0.6934 - acc: 0.3699 - val_loss: 0.6930 - val_acc: 0.6068\n",
      "Epoch 6/200\n",
      " - 4s - loss: 0.6934 - acc: 0.5510 - val_loss: 0.6932 - val_acc: 0.3932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 60%|██████    | 3/5 [01:51<01:20, 40.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/neil/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 1049 samples, validate on 117 samples\n",
      "Epoch 1/200\n",
      " - 5s - loss: 0.7475 - acc: 0.5510 - val_loss: 0.6910 - val_acc: 0.6068\n",
      "Epoch 2/200\n",
      " - 4s - loss: 0.6955 - acc: 0.5071 - val_loss: 0.6932 - val_acc: 0.3932\n",
      "Epoch 3/200\n",
      " - 4s - loss: 0.6934 - acc: 0.4280 - val_loss: 0.6931 - val_acc: 0.6154\n",
      "Epoch 4/200\n",
      " - 4s - loss: 0.6934 - acc: 0.3937 - val_loss: 0.6915 - val_acc: 0.6068\n",
      "Epoch 5/200\n",
      " - 4s - loss: 0.6935 - acc: 0.5300 - val_loss: 0.6907 - val_acc: 0.6068\n",
      "Epoch 6/200\n",
      " - 4s - loss: 0.6935 - acc: 0.6282 - val_loss: 0.6925 - val_acc: 0.6068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 80%|████████  | 4/5 [02:15<00:35, 35.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1049 samples, validate on 117 samples\n",
      "Epoch 1/200\n",
      " - 4s - loss: 0.6933 - acc: 0.6368 - val_loss: 0.6910 - val_acc: 0.6068\n",
      "Epoch 2/200\n",
      " - 3s - loss: 0.6927 - acc: 0.6435 - val_loss: 0.6904 - val_acc: 0.6581\n",
      "Epoch 3/200\n",
      " - 3s - loss: 0.6553 - acc: 0.6063 - val_loss: 0.6652 - val_acc: 0.6667\n",
      "Epoch 4/200\n",
      " - 3s - loss: 0.6102 - acc: 0.7102 - val_loss: 0.6530 - val_acc: 0.6752\n",
      "Epoch 5/200\n",
      " - 3s - loss: 0.5999 - acc: 0.7178 - val_loss: 0.6379 - val_acc: 0.6923\n",
      "Epoch 6/200\n",
      " - 3s - loss: 0.5916 - acc: 0.7312 - val_loss: 0.6277 - val_acc: 0.6923\n",
      "Epoch 7/200\n",
      " - 3s - loss: 0.5963 - acc: 0.7293 - val_loss: 0.6475 - val_acc: 0.7265\n",
      "Epoch 8/200\n",
      " - 3s - loss: 0.5943 - acc: 0.6988 - val_loss: 0.6153 - val_acc: 0.6838\n",
      "Epoch 9/200\n",
      " - 3s - loss: 0.5881 - acc: 0.7445 - val_loss: 0.6165 - val_acc: 0.6923\n",
      "Epoch 10/200\n",
      " - 3s - loss: 0.5754 - acc: 0.7359 - val_loss: 0.6153 - val_acc: 0.6923\n",
      "Epoch 11/200\n",
      " - 3s - loss: 0.5654 - acc: 0.7359 - val_loss: 0.6133 - val_acc: 0.7179\n",
      "Epoch 12/200\n",
      " - 3s - loss: 0.5568 - acc: 0.7483 - val_loss: 0.6034 - val_acc: 0.7265\n",
      "Epoch 13/200\n",
      " - 3s - loss: 0.5459 - acc: 0.7483 - val_loss: 0.6078 - val_acc: 0.7350\n",
      "Epoch 14/200\n",
      " - 3s - loss: 0.5366 - acc: 0.7626 - val_loss: 0.6314 - val_acc: 0.7350\n",
      "Epoch 15/200\n",
      " - 3s - loss: 0.5304 - acc: 0.7493 - val_loss: 0.6473 - val_acc: 0.7094\n",
      "Epoch 16/200\n",
      " - 3s - loss: 0.5201 - acc: 0.7512 - val_loss: 0.6329 - val_acc: 0.7607\n",
      "Epoch 17/200\n",
      " - 3s - loss: 0.5136 - acc: 0.7750 - val_loss: 0.6669 - val_acc: 0.6923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 5/5 [03:10<00:00, 41.21s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "t = ta.Scan(x=X_train,\n",
    "            y=asarray(y_train).reshape(-1, 1),\n",
    "            x_val=X_test, \n",
    "            y_val=asarray(y_test).reshape(-1, 1),\n",
    "            model=char_cnn_model_talos,\n",
    "            round_limit=5,\n",
    "#             fraction_limit=0.05,\n",
    "            disable_progress_bar=False,\n",
    "            params=opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights passed to activation function\n",
    "from talos import Deploy, Predict, Restore\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(p.predict(x[np.newaxis,:,:])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((len(X_test),2))\n",
    "for i,(y,x) in enumerate(zip(y_test, X_test)):\n",
    "    results[i,:] = [y, float(p.predict(x[np.newaxis,:,:]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-07-25_14-35-47'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"foo-\" + datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + \".csv\", \n",
    "           results,fmt='%.15f', \n",
    "           delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy package test_deploy have been saved.\n",
      "data is not 2d, dummy data written instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<talos.commands.deploy.Deploy at 0x7fa8de073f98>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Deploy(scan_object=t, model_name='test_deploy',metric='val_acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/neil/cloud/habitual_errors_NLP/notebooks'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwdz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
