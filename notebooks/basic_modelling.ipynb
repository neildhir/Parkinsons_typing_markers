{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preliminary ideas for starting off:\n",
    "\n",
    "- Straight up this is supervised learning, since we have the labels of the people in the trial -- this is great, because it makes life a whole lot easier\n",
    "- Look at the conditional entropy of the sentences, this should emphatically yield when habitual errors start to creep in as we are measuring the conditional Shannon Entropy of a sequence.\n",
    "- Build a language model of text and use it to reference against (not sure about this one yet, as the purpose is not to forecast but discriminate PD from HC)\n",
    "    - That being said, we could build a fairly simple LSTM autoencoder, who's soul-purpose is to separate PD from HC, based on their text input.\n",
    "        - The problem with this, as I see it now, is that we may not have enough data (yet) to build such a model. But we could collect more and it would be imminently doable.\n",
    "- Sequence modelling (not sure about this yet either; the utility of building such a model is ... unclear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, are we interested in:\n",
    "\n",
    "1. Discriminative analysis ('who has PD, who has not?'), or\n",
    "2. Decriptive analysis ('ok, you have PD, so what in your typing told us that you have PD?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Colin and Tom\n",
    "\n",
    "1. Would there be any utility in having a generative model capable of generating new instances of HC and PD sentences? I.e. we could create a model that learns to spit out a sentence with a habit-error type slight to it, but would this be of any use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) A simple information theoretic entropy approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two notions to consider:\n",
    "\n",
    "1. We can measure the conditional entropy in local sentences\n",
    "2. We can measure the conditional entropy in `global` paragraphs\n",
    "\n",
    "Essentially we are asking at what granularity we posit that habitual errors are likely to appear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Deep-learning: Autoencoder, CNNs, LSTMs, RNNs ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://lilianweng.github.io/lil-log/assets/images/autoencoder-architecture.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/brightmart/text_classification/master/images/TextCNN.JPG\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/brightmart/text_classification/master/images/TextCNN.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 2 (this one is a better overview I think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/max/1200/0*jpZmafzzqJAPAOw5.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://cdn-images-1.medium.com/max/1200/0*jpZmafzzqJAPAOw5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea:\n",
    "\n",
    "- Using the above image as reference, we would take the \"bottleneck\" compression and use that in some classifier.\n",
    "- Indeed we can incorporate the classification into the network as well.\n",
    "- The input will be:\n",
    "    - Typed sentences\n",
    "    - _probably not_ the reference sentence, but I haven't thought enough about that yet\n",
    "    - All the meta information regarding the typing e.g. IKI\n",
    "- In short we will note design any features for some classifiers, but leave it to the network to extract its own discriminative information\n",
    "- What we hope to see is a model capable of transforming the NLP data into some low-dimensional representation (which we may even be able to visualsise) which clearly separates PD/HC\n",
    "\n",
    "****Before any of this, I am going to try naive Bayes -- baselines are important****\n",
    "\n",
    "Because we have labels for all the patients (PD/non-PD) this is a relatively straightforward binary classification task (as you know of course), but since we have no SWEDD patients or GENPD patients, it is much easier to work like this.\n",
    "\n",
    "Other:\n",
    "\n",
    "- Because this DL models have so many parameters, I would like to try an idea that I have been toying with for a while regarding their training; using Bayesian optimisation for model selection (MATLAB, surprisingly, have a good explanation: https://uk.mathworks.com/help/deeplearning/examples/deep-learning-using-bayesian-optimization.html)\n",
    "- If anyone is interested, I usually code DL models in PyTorch (facebook's DL library), that being said I am fairly language agnostic, TensorFlow (Google) works fine too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is this good?\n",
    "\n",
    "- It is straightforward\n",
    "- Fairly easy\n",
    "- There is lots of precedence:\n",
    "    1. https://github.com/erickrf/autoencoder\n",
    "    2. https://arxiv.org/pdf/1408.5882.pdf\n",
    "    3. Nice high-level review/summary: https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f\n",
    "    4. https://github.com/brightmart/text_classification\n",
    "    5. https://paperswithcode.com/task/sentence-classification\n",
    "- Embedding words into sentences is easy; we can also embed single characters should we so wish -- but since we are looking for habitual errors, word embeddings might be more appropriate.\n",
    "- Encoder/Decoder potential models: CNN (probs best for small amount of data)/HAN/RNN/LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why might it be bad?\n",
    "\n",
    "- Necessarily DL models require a lot of data -- though there are novel new ways to get around this. I do not yet know if we have enough of it, since I have only started to build the model and have not got to the point where I can train it yet.\n",
    "- Unfortunately much of DL is still based on __tricks and experience__ -- often we do not know fully know why something works, just that it does. The theory has not quite caught up with practise yet. But it will. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions for Colin and Tom\n",
    "\n",
    "1. Do we have an equipment budget?\n",
    "    - If we go down this route, the analysis phase will be _a lot_ faster if I use a GPU (couple of hundred quid, nothing crazy -- can give it to Liverpool/Sheffield after) \n",
    "    - My **big machine** (i.e. my useful machine, not my laptop) does not currently have a GPU but it does have all the other bells and whistles\n",
    "2. The datasets that we have, is that all that exists in terms of the research that has been done in this area?\n",
    "3. Devil's advocate question: what is the point of this? Your AUC is already very good (from the most recent paper)\n",
    "4. In the English language data, what is the format of the timestamp? What should I convert it to, to make it interpretable?\n",
    "5. What about taking time-derivatives (i.e. speed and acceleration) of IKI? (see next cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $$x = \\textrm{IKI}$$\n",
    "then, we can extract the following meta-data:\n",
    "$$\n",
    "x, \\frac{dx}{dt}, \\frac{d^2x}{dt^2}\n",
    "$$\n",
    "Why? Well, we can bolt this onto our observation vector $\\mathbf{x}$ that we pass to the DL model, and let it choose whether or not this information has high information content w.r.t. the binary classification it is being asked to undertake.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
