{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “You shall know a word by the company it keeps” – Firth, J.R. (1957)\n",
    "\n",
    "Notebook explores various character, word and sentence embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are they?\n",
    "\n",
    "- Word embeddings represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points ('are embedded nearby each other'- Word embeddings allow you to implicitly include external information from the world into your language understanding models\n",
    "- Any set of numbers is a valid word vector, but to be useful, a set of word vectors for a vocabulary should capture the meaning of words, the relationship between words, and the context of different words as they are used naturally\n",
    "\n",
    "There’s a few key characteristics to a set of useful word embeddings:\n",
    "\n",
    "- Every word has a unique word embedding (a vector), which is just a list of numbers for each word.\n",
    "- The word embeddings are multidimensional; typically for a good model, embeddings are between 50 and 500 in length.\n",
    "- For each word, the embedding captures the “meaning” of the word.\n",
    "- Similar words end up with similar embedding values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular Word Embedding Algorithms (<--- euphemism, really called vector-space models)\n",
    "\n",
    "- Word2Vec (by far the most popular right now)\n",
    "    - Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model [(Section 3.1 and 3.2 in Mikolov et al.)](https://arxiv.org/pdf/1301.3781.pdf).\n",
    "- GloVe (global vectors)\n",
    "- [LexVec](https://github.com/alexandres/lexvec)\n",
    "- fasttext\n",
    "- [ELMo](https://arxiv.org/pdf/1802.05365.pdf)\n",
    "- BERT\n",
    "- [State of the art in 2017](http://ruder.io/word-embeddings-2017/index.html)\n",
    "- [A more recent state of the art, 2018](https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec)\n",
    "- [A pretty nice evaluation of existing embedding methods](https://github.com/kudkudak/word-embeddings-benchmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"traditional methods are frequently overshadowed amid the deep learning craze and their relevancy consequently deserves to be mentioned more often\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On combining embeddings of different languages\n",
    "\n",
    "- [This is very relevant](http://ruder.io/cross-lingual-embeddings/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo (Em-beddings from Language Models) vs word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overview of ELMo is as follows:\n",
    "1. Train an LSTM-based language model on some large corpus\n",
    "2. Use the hidden states of the LSTM for each token to compute a vector representation of each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can:\n",
    " 1. Embed the whole sentence (and measure the difference with the target)\n",
    " 2. Embed the individual tokens in the sentence (and measure the difference with the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "from haberrspd.embeddings import calculate_bert_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test= dict()\n",
    "test['a'] = [[\"I am a test sentence\"], [\"The dog walks over the road\"]]\n",
    "test['b'] = [[\"Cocaine in the membrane\"], [\"Wishful thinking is all\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neil/cloud/haberrspd/notebooks/../haberrspd/embeddings.py:159: UserWarning: For the sake of all that is good, do not use this without GPU support.\n",
      "  warnings.warn(\"For the sake of all that is good, do not use this without GPU support.\")\n"
     ]
    }
   ],
   "source": [
    "out = calculate_bert_embeddings(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
