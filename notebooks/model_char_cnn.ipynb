{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "# This blokc is important if we want the memory to grow on the GPU, and not block allocate the whole thing\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.utils import class_weight \n",
    "# Hyperparam opt\n",
    "import talos as ta\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# Set path to find modelling tools for later use\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "# Global params live here\n",
    "import haberrspd.charCNN.globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data-loader\n",
    "from haberrspd.charCNN.auxiliary_tf import create_training_data, \n",
    "# Load model\n",
    "from haberrspd.charCNN.models_tf import char_cnn_model, char_cnn_model_talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data and validation data as well as auxiliary model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\" # Note the relative path\n",
    "# Load training data and auxiliary variables\n",
    "X_train, X_test, y_train, y_test, max_sentence_length = \\\n",
    "create_training_data(DATA_ROOT,\"EnglishData-preprocessed.csv\",'sentence')\n",
    "class_weights = class_weight.compute_class_weight('balanced',list(set(y_train)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rev-up tensorboard\n",
    "# logdir=\"../logs/char-cnn-keras-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "# Set model\n",
    "model = char_cnn_model(max_sentence_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = 'squared_hinge'\n",
    "\n",
    "if loss_func == 'hinge' or loss_func == 'squared_hinge':\n",
    "    y_train = [-1 if x==0 else x for x in y_train]\n",
    "    y_test = [-1 if x==0 else x for x in y_test]\n",
    "    \n",
    "if loss_func == 'binary_crossentropy':\n",
    "    # Check if label-space is correct\n",
    "    if (-1 in y_train) or (-1 in y_test):\n",
    "        y_train = [0 if x==-1 else x for x in y_train]\n",
    "        y_test = [0 if x==-1 else x for x in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(loss=loss_func,  # TODO: change to cosine loss, cosine_proximity, binary_crossentropy\n",
    "              optimizer='adam',            # TODO: check which is most appropriate\n",
    "              metrics=['accuracy'])        # Probs other options here which are more useful\n",
    "\n",
    "# Check if checkpoints dir exists, if not make it\n",
    "if not os.path.exists('../../keras_checkpoints'):\n",
    "    os.makedirs('../../keras_checkpoints')\n",
    "\n",
    "# Callbacks\n",
    "file_name = \"char-CNN\"\n",
    "check_cb = ModelCheckpoint(file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0,\n",
    "                           save_best_only=True,\n",
    "                           mode='min')\n",
    "\n",
    "earlystop_cb = EarlyStopping(monitor='val_loss',\n",
    "                             patience=7,\n",
    "                             verbose=0,\n",
    "                             mode='auto')\n",
    "\n",
    "# history = LossHistory()\n",
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "-Add class-weight option to take into account class-imbalance on patients and controls\n",
    "\"\"\"\n",
    "fit_hist = model.fit(X_train,\n",
    "                     y_train,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     verbose=0, # Set to zero if using live plotting of losses\n",
    "                     class_weight = class_weights,\n",
    "                     batch_size=128,\n",
    "                     epochs=40,\n",
    "                     #shuffle=True, # Our data is already shuffled during data loading\n",
    "                     callbacks=[\n",
    "                                #check_cb,\n",
    "                                #tensorboard_callback,\n",
    "                                PlotLossesCallback(),\n",
    "                                #earlystop_cb\n",
    "                               ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALOS: hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.charCNN.models_tf import char_cnn_model_talos\n",
    "from haberrspd.charCNN.auxiliary_tf import create_training_data\n",
    "from numpy import vstack, asarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382205952"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights,max_sentence_length = None,None # These needs to be set\n",
    "# Set the parameter space\n",
    "opt_params ={'conv_output_space' : [8,16,32,64],\n",
    "             'number_of_large_filters' : [1,2,3,4],             \n",
    "             'number_of_small_filters' : [1,2,3,4],\n",
    "             'large_filter_lengt' : [30,60,120],\n",
    "             'small_filter_lengt' : [5,10,20],\n",
    "             'pool_length' : [2,4,8,16],\n",
    "             'dense_units_layer_3' : [32,16,8,4],\n",
    "             'dense_units_layer_2' : [32,16,8,4],\n",
    "             'batch_size': [8,16,32,64],\n",
    "             'epochs': [200], # Use early stopping instead\n",
    "             'dropout': (0, 0.5, 5),\n",
    "             'conv_kernel_initializer': ['uniform','normal'],\n",
    "             'conv_bias_initializer': ['uniform','normal'],\n",
    "             'dense_kernel_initializer': ['uniform','normal'],\n",
    "             'dense_bias_initializer': ['uniform','normal'],\n",
    "             'optimizer': ['adam', 'nadam', 'rmsprop'],\n",
    "             'loss': ['logcosh', 'binary_crossentropy'],\n",
    "             'conv_activation':['relu', 'elu'],\n",
    "             'dense_activation':['relu', 'elu'],\n",
    "             'last_activation': ['sigmoid'],\n",
    "             # Stationary parameters, i.e. do not get optimised\n",
    "             'max_sentence_length':[max_sentence_length]\n",
    "            }\n",
    "\n",
    "\"\"\"\n",
    "'sgd': SGD,\n",
    "'rmsprop': RMSprop,\n",
    "'adagrad': Adagrad,\n",
    "'adadelta': Adadelta,\n",
    "'adam': Adam,\n",
    "'adamax': Adamax,\n",
    "'nadam': Nadam\n",
    "\"\"\"\n",
    "\n",
    "def size_of_optimisation_space(opt_params):\n",
    "    space = 1\n",
    "    for attribute in opt_params.keys():\n",
    "        space*=len(opt_params[attribute])\n",
    "        \n",
    "    return space\n",
    "\n",
    "int(size_of_optimisation_space(opt_params)*0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, Nadam\n",
    "\n",
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\" # Note the relative path\n",
    "# Load training data and auxiliary variables\n",
    "X_train, X_test, y_train, y_test, max_sentence_length = \\\n",
    "create_training_data(DATA_ROOT,\"EnglishData-preprocessed.csv\",'sentence')\n",
    "class_weights = dict(zip([0,1], \n",
    "                         class_weight.compute_class_weight('balanced',\n",
    "                                                           list(set(y_train)),\n",
    "                                                           y_train)))\n",
    "\n",
    "params ={\n",
    "         # Learning rate\n",
    "         'lr': (0.1, 10, 2),\n",
    "         'conv_output_space' : [4],#,8],\n",
    "         'number_of_large_filters' : [2],             \n",
    "         'number_of_small_filters' : [2],\n",
    "         'large_filter_length' : [60],\n",
    "         'small_filter_length' : [5],\n",
    "         'pool_length' : [2],\n",
    "         'dense_units_layer_3' : [32],\n",
    "         'dense_units_layer_2' : [16],\n",
    "         'batch_size': [4,8],\n",
    "         'epochs': [100],\n",
    "         'dropout': [0.05],#,0.1,0.2],\n",
    "         'conv_kernel_initializer': ['uniform'],\n",
    "         'conv_bias_initializer': ['uniform'],\n",
    "         'dense_kernel_initializer': ['uniform'],\n",
    "         'dense_bias_initializer': ['uniform'],\n",
    "         'optimizer': [Adam],\n",
    "         'loss': ['binary_crossentropy'],\n",
    "         'conv_activation':['relu'],\n",
    "         'dense_activation':['relu'],\n",
    "         'last_activation': ['sigmoid'],\n",
    "         # Stationary parameters, i.e. do not get optimised\n",
    "         'max_sentence_length':[max_sentence_length]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ta.Scan(vstack([X_train,X_test]), \n",
    "            asarray(y_train+y_test).reshape(-1, 1), \n",
    "            model=char_cnn_model_talos,\n",
    "            disable_progress_bar=False,\n",
    "            params=params, \n",
    "            val_split=0.2)\n",
    "#             grid_downsample=0.01,  # Randomly samples 1% of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights passed to activation function\n",
    "from talos import Deploy, Predict\n",
    "from haberrspd.charCNN.auxiliary_tf import binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Predict(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.predict(binarize(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in X_test:\n",
    "    print(p.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import one_hot, float32, cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_test(x):\n",
    "    \"\"\"\n",
    "    # XXX: change this to use a smaller dimensional representation of each character, see footnote 3 of paper `Character-aware neural language model`\n",
    "\n",
    "    # Example from Torch: torch.nn.Embedding(big number, much smaller number)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : string\n",
    "        A character string from a sentence\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor\n",
    "        A one-hot encoded tensor-representation of a character\n",
    "    \"\"\"\n",
    "    # TODO: double-check how this encoding actually prints out [ 0 0 0 0 1 ...] etc\n",
    "    return cast(one_hot(x,\n",
    "                        49,\n",
    "                        on_value=1,\n",
    "                        off_value=0,\n",
    "                        axis=-1),\n",
    "                float32)  # TODO: check precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast:0' shape=(14000, 49) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarize_test(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00666015],\n",
       "        [-0.0045552 ],\n",
       "        [ 0.00290709],\n",
       "        [-0.00390608],\n",
       "        [-0.02322032],\n",
       "        [ 0.0098001 ],\n",
       "        [-0.00305747],\n",
       "        [ 0.01425729],\n",
       "        [ 0.06003369],\n",
       "        [-0.00109358],\n",
       "        [-0.01998529],\n",
       "        [-0.02026838],\n",
       "        [-0.00320968],\n",
       "        [-0.03552249],\n",
       "        [-0.03317583],\n",
       "        [ 0.00533802]], dtype=float32), array([0.03063761], dtype=float32)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.saved_weights[-1][-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deploy(t, metric='val_acc', model_name='test_last_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.data.sort_values(by=['val_acc'],ascending=False)[['val_acc', 'batch_size','dropout']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.charCNN.auxiliary_tf import create_mjff_data_objects\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of alphabet is: 49\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\" # Note the relative path\n",
    "df = read_csv(DATA_ROOT / \"EnglishData-preprocessed.csv\", header=0)  # MJFF data\n",
    "# TODO: make the fuck sure that subject diagnoses are correctly ordered\n",
    "subject_documents, subjects_diagnoses, alphabet = create_mjff_data_objects(df)\n",
    "print(\"Length of alphabet is: {}\".format(len(alphabet)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 49\n"
     ]
    }
   ],
   "source": [
    "# Store alphabet size\n",
    "alphabet_size = len(alphabet)\n",
    "print('Total number of characters:', alphabet_size)\n",
    "alphabet_indices = dict((c, i) for i, c in enumerate(alphabet))\n",
    "# indices_alphabet = dict((i, c) for i, c in enumerate(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.word_index = alphabet_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise\n",
    "tk = Tokenizer(num_words=None, char_level=True)\n",
    "# Fit to text\n",
    "test = ['aaaaaaBBBB!\"#$%&() ', 'I am a little teapot']\n",
    "tk.fit_on_texts(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = tk.texts_to_sequences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  1,  1,  1,  1,  3,  3,  3,  3,  8,  9, 10, 11, 12, 13,\n",
       "        14, 15,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 5,  2,  1, 16,  2,  1,  2,  6,  5,  4,  4,  6,  7,  2,  4,  7,\n",
       "         1, 17, 18,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0]], dtype=int32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding\n",
    "test_seqs =  pad_sequences(seqs, maxlen=40, padding='post')\n",
    "test_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data_keras(DATA_ROOT, \n",
    "                               data_string):\n",
    "    \"\"\"\n",
    "    This function creats one-hot encoded character -data for the document (=subject)\n",
    "    classification model, as well as the sentence classification model. The functionality\n",
    "    within is keras specific.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    DATA_ROOT : str\n",
    "        Location of the MJFF data folder\n",
    "    data_string : str\n",
    "        The .csv file that we want to analyse\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Contains the training and test data as well as some parameters\n",
    "    \"\"\"\n",
    "    assert type(data_string) is str\n",
    "\n",
    "    df = read_csv(DATA_ROOT / data_string, header=0)  # MJFF data\n",
    "    subject_documents, subjects_diagnoses, alphabet = create_mjff_data_objects(df)\n",
    "\n",
    "    # Store alphabet size\n",
    "    alphabet_size = len(alphabet)\n",
    "\n",
    "    print('Total number of characters:', alphabet_size)\n",
    "    alphabet_indices = dict((c, i) for i, c in enumerate(alphabet))\n",
    "\n",
    "    # Rounds (up) to nearest thousand\n",
    "    max_sentence_length = round(df.Preprocessed_typed_sentence.apply(lambda x: len(x)).max(), -3)\n",
    "\n",
    "    # Make training data array\n",
    "    all_sentences = [item for sublist in subject_documents for item in sublist]\n",
    "    \n",
    "    # Initialise tokenizer which maps characters to integers\n",
    "    tk = Tokenizer(num_words=None, char_level=True)\n",
    "    \n",
    "    # Fit to text: convert all chars to ints\n",
    "    tk.fit_on_texts(all_sentences)\n",
    "    \n",
    "    # Update alphabet \n",
    "    tk.word_index = alphabet_indices\n",
    "    \n",
    "    # Get integer sequences: converts sequences of chars to sequences of ints\n",
    "    int_sequences = tk.texts_to_sequences(all_sentences)\n",
    "    \n",
    "    # Pad sequences so that they all have the same length and then one-hot encode\n",
    "    X = to_categorical(pad_sequences(int_sequences, maxlen=max_sentence_length, padding='post'))\n",
    "    \n",
    "    # Get labels (diagnoses)\n",
    "    y = df.Diagnosis.tolist()\n",
    "\n",
    "    # Chop up data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test, max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 49\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, max_sentence_length = create_training_data_keras(DATA_ROOT, \"EnglishData-preprocessed.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
