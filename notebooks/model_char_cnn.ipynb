{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "# This blokc is important if we want the memory to grow on the GPU, and not block allocate the whole thing\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from datetime import datetime\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from livelossplot.keras import PlotLossesCallback\n",
    "from pathlib import Path\n",
    "from sklearn.utils import class_weight \n",
    "# Hyperparam opt\n",
    "import talos as ta\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# Set path to find modelling tools for later use\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "# Global params live here\n",
    "import haberrspd.charCNN.globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data-loader\n",
    "from haberrspd.charCNN.auxiliary_tf import create_training_data, \n",
    "# Load model\n",
    "from haberrspd.charCNN.models_tf import char_cnn_model, char_cnn_model_talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data and validation data as well as auxiliary model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\" # Note the relative path\n",
    "# Load training data and auxiliary variables\n",
    "X_train, X_test, y_train, y_test, max_sentence_length = \\\n",
    "create_training_data(DATA_ROOT,\"EnglishData-preprocessed.csv\",'sentence')\n",
    "class_weights = class_weight.compute_class_weight('balanced',list(set(y_train)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rev-up tensorboard\n",
    "# logdir=\"../logs/char-cnn-keras-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "# Set model\n",
    "model = char_cnn_model(max_sentence_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = 'squared_hinge'\n",
    "\n",
    "if loss_func == 'hinge' or loss_func == 'squared_hinge':\n",
    "    y_train = [-1 if x==0 else x for x in y_train]\n",
    "    y_test = [-1 if x==0 else x for x in y_test]\n",
    "    \n",
    "if loss_func == 'binary_crossentropy':\n",
    "    # Check if label-space is correct\n",
    "    if (-1 in y_train) or (-1 in y_test):\n",
    "        y_train = [0 if x==-1 else x for x in y_train]\n",
    "        y_test = [0 if x==-1 else x for x in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(loss=loss_func,  # TODO: change to cosine loss, cosine_proximity, binary_crossentropy\n",
    "              optimizer='adam',            # TODO: check which is most appropriate\n",
    "              metrics=['accuracy'])        # Probs other options here which are more useful\n",
    "\n",
    "# Check if checkpoints dir exists, if not make it\n",
    "if not os.path.exists('../../keras_checkpoints'):\n",
    "    os.makedirs('../../keras_checkpoints')\n",
    "\n",
    "# Callbacks\n",
    "file_name = \"char-CNN\"\n",
    "check_cb = ModelCheckpoint(file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0,\n",
    "                           save_best_only=True,\n",
    "                           mode='min')\n",
    "\n",
    "earlystop_cb = EarlyStopping(monitor='val_loss',\n",
    "                             patience=7,\n",
    "                             verbose=0,\n",
    "                             mode='auto')\n",
    "\n",
    "# history = LossHistory()\n",
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "-Add class-weight option to take into account class-imbalance on patients and controls\n",
    "\"\"\"\n",
    "fit_hist = model.fit(X_train,\n",
    "                     y_train,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     verbose=0, # Set to zero if using live plotting of losses\n",
    "                     class_weight = class_weights,\n",
    "                     batch_size=128,\n",
    "                     epochs=40,\n",
    "                     #shuffle=True, # Our data is already shuffled during data loading\n",
    "                     callbacks=[\n",
    "                                #check_cb,\n",
    "                                #tensorboard_callback,\n",
    "                                PlotLossesCallback(),\n",
    "                                #earlystop_cb\n",
    "                               ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALOS: hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.charCNN.models_tf import char_cnn_model_talos\n",
    "from haberrspd.charCNN.auxiliary_tf import create_training_data\n",
    "from numpy import vstack, asarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159252"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights,max_sentence_length = None,None # These needs to be set\n",
    "# Set the parameter space\n",
    "opt_params ={'conv_output_space' : [16,32,64],\n",
    "             'number_of_filters' : [1,2,3,4],\n",
    "             'filter_length' : [10,15,20],\n",
    "             'pool_length' : [2,4,8],\n",
    "             'dense_units_layer_3' : [32,16,8,4],\n",
    "             'dense_units_layer_2' : [32,16,8,4],\n",
    "             'batch_size': [16,32,64,128],\n",
    "             'epochs': [64,128],\n",
    "             'dropout': (0, 0.5, 5),\n",
    "             'conv_kernel_initializer': ['uniform','normal'],\n",
    "             'conv_bias_initializer': ['uniform','normal'],\n",
    "             'dense_kernel_initializer': ['uniform','normal'],\n",
    "             'dense_bias_initializer': ['uniform','normal'],\n",
    "             'optimizer': ['adam', 'nadam', 'rmsprop'],\n",
    "             'loss': ['logcosh', 'binary_crossentropy'],\n",
    "             'conv_activation':['relu', 'elu'],\n",
    "             'dense_activation':['relu', 'elu'],\n",
    "             'last_activation': ['sigmoid'],\n",
    "             # Stationary parameters, i.e. do not get optimised\n",
    "             'class_weight':[class_weights],\n",
    "             'max_sentence_length':[max_sentence_length]\n",
    "            }\n",
    "\n",
    "\"\"\"\n",
    "'sgd': SGD,\n",
    "'rmsprop': RMSprop,\n",
    "'adagrad': Adagrad,\n",
    "'adadelta': Adadelta,\n",
    "'adam': Adam,\n",
    "'adamax': Adamax,\n",
    "'nadam': Nadam\n",
    "\"\"\"\n",
    "\n",
    "def size_of_optimisation_space(opt_params):\n",
    "    space = 1\n",
    "    for attribute in opt_params.keys():\n",
    "        space*=len(opt_params[attribute])\n",
    "        \n",
    "    return space\n",
    "\n",
    "int(size_of_optimisation_space(opt_params)*0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 49\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\" # Note the relative path\n",
    "# Load training data and auxiliary variables\n",
    "X_train, X_test, y_train, y_test, max_sentence_length = \\\n",
    "create_training_data(DATA_ROOT,\"EnglishData-preprocessed.csv\",'sentence')\n",
    "class_weights = class_weight.compute_class_weight('balanced',list(set(y_train)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7770370370370371, 1.4024064171122994]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params ={'conv_output_space' : [16],\n",
    "             'number_of_filters' : [2],\n",
    "             'filter_length' : [10],\n",
    "             'pool_length' : [2],\n",
    "             'dense_units_layer_3' : [4],\n",
    "             'dense_units_layer_2' : [4],\n",
    "             'batch_size': [64],\n",
    "             'epochs': [64],\n",
    "             'dropout': [0.5],\n",
    "             'conv_kernel_initializer': ['uniform','normal'],\n",
    "             'conv_bias_initializer': ['normal'],\n",
    "             'dense_kernel_initializer': ['normal'],\n",
    "             'dense_bias_initializer': ['normal'],\n",
    "             'optimizer': ['adam'],\n",
    "             'loss': ['binary_crossentropy'],\n",
    "             'conv_activation':['elu'],\n",
    "             'dense_activation':['elu'],\n",
    "             'last_activation': ['sigmoid'],\n",
    "             # Stationary parameters, i.e. do not get optimised\n",
    "             'class_weight': class_weights.tolist(),\n",
    "             'max_sentence_length':[max_sentence_length]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/neil/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/neil/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/neil/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [02:33<00:00, 38.59s/it]\n"
     ]
    }
   ],
   "source": [
    "t = ta.Scan(vstack([X_train,X_test]), \n",
    "            asarray(y_train+y_test).reshape(-1, 1), \n",
    "            model=char_cnn_model_talos,\n",
    "            disable_progress_bar=True,\n",
    "            params=opt_params, \n",
    "            val_split=.1)\n",
    "#             grid_downsample=0.01,  # Randomly samples 1% of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>class_weight</th>\n",
       "      <th>conv_activation</th>\n",
       "      <th>conv_bias_initializer</th>\n",
       "      <th>conv_kernel_initializer</th>\n",
       "      <th>...</th>\n",
       "      <th>dense_units_layer_3</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>filter_length</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>loss</th>\n",
       "      <th>max_sentence_length</th>\n",
       "      <th>number_of_filters</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>pool_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>0.853589</td>\n",
       "      <td>0.760684</td>\n",
       "      <td>0.377801</td>\n",
       "      <td>0.825548</td>\n",
       "      <td>64</td>\n",
       "      <td>0.792296</td>\n",
       "      <td>elu</td>\n",
       "      <td>normal</td>\n",
       "      <td>uniform</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>14000</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>1.942644</td>\n",
       "      <td>0.683761</td>\n",
       "      <td>0.277670</td>\n",
       "      <td>0.877026</td>\n",
       "      <td>64</td>\n",
       "      <td>0.792296</td>\n",
       "      <td>elu</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>14000</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>1.028637</td>\n",
       "      <td>0.786325</td>\n",
       "      <td>0.411917</td>\n",
       "      <td>0.804576</td>\n",
       "      <td>64</td>\n",
       "      <td>1.355297</td>\n",
       "      <td>elu</td>\n",
       "      <td>normal</td>\n",
       "      <td>uniform</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>14000</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>1.490123</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.341364</td>\n",
       "      <td>0.854147</td>\n",
       "      <td>64</td>\n",
       "      <td>1.355297</td>\n",
       "      <td>elu</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>14000</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs  val_loss   val_acc      loss       acc  batch_size  \\\n",
       "0            64  0.853589  0.760684  0.377801  0.825548          64   \n",
       "1            64  1.942644  0.683761  0.277670  0.877026          64   \n",
       "2            64  1.028637  0.786325  0.411917  0.804576          64   \n",
       "3            64  1.490123  0.769231  0.341364  0.854147          64   \n",
       "\n",
       "   class_weight conv_activation conv_bias_initializer conv_kernel_initializer  \\\n",
       "0      0.792296             elu                normal                 uniform   \n",
       "1      0.792296             elu                normal                  normal   \n",
       "2      1.355297             elu                normal                 uniform   \n",
       "3      1.355297             elu                normal                  normal   \n",
       "\n",
       "   ...  dense_units_layer_3 dropout epochs filter_length  last_activation  \\\n",
       "0  ...                    4     0.5     64            10          sigmoid   \n",
       "1  ...                    4     0.5     64            10          sigmoid   \n",
       "2  ...                    4     0.5     64            10          sigmoid   \n",
       "3  ...                    4     0.5     64            10          sigmoid   \n",
       "\n",
       "                  loss  max_sentence_length  number_of_filters  optimizer  \\\n",
       "0  binary_crossentropy                14000                  2       adam   \n",
       "1  binary_crossentropy                14000                  2       adam   \n",
       "2  binary_crossentropy                14000                  2       adam   \n",
       "3  binary_crossentropy                14000                  2       adam   \n",
       "\n",
       "  pool_length  \n",
       "0           2  \n",
       "1           2  \n",
       "2           2  \n",
       "3           2  \n",
       "\n",
       "[4 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
