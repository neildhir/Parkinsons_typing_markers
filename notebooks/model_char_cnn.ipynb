{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "# This blokc is important if we want the memory to grow on the GPU, and not block allocate the whole thing\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from datetime import datetime\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from livelossplot.keras import PlotLossesCallback\n",
    "from pathlib import Path\n",
    "from sklearn.utils import class_weight \n",
    "# Hyperparam opt\n",
    "import talos as ta\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# Set path to find modelling tools for later use\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "# Global params live here\n",
    "import haberrspd.charCNN.globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data-loader\n",
    "from haberrspd.charCNN.auxiliary_tf import create_training_data, \n",
    "# Load model\n",
    "from haberrspd.charCNN.models_tf import char_cnn_model, char_cnn_model_talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data and validation data as well as auxiliary model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\" # Note the relative path\n",
    "# Load training data and auxiliary variables\n",
    "X_train, X_test, y_train, y_test, max_sentence_length = \\\n",
    "create_training_data(DATA_ROOT,\"EnglishData-preprocessed.csv\",'sentence')\n",
    "class_weights = class_weight.compute_class_weight('balanced',list(set(y_train)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rev-up tensorboard\n",
    "# logdir=\"../logs/char-cnn-keras-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "# Set model\n",
    "model = char_cnn_model(max_sentence_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = 'squared_hinge'\n",
    "\n",
    "if loss_func == 'hinge' or loss_func == 'squared_hinge':\n",
    "    y_train = [-1 if x==0 else x for x in y_train]\n",
    "    y_test = [-1 if x==0 else x for x in y_test]\n",
    "    \n",
    "if loss_func == 'binary_crossentropy':\n",
    "    # Check if label-space is correct\n",
    "    if (-1 in y_train) or (-1 in y_test):\n",
    "        y_train = [0 if x==-1 else x for x in y_train]\n",
    "        y_test = [0 if x==-1 else x for x in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(loss=loss_func,  # TODO: change to cosine loss, cosine_proximity, binary_crossentropy\n",
    "              optimizer='adam',            # TODO: check which is most appropriate\n",
    "              metrics=['accuracy'])        # Probs other options here which are more useful\n",
    "\n",
    "# Check if checkpoints dir exists, if not make it\n",
    "if not os.path.exists('../../keras_checkpoints'):\n",
    "    os.makedirs('../../keras_checkpoints')\n",
    "\n",
    "# Callbacks\n",
    "file_name = \"char-CNN\"\n",
    "check_cb = ModelCheckpoint(file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0,\n",
    "                           save_best_only=True,\n",
    "                           mode='min')\n",
    "\n",
    "earlystop_cb = EarlyStopping(monitor='val_loss',\n",
    "                             patience=7,\n",
    "                             verbose=0,\n",
    "                             mode='auto')\n",
    "\n",
    "# history = LossHistory()\n",
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "-Add class-weight option to take into account class-imbalance on patients and controls\n",
    "\"\"\"\n",
    "fit_hist = model.fit(X_train,\n",
    "                     y_train,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     verbose=0, # Set to zero if using live plotting of losses\n",
    "                     class_weight = class_weights,\n",
    "                     batch_size=128,\n",
    "                     epochs=40,\n",
    "                     #shuffle=True, # Our data is already shuffled during data loading\n",
    "                     callbacks=[\n",
    "                                #check_cb,\n",
    "                                #tensorboard_callback,\n",
    "                                PlotLossesCallback(),\n",
    "                                #earlystop_cb\n",
    "                               ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALOS: hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.charCNN.models_tf import char_cnn_model_talos\n",
    "from haberrspd.charCNN.auxiliary_tf import create_training_data\n",
    "from numpy import vstack, asarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159252"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights,max_sentence_length = None,None # These needs to be set\n",
    "# Set the parameter space\n",
    "opt_params ={'conv_output_space' : [16,32,64],\n",
    "             'number_of_filters' : [1,2,3,4],\n",
    "             'filter_length' : [10,15,20],\n",
    "             'pool_length' : [2,4,8],\n",
    "             'dense_units_layer_3' : [32,16,8,4],\n",
    "             'dense_units_layer_2' : [32,16,8,4],\n",
    "             'batch_size': [16,32,64,128],\n",
    "             'epochs': [64,128],\n",
    "             'dropout': (0, 0.5, 5),\n",
    "             'conv_kernel_initializer': ['uniform','normal'],\n",
    "             'conv_bias_initializer': ['uniform','normal'],\n",
    "             'dense_kernel_initializer': ['uniform','normal'],\n",
    "             'dense_bias_initializer': ['uniform','normal'],\n",
    "             'optimizer': ['adam', 'nadam', 'rmsprop'],\n",
    "             'loss': ['logcosh', 'binary_crossentropy'],\n",
    "             'conv_activation':['relu', 'elu'],\n",
    "             'dense_activation':['relu', 'elu'],\n",
    "             'last_activation': ['sigmoid'],\n",
    "             # Stationary parameters, i.e. do not get optimised\n",
    "             'class_weight':[class_weights],\n",
    "             'max_sentence_length':[max_sentence_length]\n",
    "            }\n",
    "\n",
    "\"\"\"\n",
    "'sgd': SGD,\n",
    "'rmsprop': RMSprop,\n",
    "'adagrad': Adagrad,\n",
    "'adadelta': Adadelta,\n",
    "'adam': Adam,\n",
    "'adamax': Adamax,\n",
    "'nadam': Nadam\n",
    "\"\"\"\n",
    "\n",
    "def size_of_optimisation_space(opt_params):\n",
    "    space = 1\n",
    "    for attribute in opt_params.keys():\n",
    "        space*=len(opt_params[attribute])\n",
    "        \n",
    "    return space\n",
    "\n",
    "int(size_of_optimisation_space(opt_params)*0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 0.7840059790732437, 1: 1.3802631578947369}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\" # Note the relative path\n",
    "# Load training data and auxiliary variables\n",
    "X_train, X_test, y_train, y_test, max_sentence_length = \\\n",
    "create_training_data(DATA_ROOT,\"EnglishData-preprocessed.csv\",'sentence')\n",
    "class_weights = dict(zip([0,1], \n",
    "                         class_weight.compute_class_weight('balanced',\n",
    "                                                           list(set(y_train)),\n",
    "                                                           y_train)))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(class_weights) is dict\n",
    "opt_params ={'conv_output_space' : [16],\n",
    "             'number_of_filters' : [2],\n",
    "             'filter_length' : [10],\n",
    "             'pool_length' : [2],\n",
    "             'dense_units_layer_3' : [4],\n",
    "             'dense_units_layer_2' : [4],\n",
    "             'batch_size': [32, 64],\n",
    "             'epochs': [64],\n",
    "             'dropout': [0.5],\n",
    "             'conv_kernel_initializer': ['uniform','normal'],\n",
    "             'conv_bias_initializer': ['normal'],\n",
    "             'dense_kernel_initializer': ['normal'],\n",
    "             'dense_bias_initializer': ['normal'],\n",
    "             'optimizer': ['adam'],\n",
    "             'loss': ['binary_crossentropy'],\n",
    "             'conv_activation':['elu'],\n",
    "             'dense_activation':['elu'],\n",
    "             'last_activation': ['sigmoid'],\n",
    "#              Stationary parameters, i.e. do not get optimised\n",
    "#              'class_weight': [],#class_weights,\n",
    "             'max_sentence_length':[max_sentence_length]\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Talos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/neil/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/neil/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\n",
      " WARNING: Model currently employs hard-coded class weights.\n",
      "\n",
      "WARNING:tensorflow:From /home/neil/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      " WARNING: Model currently employs hard-coded class weights.\n",
      "\n",
      "\n",
      " WARNING: Model currently employs hard-coded class weights.\n",
      "\n",
      "\n",
      " WARNING: Model currently employs hard-coded class weights.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = ta.Scan(vstack([X_train,X_test]), \n",
    "            asarray(y_train+y_test).reshape(-1, 1), \n",
    "            model=char_cnn_model_talos,\n",
    "            disable_progress_bar=True,\n",
    "            params=opt_params, \n",
    "            val_split=.1)\n",
    "#             grid_downsample=0.01,  # Randomly samples 1% of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>conv_activation</th>\n",
       "      <th>conv_bias_initializer</th>\n",
       "      <th>conv_kernel_initializer</th>\n",
       "      <th>conv_output_space</th>\n",
       "      <th>...</th>\n",
       "      <th>dense_units_layer_3</th>\n",
       "      <th>dropout</th>\n",
       "      <th>epochs</th>\n",
       "      <th>filter_length</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>loss</th>\n",
       "      <th>max_sentence_length</th>\n",
       "      <th>number_of_filters</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>pool_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>1.608765</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.300524</td>\n",
       "      <td>0.856053</td>\n",
       "      <td>32</td>\n",
       "      <td>elu</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>14000</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>0.946162</td>\n",
       "      <td>0.649573</td>\n",
       "      <td>0.380220</td>\n",
       "      <td>0.820782</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>normal</td>\n",
       "      <td>normal</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>14000</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>1.374473</td>\n",
       "      <td>0.632479</td>\n",
       "      <td>0.412882</td>\n",
       "      <td>0.871306</td>\n",
       "      <td>64</td>\n",
       "      <td>elu</td>\n",
       "      <td>normal</td>\n",
       "      <td>uniform</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>14000</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>2.086819</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.344246</td>\n",
       "      <td>0.812202</td>\n",
       "      <td>32</td>\n",
       "      <td>elu</td>\n",
       "      <td>normal</td>\n",
       "      <td>uniform</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>binary_crossentropy</td>\n",
       "      <td>14000</td>\n",
       "      <td>2</td>\n",
       "      <td>adam</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   round_epochs  val_loss   val_acc      loss       acc  batch_size  \\\n",
       "1            64  1.608765  0.692308  0.300524  0.856053          32   \n",
       "3            64  0.946162  0.649573  0.380220  0.820782          64   \n",
       "2            64  1.374473  0.632479  0.412882  0.871306          64   \n",
       "0            64  2.086819  0.589744  0.344246  0.812202          32   \n",
       "\n",
       "  conv_activation conv_bias_initializer conv_kernel_initializer  \\\n",
       "1             elu                normal                  normal   \n",
       "3             elu                normal                  normal   \n",
       "2             elu                normal                 uniform   \n",
       "0             elu                normal                 uniform   \n",
       "\n",
       "   conv_output_space  ... dense_units_layer_3 dropout epochs  filter_length  \\\n",
       "1                 16  ...                   4     0.5     64             10   \n",
       "3                 16  ...                   4     0.5     64             10   \n",
       "2                 16  ...                   4     0.5     64             10   \n",
       "0                 16  ...                   4     0.5     64             10   \n",
       "\n",
       "   last_activation                 loss  max_sentence_length  \\\n",
       "1          sigmoid  binary_crossentropy                14000   \n",
       "3          sigmoid  binary_crossentropy                14000   \n",
       "2          sigmoid  binary_crossentropy                14000   \n",
       "0          sigmoid  binary_crossentropy                14000   \n",
       "\n",
       "   number_of_filters optimizer pool_length  \n",
       "1                  2      adam           2  \n",
       "3                  2      adam           2  \n",
       "2                  2      adam           2  \n",
       "0                  2      adam           2  \n",
       "\n",
       "[4 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.data.sort_values(by=['val_acc'],ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
