{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "# This blokc is important if we want the memory to grow on the GPU, and not block allocate the whole thing\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from datetime import datetime\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from livelossplot.keras import PlotLossesCallback\n",
    "# Hyperparam opt\n",
    "import talos as ta\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# Set path to find modelling tools for later use\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "# Global params live here\n",
    "import haberrspd.charCNN.globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data-loader\n",
    "from haberrspd.charCNN.auxiliary_tf import create_training_data, LossHistory, binarize\n",
    "# Load model\n",
    "from haberrspd.charCNN.models_tf import char_cnn_model, char_cnn_model_talos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data and validation data as well as auxiliary model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.utils import class_weight \n",
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\" # Note the relative path\n",
    "# Load training data and auxiliary variables\n",
    "X_train, X_test, y_train, y_test, max_sentence_length = \\\n",
    "create_training_data(DATA_ROOT,\"EnglishData-preprocessed.csv\",'sentence')\n",
    "class_weights = class_weight.compute_class_weight('balanced',list(set(y_train)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rev-up tensorboard\n",
    "# logdir=\"../logs/char-cnn-keras-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=logdir)\n",
    "# Set model\n",
    "model = char_cnn_model(max_sentence_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = 'squared_hinge'\n",
    "\n",
    "if loss_func == 'hinge' or loss_func == 'squared_hinge':\n",
    "    y_train = [-1 if x==0 else x for x in y_train]\n",
    "    y_test = [-1 if x==0 else x for x in y_test]\n",
    "    \n",
    "if loss_func == 'binary_crossentropy':\n",
    "    # Check if label-space is correct\n",
    "    if (-1 in y_train) or (-1 in y_test):\n",
    "        y_train = [0 if x==-1 else x for x in y_train]\n",
    "        y_test = [0 if x==-1 else x for x in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "model.compile(loss=loss_func,  # TODO: change to cosine loss, cosine_proximity, binary_crossentropy\n",
    "              optimizer='adam',            # TODO: check which is most appropriate\n",
    "              metrics=['accuracy'])        # Probs other options here which are more useful\n",
    "\n",
    "# Check if checkpoints dir exists, if not make it\n",
    "if not os.path.exists('../../keras_checkpoints'):\n",
    "    os.makedirs('../../keras_checkpoints')\n",
    "\n",
    "# Callbacks\n",
    "file_name = \"char-CNN\"\n",
    "check_cb = ModelCheckpoint(file_name + '.{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                           monitor='val_loss',\n",
    "                           verbose=0,\n",
    "                           save_best_only=True,\n",
    "                           mode='min')\n",
    "\n",
    "earlystop_cb = EarlyStopping(monitor='val_loss',\n",
    "                             patience=7,\n",
    "                             verbose=0,\n",
    "                             mode='auto')\n",
    "\n",
    "# history = LossHistory()\n",
    "\"\"\"\n",
    "TODO:\n",
    "\n",
    "-Add class-weight option to take into account class-imbalance on patients and controls\n",
    "\"\"\"\n",
    "fit_hist = model.fit(X_train,\n",
    "                     y_train,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     verbose=0, # Set to zero if using live plotting of losses\n",
    "                     class_weight = class_weights,\n",
    "                     batch_size=128,\n",
    "                     epochs=40,\n",
    "                     #shuffle=True, # Our data is already shuffled during data loading\n",
    "                     callbacks=[\n",
    "                                #check_cb,\n",
    "                                #tensorboard_callback,\n",
    "                                PlotLossesCallback(),\n",
    "                                #earlystop_cb\n",
    "                               ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TALOS: hyperparameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the parameter space\n",
    "p = {'conv_output_space' = [16,32,64],\n",
    "     'number_of_filters' = [1,2,3,4,5,6],\n",
    "     'filter_length' = [10,15,20],\n",
    "     'pool_length' = [2,4,8],\n",
    "     'lr': (0.5, 5, 10),\n",
    "     'first_neuron':[4, 8, 16, 32, 64],\n",
    "     'hidden_layers':[0, 1, 2],\n",
    "     'batch_size': [16,32,64,128],\n",
    "     'epochs': [16,32,64,128],\n",
    "     'dropout': (0, 0.5, 5),\n",
    "     'kernel_initializer': [RandomNormal(mean=0.0, stddev=0.05)],\n",
    "     'bias_initializer': [RandomNormal(mean=0.0, stddev=0.05)],\n",
    "     'weight_regulizer':[None],\n",
    "     'emb_output_dims': [None],\n",
    "     'shape':['brick','long_funnel'],\n",
    "     'optimizer': [Adam, Nadam, RMSprop],\n",
    "     'losses': [logcosh, binary_crossentropy],\n",
    "     'activation':[relu, elu],\n",
    "     'last_activation': [sigmoid]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'a':1,'b':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "test(4,**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(k,**args):\n",
    "    print(k+args['a']+args['b'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
