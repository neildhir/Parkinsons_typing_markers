{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PyTorch version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference is found here: http://www.realworldnlpbook.com/blog/training-sentiment-analyzer-using-allennlp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "    StanfordSentimentTreeBankDatasetReader\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding, ElmoTokenEmbedder\n",
    "from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from allennlp.training.trainer import Trainer\n",
    "\n",
    "from haberrspd.predictors import SentenceClassifierPredictor # copied to main directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build bespoke dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\") / \"MJFF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "\n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "config = Config(\n",
    "    testing=True,\n",
    "    seed=1,\n",
    "    batch_size=32,\n",
    "    lr=1e-4,\n",
    "    epochs=20,\n",
    "    hidden_sz=128,\n",
    "    max_seq_len=100,  # necessary to limit memory usage\n",
    "    max_vocab_size=100000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "214it [00:00, 2139.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "436it [00:00, 2161.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "655it [00:00, 2165.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "873it [00:00, 2164.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "941it [00:00, 2160.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "118it [00:00, 2055.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "118it [00:00, 2081.12it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from haberrspd.pdnet_elmo import MJFFdatasetReader\n",
    "\n",
    "# The token indexer is responsible for mapping tokens to integers\n",
    "token_indexer = ELMoTokenCharactersIndexer()\n",
    "\n",
    "# Specify the tokeniser\n",
    "def tokenizer(x: str):\n",
    "    return [w.text for w in SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words(x)[:config.max_seq_len]]\n",
    "\n",
    "# Reader instance to get the MJFF data\n",
    "reader = MJFFdatasetReader(tokenizer=tokenizer,\n",
    "                           token_indexers={\"tokens\": token_indexer},\n",
    "                           max_seq_len=config.max_seq_len,\n",
    "                           testing=config.testing)\n",
    "\n",
    "# Load the processed MJFF dataset members (train and test)\n",
    "# TODO: ensure that we are shuffling the rows of inputs before we do the actual classification\n",
    "train_ds, validate_ds, test_ds = (reader.read(DATA_ROOT / filename) for filename in [\"train_MJFF_English.csv\",\n",
    "                                                                        \"validate_MJFF_English.csv\",\n",
    "                                                                        \"test_MJFF_English.csv\"])\n",
    "# We don't need to build the vocab: all that is handled by the token indexer\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# The iterator is responsible for batching the data and preparing it for input into the model. \n",
    "# We'll use the BucketIterator that batches text sequences of similar lengths together.\n",
    "iterator = BucketIterator(batch_size=config.batch_size, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in AllenNLP represents a model that is trained.\n",
    "class BasicSentenceClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 output_size: int) -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "        # Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
    "        # (usually a sequence of embedded word vectors), processes it, and returns it as a single\n",
    "        # vector. Oftentimes, this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
    "        # AllenNLP also supports CNNs and other simple architectures (for example,\n",
    "        # just averaging over the input vectors).\n",
    "        self.encoder = encoder\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.projection = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=output_size)\n",
    "        # We use the cross-entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        \n",
    "        #self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "        self.loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                Patient_ID: Any,\n",
    "                Diagnosis: torch.Tensor) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them of equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        class_logits = self.projection(encoder_out).view(-1)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"class_logit\": class_logits}\n",
    "        output[\"loss\"] = self.loss_function(class_logits, Diagnosis.float())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 'Small' pre-trained model\n",
    "options_file = ('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo'\n",
    "                '/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json')\n",
    "weight_file = ('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo'\n",
    "               '/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5')\n",
    "elmo_embedder = ElmoTokenEmbedder(options_file, weight_file)\n",
    "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
    "# not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": elmo_embedder})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medium pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = ('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo'\n",
    "                '/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_options.json')\n",
    "weight_file = ('https://s3-us-west-2.amazonaws.com/allennlp/models/elmo'\n",
    "               '/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_weights.hdf5')\n",
    "# elmo_embedder = Elmo(options_file, weight_file,num_output_representations=2)\n",
    "elmo_embedder = ElmoTokenEmbedder(options_file, weight_file)\n",
    "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
    "# not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": elmo_embedder})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = ('')\n",
    "weight_file = ('')\n",
    "elmo_embedder = Elmo(options_file, weight_file,num_output_representations=)\n",
    "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
    "# not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": elmo_embedder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM = PytorchSeq2VecWrapper(torch.nn.LSTM(word_embeddings.get_output_dim(), \n",
    "                                           config.hidden_sz,\n",
    "                                           bidirectional=True,\n",
    "                                           batch_first=True))\n",
    "model = BasicSentenceClassifier(word_embeddings, \n",
    "                                LSTM,\n",
    "                                1)\n",
    "# Next let's check if we have access to a GPU.\n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    # Since we do, we move our model to GPU 0.\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    # In this case we don't, so we specify -1 to fall back to the CPU. (Where the model already resides.)\n",
    "    cuda_device = -1\n",
    "    print(\"CUDA was not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=config.lr, \n",
    "                       weight_decay=1e-5)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_ds,\n",
    "                  validation_dataset=validate_ds,\n",
    "                  patience=10,\n",
    "                  num_epochs=config.epochs,\n",
    "                  cuda_device=cuda_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.6984 ||:   3%|▎         | 1/30 [00:00<00:05,  5.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6991 ||:  10%|█         | 3/30 [00:00<00:03,  7.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6970 ||:  17%|█▋        | 5/30 [00:00<00:02,  8.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6914 ||:  27%|██▋       | 8/30 [00:00<00:02, 10.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6887 ||:  33%|███▎      | 10/30 [00:00<00:01, 11.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6857 ||:  43%|████▎     | 13/30 [00:00<00:01, 13.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6803 ||:  53%|█████▎    | 16/30 [00:00<00:00, 15.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6823 ||:  60%|██████    | 18/30 [00:01<00:00, 16.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6822 ||:  67%|██████▋   | 20/30 [00:01<00:00, 14.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6838 ||:  73%|███████▎  | 22/30 [00:01<00:00, 15.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6800 ||:  80%|████████  | 24/30 [00:01<00:00, 16.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6790 ||:  90%|█████████ | 27/30 [00:01<00:00, 17.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6787 ||:  97%|█████████▋| 29/30 [00:01<00:00, 18.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6780 ||: 100%|██████████| 30/30 [00:01<00:00, 17.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6640 ||:  75%|███████▌  | 3/4 [00:00<00:00, 20.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6791 ||: 100%|██████████| 4/4 [00:00<00:00, 17.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.6889 ||:   7%|▋         | 2/30 [00:00<00:01, 18.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6750 ||:  17%|█▋        | 5/30 [00:00<00:01, 19.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6665 ||:  23%|██▎       | 7/30 [00:00<00:01, 19.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6632 ||:  30%|███       | 9/30 [00:00<00:01, 18.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6601 ||:  37%|███▋      | 11/30 [00:00<00:01, 18.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6673 ||:  43%|████▎     | 13/30 [00:00<00:00, 18.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6653 ||:  50%|█████     | 15/30 [00:00<00:00, 15.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6668 ||:  57%|█████▋    | 17/30 [00:00<00:00, 16.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6613 ||:  63%|██████▎   | 19/30 [00:01<00:00, 16.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6564 ||:  73%|███████▎  | 22/30 [00:01<00:00, 17.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6565 ||:  80%|████████  | 24/30 [00:01<00:00, 18.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6547 ||:  90%|█████████ | 27/30 [00:01<00:00, 18.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6591 ||:  97%|█████████▋| 29/30 [00:01<00:00, 18.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6586 ||: 100%|██████████| 30/30 [00:01<00:00, 18.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6998 ||:  75%|███████▌  | 3/4 [00:00<00:00, 22.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6755 ||: 100%|██████████| 4/4 [00:00<00:00, 18.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.6509 ||:   7%|▋         | 2/30 [00:00<00:01, 16.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6332 ||:  13%|█▎        | 4/30 [00:00<00:01, 16.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6655 ||:  20%|██        | 6/30 [00:00<00:01, 17.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6587 ||:  30%|███       | 9/30 [00:00<00:01, 17.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6564 ||:  40%|████      | 12/30 [00:00<00:00, 18.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6514 ||:  47%|████▋     | 14/30 [00:00<00:01, 15.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6502 ||:  53%|█████▎    | 16/30 [00:00<00:00, 16.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6478 ||:  60%|██████    | 18/30 [00:01<00:00, 16.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6457 ||:  70%|███████   | 21/30 [00:01<00:00, 17.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6541 ||:  80%|████████  | 24/30 [00:01<00:00, 19.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6554 ||:  90%|█████████ | 27/30 [00:01<00:00, 19.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6552 ||:  97%|█████████▋| 29/30 [00:01<00:00, 18.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6550 ||: 100%|██████████| 30/30 [00:01<00:00, 18.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6858 ||:  75%|███████▌  | 3/4 [00:00<00:00, 20.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6881 ||: 100%|██████████| 4/4 [00:00<00:00, 17.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.5771 ||:   3%|▎         | 1/30 [00:00<00:03,  8.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6182 ||:  10%|█         | 3/30 [00:00<00:02,  9.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6230 ||:  20%|██        | 6/30 [00:00<00:02, 11.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6346 ||:  30%|███       | 9/30 [00:00<00:01, 13.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6406 ||:  37%|███▋      | 11/30 [00:00<00:01, 14.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6441 ||:  43%|████▎     | 13/30 [00:00<00:01, 15.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6532 ||:  53%|█████▎    | 16/30 [00:00<00:00, 16.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6532 ||:  60%|██████    | 18/30 [00:00<00:00, 17.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6503 ||:  67%|██████▋   | 20/30 [00:01<00:00, 17.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6491 ||:  73%|███████▎  | 22/30 [00:01<00:00, 16.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6484 ||:  80%|████████  | 24/30 [00:01<00:00, 16.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6504 ||:  87%|████████▋ | 26/30 [00:01<00:00, 17.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6542 ||:  97%|█████████▋| 29/30 [00:01<00:00, 18.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6544 ||: 100%|██████████| 30/30 [00:01<00:00, 18.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6889 ||:  75%|███████▌  | 3/4 [00:00<00:00, 21.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6883 ||: 100%|██████████| 4/4 [00:00<00:00, 18.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.6237 ||:   7%|▋         | 2/30 [00:00<00:01, 18.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6285 ||:  13%|█▎        | 4/30 [00:00<00:01, 18.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6390 ||:  23%|██▎       | 7/30 [00:00<00:01, 18.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6194 ||:  30%|███       | 9/30 [00:00<00:01, 18.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6382 ||:  40%|████      | 12/30 [00:00<00:00, 18.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6383 ||:  47%|████▋     | 14/30 [00:00<00:00, 18.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6393 ||:  53%|█████▎    | 16/30 [00:00<00:00, 18.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6499 ||:  63%|██████▎   | 19/30 [00:01<00:00, 18.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6481 ||:  70%|███████   | 21/30 [00:01<00:00, 18.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6499 ||:  77%|███████▋  | 23/30 [00:01<00:00, 18.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6453 ||:  83%|████████▎ | 25/30 [00:01<00:00, 15.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6473 ||:  90%|█████████ | 27/30 [00:01<00:00, 16.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6534 ||: 100%|██████████| 30/30 [00:01<00:00, 18.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6816 ||:  75%|███████▌  | 3/4 [00:00<00:00, 21.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6925 ||: 100%|██████████| 4/4 [00:00<00:00, 18.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.7092 ||:   7%|▋         | 2/30 [00:00<00:01, 14.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6664 ||:  17%|█▋        | 5/30 [00:00<00:01, 15.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6529 ||:  23%|██▎       | 7/30 [00:00<00:01, 16.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6621 ||:  33%|███▎      | 10/30 [00:00<00:01, 18.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6658 ||:  40%|████      | 12/30 [00:00<00:00, 18.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6674 ||:  47%|████▋     | 14/30 [00:00<00:00, 18.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6693 ||:  53%|█████▎    | 16/30 [00:00<00:00, 18.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6615 ||:  60%|██████    | 18/30 [00:00<00:00, 17.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6618 ||:  70%|███████   | 21/30 [00:01<00:00, 18.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6621 ||:  77%|███████▋  | 23/30 [00:01<00:00, 18.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6599 ||:  83%|████████▎ | 25/30 [00:01<00:00, 15.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6583 ||:  90%|█████████ | 27/30 [00:01<00:00, 16.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6548 ||: 100%|██████████| 30/30 [00:01<00:00, 17.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6902 ||:  75%|███████▌  | 3/4 [00:00<00:00, 21.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6874 ||: 100%|██████████| 4/4 [00:00<00:00, 17.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.6385 ||:   3%|▎         | 1/30 [00:00<00:02,  9.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6453 ||:  13%|█▎        | 4/30 [00:00<00:02, 11.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6545 ||:  20%|██        | 6/30 [00:00<00:01, 13.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6598 ||:  30%|███       | 9/30 [00:00<00:01, 14.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6576 ||:  37%|███▋      | 11/30 [00:00<00:01, 15.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6624 ||:  43%|████▎     | 13/30 [00:00<00:01, 16.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6569 ||:  50%|█████     | 15/30 [00:00<00:00, 16.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6538 ||:  60%|██████    | 18/30 [00:00<00:00, 17.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6507 ||:  67%|██████▋   | 20/30 [00:01<00:00, 18.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6480 ||:  73%|███████▎  | 22/30 [00:01<00:00, 15.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6476 ||:  80%|████████  | 24/30 [00:01<00:00, 16.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6509 ||:  87%|████████▋ | 26/30 [00:01<00:00, 17.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6514 ||:  97%|█████████▋| 29/30 [00:01<00:00, 18.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6502 ||: 100%|██████████| 30/30 [00:01<00:00, 17.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6887 ||:  75%|███████▌  | 3/4 [00:00<00:00, 21.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6956 ||: 100%|██████████| 4/4 [00:00<00:00, 18.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.6452 ||:   7%|▋         | 2/30 [00:00<00:01, 16.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6924 ||:  13%|█▎        | 4/30 [00:00<00:01, 16.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6684 ||:  20%|██        | 6/30 [00:00<00:01, 17.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6556 ||:  27%|██▋       | 8/30 [00:00<00:01, 17.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6522 ||:  33%|███▎      | 10/30 [00:00<00:01, 17.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6555 ||:  43%|████▎     | 13/30 [00:00<00:00, 18.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6515 ||:  53%|█████▎    | 16/30 [00:00<00:00, 19.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6549 ||:  63%|██████▎   | 19/30 [00:00<00:00, 20.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6555 ||:  73%|███████▎  | 22/30 [00:01<00:00, 19.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6538 ||:  83%|████████▎ | 25/30 [00:01<00:00, 17.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6533 ||:  93%|█████████▎| 28/30 [00:01<00:00, 18.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6542 ||: 100%|██████████| 30/30 [00:01<00:00, 17.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6941 ||:  75%|███████▌  | 3/4 [00:00<00:00, 20.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6957 ||: 100%|██████████| 4/4 [00:00<00:00, 17.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.6875 ||:   3%|▎         | 1/30 [00:00<00:02,  9.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6551 ||:  10%|█         | 3/30 [00:00<00:02, 11.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6627 ||:  20%|██        | 6/30 [00:00<00:01, 13.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6408 ||:  27%|██▋       | 8/30 [00:00<00:01, 13.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6478 ||:  33%|███▎      | 10/30 [00:00<00:01, 13.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6508 ||:  40%|████      | 12/30 [00:00<00:01, 14.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6541 ||:  50%|█████     | 15/30 [00:00<00:00, 16.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6523 ||:  57%|█████▋    | 17/30 [00:01<00:00, 16.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6550 ||:  67%|██████▋   | 20/30 [00:01<00:00, 18.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6549 ||:  77%|███████▋  | 23/30 [00:01<00:00, 19.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6541 ||:  87%|████████▋ | 26/30 [00:01<00:00, 18.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6549 ||:  93%|█████████▎| 28/30 [00:01<00:00, 18.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6491 ||: 100%|██████████| 30/30 [00:01<00:00, 18.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6805 ||:  75%|███████▌  | 3/4 [00:00<00:00, 22.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6973 ||: 100%|██████████| 4/4 [00:00<00:00, 18.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.6638 ||:  10%|█         | 3/30 [00:00<00:01, 21.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6477 ||:  17%|█▋        | 5/30 [00:00<00:01, 20.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6446 ||:  27%|██▋       | 8/30 [00:00<00:01, 20.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6509 ||:  37%|███▋      | 11/30 [00:00<00:00, 20.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6520 ||:  43%|████▎     | 13/30 [00:00<00:00, 18.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6484 ||:  50%|█████     | 15/30 [00:00<00:00, 18.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6545 ||:  57%|█████▋    | 17/30 [00:00<00:00, 17.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6539 ||:  63%|██████▎   | 19/30 [00:01<00:00, 18.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6507 ||:  70%|███████   | 21/30 [00:01<00:00, 18.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6495 ||:  77%|███████▋  | 23/30 [00:01<00:00, 16.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6507 ||:  83%|████████▎ | 25/30 [00:01<00:00, 16.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6491 ||:  93%|█████████▎| 28/30 [00:01<00:00, 17.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6495 ||: 100%|██████████| 30/30 [00:01<00:00, 18.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6807 ||:  75%|███████▌  | 3/4 [00:00<00:00, 23.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.7054 ||: 100%|██████████| 4/4 [00:00<00:00, 19.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.6045 ||:   7%|▋         | 2/30 [00:00<00:01, 16.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6363 ||:  17%|█▋        | 5/30 [00:00<00:01, 17.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6358 ||:  27%|██▋       | 8/30 [00:00<00:01, 18.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6475 ||:  37%|███▋      | 11/30 [00:00<00:00, 19.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6471 ||:  43%|████▎     | 13/30 [00:00<00:00, 18.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6455 ||:  50%|█████     | 15/30 [00:00<00:00, 18.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6484 ||:  57%|█████▋    | 17/30 [00:00<00:00, 18.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6481 ||:  63%|██████▎   | 19/30 [00:00<00:00, 18.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6412 ||:  73%|███████▎  | 22/30 [00:01<00:00, 17.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6398 ||:  80%|████████  | 24/30 [00:01<00:00, 18.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6384 ||:  90%|█████████ | 27/30 [00:01<00:00, 18.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6461 ||: 100%|██████████| 30/30 [00:01<00:00, 18.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.7357 ||:  75%|███████▌  | 3/4 [00:00<00:00, 22.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6899 ||: 100%|██████████| 4/4 [00:00<00:00, 19.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/allennlp/nn/util.py:149: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n",
      "\n",
      "\n",
      "loss: 0.6144 ||:   7%|▋         | 2/30 [00:00<00:01, 14.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6486 ||:  17%|█▋        | 5/30 [00:00<00:01, 16.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6467 ||:  23%|██▎       | 7/30 [00:00<00:01, 16.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6568 ||:  30%|███       | 9/30 [00:00<00:01, 16.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6454 ||:  40%|████      | 12/30 [00:00<00:01, 16.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6536 ||:  50%|█████     | 15/30 [00:00<00:00, 18.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6518 ||:  57%|█████▋    | 17/30 [00:00<00:00, 18.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6445 ||:  63%|██████▎   | 19/30 [00:01<00:00, 18.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6515 ||:  73%|███████▎  | 22/30 [00:01<00:00, 18.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6490 ||:  83%|████████▎ | 25/30 [00:01<00:00, 19.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6488 ||:  93%|█████████▎| 28/30 [00:01<00:00, 19.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6495 ||: 100%|██████████| 30/30 [00:01<00:00, 16.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.7139 ||:  75%|███████▌  | 3/4 [00:00<00:00, 21.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "loss: 0.6970 ||: 100%|██████████| 4/4 [00:00<00:00, 18.18it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "metrics =  trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate training on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import DataIterator\n",
    "from tqdm import tqdm\n",
    "from typing import Iterable\n",
    "from allennlp.data import Instance\n",
    "from scipy.special import expit # the sigmoid function\n",
    "\n",
    "def tonp(tsr): \n",
    "    return tsr.detach().cpu().numpy()\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, \n",
    "                 iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> np.ndarray:\n",
    "        out_dict = self.model(**batch)\n",
    "        return expit(tonp(out_dict[\"class_logit\"]))\n",
    "    \n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = tqdm(pred_generator,\n",
    "                                   total=self.iterator.get_num_batches(ds))\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                preds.append(self._extract_data(batch))\n",
    "        return np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BasicIterator\n",
    "from allennlp.nn import util as nn_util\n",
    "# Iterate over the dataset without changing its order\n",
    "seq_iterator = BasicIterator(batch_size=32)\n",
    "seq_iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  7%|▋         | 2/30 [00:00<00:02, 13.24it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 13%|█▎        | 4/30 [00:00<00:01, 14.16it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 20%|██        | 6/30 [00:00<00:01, 15.31it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 27%|██▋       | 8/30 [00:00<00:01, 16.00it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 33%|███▎      | 10/30 [00:00<00:01, 16.58it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 40%|████      | 12/30 [00:00<00:01, 15.73it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 47%|████▋     | 14/30 [00:00<00:00, 16.49it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 53%|█████▎    | 16/30 [00:00<00:00, 16.93it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 60%|██████    | 18/30 [00:01<00:00, 17.17it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 67%|██████▋   | 20/30 [00:01<00:00, 17.13it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 73%|███████▎  | 22/30 [00:01<00:00, 15.84it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 80%|████████  | 24/30 [00:01<00:00, 16.59it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 87%|████████▋ | 26/30 [00:01<00:00, 16.57it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 93%|█████████▎| 28/30 [00:01<00:00, 14.27it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 30/30 [00:01<00:00, 15.35it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 2/4 [00:00<00:00, 17.05it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 17.10it/s]\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "USE_GPU = torch.cuda.is_available()\n",
    "predictor = Predictor(model, \n",
    "                      seq_iterator, \n",
    "                      cuda_device=0 if USE_GPU else -1)\n",
    "train_preds = predictor.predict(train_ds) \n",
    "test_preds = predictor.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "import pandas as pd\n",
    "\n",
    "test_ds_raw = pd.read_csv(DATA_ROOT / \"test_MJFF_English.csv\") \n",
    "def plot_roc_curve(labels,label_probs):\n",
    "\n",
    "    # Binarize the output\n",
    "    a = label_binarize(labels, classes=[0, 1])\n",
    "    b = np.logical_not(a).astype(int)\n",
    "    y_test = np.hstack((a,b))\n",
    "    n_classes = 2\n",
    "\n",
    "    # Learn to predict each class against the other\n",
    "    label_probs = np.reshape(label_probs,(-1,1))\n",
    "    y_score = np.hstack((label_probs, 1- label_probs))\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    print(y_score.shape,y_test.shape)\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    plt.figure(figsize=(7,7))\n",
    "    lw = 2\n",
    "    plt.plot(fpr[1], tpr[1], color='darkorange',\n",
    "             lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(118, 2) (118, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_roc_curve(test_ds_raw.Diagnosis,test_preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
