{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtracking dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Set path to find modelling tools for later use\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "from src.charCNN.data_utilities import create_data_objects\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pandas import read_csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters used in all typed sentences: 60\n"
     ]
    }
   ],
   "source": [
    "which_information = 'char_time'\n",
    "csv_file = 'EnglishData-preprocessed_attempt_1.csv'\n",
    "data_root = \"/home/nd/cloud/habitual_errors_NLP/data/MRC\"\n",
    "DATA_ROOT = Path(data_root)\n",
    "\n",
    "if which_information == \"char_time_space\":\n",
    "    # Get relevant long-format data\n",
    "    df = read_csv(DATA_ROOT / \"char_time\" / csv_file, header=0)\n",
    "else:\n",
    "    df = read_csv(DATA_ROOT / which_information / csv_file, header=0)\n",
    "\n",
    "# Get relevant data objects\n",
    "subject_documents, subject_locations, subjects_diagnoses, alphabet = create_data_objects(df)\n",
    "# Make training data array\n",
    "all_sentences = [item for sublist in subject_documents for item in sublist]\n",
    "all_locations = [item for sublist in subject_locations for item in sublist]\n",
    "# Store alphabet size\n",
    "alphabet_size = len(alphabet)\n",
    "print(\"Total number of characters used in all typed sentences:\", alphabet_size)\n",
    "alphabet_indices = dict((c, i) for i, c in enumerate(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '!': 1, '\"': 2, '$': 3, '%': 4, '&': 5, \"'\": 6, ',': 7, '-': 8, '.': 9, '/': 10, '0': 11, '2': 12, '3': 13, '4': 14, '5': 15, '8': 16, '9': 17, ';': 18, '=': 19, '[': 20, '\\\\': 21, ']': 22, '^': 23, '_': 24, 'a': 25, 'b': 26, 'c': 27, 'd': 28, 'e': 29, 'f': 30, 'g': 31, 'h': 32, 'i': 33, 'j': 34, 'k': 35, 'l': 36, 'm': 37, 'n': 38, 'o': 39, 'p': 40, 'q': 41, 'r': 42, 's': 43, 't': 44, 'u': 45, 'v': 46, 'w': 47, 'x': 48, 'y': 49, 'z': 50, '{': 51, '|': 52, '£': 53, 'γ': 54, 'δ': 55, 'ε': 56, 'ζ': 57, 'η': 58, 'ω': 59}\n"
     ]
    }
   ],
   "source": [
    "print(alphabet_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "1. Push to repo\n",
    "2. Run new job on Liverpool machine\n",
    "2a. Make sure that the dictionary used is printed\n",
    "3. Check that the dict is deterministic\n",
    "4. Send new results to Mathias\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
