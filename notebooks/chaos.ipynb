{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.4\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "\n",
    "# Set path to find modelling tools for later use\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "\n",
    "from haberrspd.preprocess import preprocessMJFF, preprocessMRC\n",
    "from haberrspd.charCNN.data_utilities import create_training_data_keras, create_data_objects\n",
    "                         \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from scipy.stats import (gamma, lognorm, gengamma)\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# Plot stuff\n",
    "import seaborn as sns\n",
    "from scipy.constants import golden\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path(\"../data\") / 'MJFF' / 'preproc' / 'char' / 'SpanishData-preprocessed.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = defaultdict(list)\n",
    "y = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in df.Patient_ID.unique():\n",
    "    X[s] = df[(df.Patient_ID == s)]['Preprocessed_typed_sentence'].tolist()\n",
    "    y[s] = df[(df.Patient_ID == s)]['Diagnosis'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_documents, subject_locations, subjects_diagnoses, alphabet = create_data_objects(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters used in all typed sentences: 54\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "def roundup(x):\n",
    "    return int(ceil(x / 100.0)) * 100\n",
    "all_sentences = [item for sublist in subject_documents for item in sublist]\n",
    "max_sentence_length = roundup(max([len(s) for s in all_sentences]))\n",
    "# Store alphabet size\n",
    "alphabet_size = len(alphabet)\n",
    "print(\"Total number of characters used in all typed sentences:\", alphabet_size)\n",
    "alphabet_indices = dict((c, i) for i, c in enumerate(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise tokenizer which maps characters to integers\n",
    "tk = Tokenizer(num_words=None, char_level=True)\n",
    "# Fit to text: convert all chars to ints\n",
    "tk.fit_on_texts(all_sentences)\n",
    "# Update alphabet\n",
    "tk.word_index = alphabet_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_int = defaultdict(list)\n",
    "for s in df.Patient_ID.unique():\n",
    "    # Get integer sequences: converts sequences of chars to sequences of ints\n",
    "    X_int[s] = tk.texts_to_sequences(\" \".join(X[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = defaultdict(list)\n",
    "yy = defaultdict(list)\n",
    "for i,s in enumerate(df.Patient_ID.unique()):\n",
    "    XX[i] = to_categorical(X_int[s])\n",
    "    yy[i] = y[s][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3443, 54)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([100, 110, 121, 130, 132, 134, 142, 147, 148, 153, 154, 157, 159, 164, 165, 166, 170, 171, 172, 173])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([100, 110, 121, 130, 132, 134, 142, 147, 148, 153, 154, 157, 159, 164, 165, 166, 170, 171, 172, 173])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(list)\n",
    "for s in df.Patient_ID.unique():\n",
    "    data[s] = (XX[s],y[s][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "class DataHandler(object):\n",
    "    \n",
    "    def __init__(self, X, y, window_len, n_samp):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.window_len = window_len\n",
    "        self.n_samp=n_samp\n",
    "        \n",
    "        \n",
    "    def get_sample(self,idx):\n",
    "        x_len = self.X[idx].shape[0]\n",
    "        window_start = randint(0,x_len - self.window_len)\n",
    "        window_end = window_start + self.window_len\n",
    "        return self.X[idx][window_start:window_end], self.y[idx]\n",
    "    \n",
    "    \n",
    "    def gen_epoch(self):\n",
    "        X_epoch = []\n",
    "        y_epoch = []\n",
    "        \n",
    "        for idx in range(len(self.X)):\n",
    "            \n",
    "            for s in range(self.n_samp):\n",
    "                x,y = self.get_sample(idx)\n",
    "                X_epoch.append(x)\n",
    "                y_epoch.append(y)\n",
    "                \n",
    "                \n",
    "                \n",
    "        return np.asarray(X_epoch), np.asarray(y_epoch)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX_list = []\n",
    "yy_list = []\n",
    "\n",
    "for key, _ in XX.items():\n",
    "    XX_list.append(XX[key])\n",
    "    yy_list.append(yy[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datahandler = DataHandler(X=XX_list[4:], y=yy_list[4:],window_len = 60,n_samp = 3)\n",
    "val_datahandler = DataHandler(X=XX_list[:4], y=yy_list[:4],window_len = 60,n_samp = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_datahandler.gen_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 60, 54)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "model = load_model('EXPERIMENTAL_MODE.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1_16:0' shape=(?, 60, 54) dtype=float32>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.set_value(model.optimizer.lr, 1e-3)\n",
    "K.get_value(model.optimizer.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'binary_crossentropy'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.ones_like(X)\n",
    "test2 = np.zeros_like(X)\n",
    "X_test = np.concatenate([test1,test2])\n",
    "\n",
    "y_test = np.concatenate([np.ones_like(y),np.zeros_like(y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6842 - acc: 0.5625 - val_loss: 0.6959 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 867us/sample - loss: 0.6857 - acc: 0.5625 - val_loss: 0.6960 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 838us/sample - loss: 0.6881 - acc: 0.5625 - val_loss: 0.6961 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 896us/sample - loss: 0.6889 - acc: 0.5625 - val_loss: 0.6962 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 816us/sample - loss: 0.6884 - acc: 0.5625 - val_loss: 0.6962 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 873us/sample - loss: 0.6892 - acc: 0.5625 - val_loss: 0.6963 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 862us/sample - loss: 0.6869 - acc: 0.5625 - val_loss: 0.6964 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 808us/sample - loss: 0.6879 - acc: 0.5625 - val_loss: 0.6965 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 918us/sample - loss: 0.6897 - acc: 0.5625 - val_loss: 0.6966 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 835us/sample - loss: 0.6885 - acc: 0.5625 - val_loss: 0.6966 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6875 - acc: 0.5625 - val_loss: 0.6967 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6914 - acc: 0.5625 - val_loss: 0.6967 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6889 - acc: 0.5625 - val_loss: 0.6967 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6882 - acc: 0.5625 - val_loss: 0.6967 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6882 - acc: 0.5625 - val_loss: 0.6967 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6904 - acc: 0.5625 - val_loss: 0.6968 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6862 - acc: 0.5625 - val_loss: 0.6968 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6847 - acc: 0.5625 - val_loss: 0.6969 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6869 - acc: 0.5625 - val_loss: 0.6970 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6874 - acc: 0.5625 - val_loss: 0.6970 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6895 - acc: 0.5625 - val_loss: 0.6971 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6893 - acc: 0.5625 - val_loss: 0.6970 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 2ms/sample - loss: 0.6834 - acc: 0.5625 - val_loss: 0.6970 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6844 - acc: 0.5625 - val_loss: 0.6969 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6838 - acc: 0.5625 - val_loss: 0.6969 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6875 - acc: 0.5625 - val_loss: 0.6970 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6813 - acc: 0.5625 - val_loss: 0.6970 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6837 - acc: 0.5625 - val_loss: 0.6970 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6818 - acc: 0.5625 - val_loss: 0.6971 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6895 - acc: 0.5625 - val_loss: 0.6971 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 950us/sample - loss: 0.6844 - acc: 0.5625 - val_loss: 0.6972 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6821 - acc: 0.5625 - val_loss: 0.6972 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 980us/sample - loss: 0.6812 - acc: 0.5625 - val_loss: 0.6973 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6874 - acc: 0.5625 - val_loss: 0.6974 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6885 - acc: 0.5625 - val_loss: 0.6974 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6842 - acc: 0.5625 - val_loss: 0.6975 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6875 - acc: 0.5625 - val_loss: 0.6976 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6869 - acc: 0.5625 - val_loss: 0.6976 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6879 - acc: 0.5625 - val_loss: 0.6976 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6889 - acc: 0.5625 - val_loss: 0.6977 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6846 - acc: 0.5625 - val_loss: 0.6977 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6852 - acc: 0.5625 - val_loss: 0.6977 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6839 - acc: 0.5625 - val_loss: 0.6978 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6841 - acc: 0.5625 - val_loss: 0.6979 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 913us/sample - loss: 0.6852 - acc: 0.5625 - val_loss: 0.6980 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 910us/sample - loss: 0.6876 - acc: 0.5625 - val_loss: 0.6981 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 972us/sample - loss: 0.6861 - acc: 0.5625 - val_loss: 0.6982 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 780us/sample - loss: 0.6882 - acc: 0.5625 - val_loss: 0.6983 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 890us/sample - loss: 0.6842 - acc: 0.5625 - val_loss: 0.6984 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 912us/sample - loss: 0.6820 - acc: 0.5625 - val_loss: 0.6985 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 852us/sample - loss: 0.6808 - acc: 0.5625 - val_loss: 0.6986 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 842us/sample - loss: 0.6868 - acc: 0.5625 - val_loss: 0.6987 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 958us/sample - loss: 0.6855 - acc: 0.5625 - val_loss: 0.6988 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 874us/sample - loss: 0.6865 - acc: 0.5625 - val_loss: 0.6990 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 879us/sample - loss: 0.6854 - acc: 0.5625 - val_loss: 0.6991 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 904us/sample - loss: 0.6861 - acc: 0.5625 - val_loss: 0.6991 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 982us/sample - loss: 0.6873 - acc: 0.5625 - val_loss: 0.6992 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 908us/sample - loss: 0.6897 - acc: 0.5625 - val_loss: 0.6993 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 962us/sample - loss: 0.6849 - acc: 0.5625 - val_loss: 0.6993 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 849us/sample - loss: 0.6897 - acc: 0.5625 - val_loss: 0.6994 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 861us/sample - loss: 0.6840 - acc: 0.5625 - val_loss: 0.6994 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 963us/sample - loss: 0.6854 - acc: 0.5625 - val_loss: 0.6994 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6846 - acc: 0.5625 - val_loss: 0.6994 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6871 - acc: 0.5625 - val_loss: 0.6993 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 851us/sample - loss: 0.6884 - acc: 0.5625 - val_loss: 0.6993 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 793us/sample - loss: 0.6885 - acc: 0.5625 - val_loss: 0.6993 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 839us/sample - loss: 0.6828 - acc: 0.5625 - val_loss: 0.6993 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 815us/sample - loss: 0.6899 - acc: 0.5625 - val_loss: 0.6994 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 969us/sample - loss: 0.6858 - acc: 0.5625 - val_loss: 0.6995 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 982us/sample - loss: 0.6837 - acc: 0.5625 - val_loss: 0.6997 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6848 - acc: 0.5625 - val_loss: 0.6998 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6834 - acc: 0.5625 - val_loss: 0.7000 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6814 - acc: 0.5625 - val_loss: 0.7003 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6811 - acc: 0.5625 - val_loss: 0.7004 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6805 - acc: 0.5625 - val_loss: 0.7005 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6854 - acc: 0.5625 - val_loss: 0.7006 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6875 - acc: 0.5625 - val_loss: 0.7007 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6895 - acc: 0.5625 - val_loss: 0.7009 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6846 - acc: 0.5625 - val_loss: 0.7010 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6872 - acc: 0.5625 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6892 - acc: 0.5625 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 902us/sample - loss: 0.6908 - acc: 0.5625 - val_loss: 0.7010 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 893us/sample - loss: 0.6815 - acc: 0.5625 - val_loss: 0.7010 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 798us/sample - loss: 0.6784 - acc: 0.5625 - val_loss: 0.7009 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 816us/sample - loss: 0.6856 - acc: 0.5625 - val_loss: 0.7008 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 826us/sample - loss: 0.6791 - acc: 0.5625 - val_loss: 0.7009 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 811us/sample - loss: 0.6845 - acc: 0.5625 - val_loss: 0.7009 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 1ms/sample - loss: 0.6833 - acc: 0.5625 - val_loss: 0.7010 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 767us/sample - loss: 0.6831 - acc: 0.5625 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 797us/sample - loss: 0.6788 - acc: 0.5625 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 792us/sample - loss: 0.6841 - acc: 0.5625 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 779us/sample - loss: 0.6769 - acc: 0.5625 - val_loss: 0.7011 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 822us/sample - loss: 0.6818 - acc: 0.5625 - val_loss: 0.7012 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 758us/sample - loss: 0.6822 - acc: 0.5625 - val_loss: 0.7013 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 838us/sample - loss: 0.6788 - acc: 0.5625 - val_loss: 0.7014 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 828us/sample - loss: 0.6824 - acc: 0.5625 - val_loss: 0.7015 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 799us/sample - loss: 0.6863 - acc: 0.5625 - val_loss: 0.7015 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 873us/sample - loss: 0.6797 - acc: 0.5625 - val_loss: 0.7016 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 786us/sample - loss: 0.6863 - acc: 0.5625 - val_loss: 0.7017 - val_acc: 0.5000\n",
      "Train on 48 samples, validate on 400 samples\n",
      "48/48 [==============================] - 0s 787us/sample - loss: 0.6894 - acc: 0.5625 - val_loss: 0.7017 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n_epochs = 100\n",
    "X_val, y_val = val_datahandler.gen_epoch()\n",
    "X, y = train_datahandler.gen_epoch() \n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    model.fit(X,y, validation_data=(X_val,y_val),batch_size=32,epochs = 1,shuffle = True )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_4:0\", shape=(?, 100, 54), dtype=float32) at layer \"input\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-08f0950883b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_of_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Build model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adam, categorical_crossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 231\u001b[0;31m             self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1441\u001b[0m                                          \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                          \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                          str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1444\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_4:0\", shape=(?, 100, 54), dtype=float32) at layer \"input\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# =====================Char CNN=======================\n",
    "# parameter\n",
    "input_size = 1014\n",
    "vocab_size = len(tk.word_index)\n",
    "embedding_size = 54\n",
    "conv_layers = [[256, 7, 3],\n",
    "               [256, 3, 3]]\n",
    "\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = 4\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "\n",
    "\n",
    "# Model Construction\n",
    "# Input\n",
    "input_shape = (100,54)\n",
    "x = Input(shape=input_shape, name='input', dtype='float32')  # shape=(?, 1014)\n",
    "# Embedding\n",
    "\n",
    "# Conv\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x)  # Final shape=(None, 34, 256)\n",
    "x = Flatten()(x)  # (None, 8704)\n",
    "# Fully connected layers\n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)  # dense_size == 1024\n",
    "    x = Dropout(dropout_p)(x)\n",
    "# Output Layer\n",
    "predictions = Dense(num_of_classes, activation='softmax')(x)\n",
    "# Build model\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])  # Adam, categorical_crossentropy\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(?, 1014) dtype=int64>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
