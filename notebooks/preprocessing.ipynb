{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.4\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "\n",
    "# Set path to find modelling tools for later use\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "\n",
    "\n",
    "from haberrspd.preprocess import preprocessMJFF\n",
    "                         \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from scipy.stats import (gamma, lognorm, gengamma)\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# Plot stuff\n",
    "import seaborn as sns\n",
    "from scipy.constants import golden\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "\n",
    "data_root = '../data/MJFF/' # My local path\n",
    "data_root = Path(data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character + Timing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = preprocessMJFF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = proc('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(\"../data/MJFF/preproc/EnglishSpanishData-preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = proc('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(\"../data/MJFF/preproc/SpanishData-preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = proc('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(\"../data/MJFF/preproc/EnglishData-preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[out['Preprocessed_typed_sentence'].apply(lambda x: len(x) > 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(data_root / 'SpanishData-duplicateeventsremoved.csv')\n",
    "df = pd.read_csv(data_root / 'SpanishData.csv')\n",
    "df_meta = pd.read_csv(data_root / \"SpanishParticipantKey.csv\",\n",
    "                      index_col=0,\n",
    "                      header=0,\n",
    "                      names=['participant_id', 'diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character ONLY data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = preprocessMJFF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = proc('english',include_time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[out['Preprocessed_typed_sentence'].apply(lambda x: len(x) < 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(\"../data/MJFF/preproc/char/EnglishSpanishData-preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = set(df_meta.loc[df_meta.diagnosis == 0].participant_id)\n",
    "pd_subjects = set(df_meta.loc[df_meta.diagnosis == 1].participant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_timestamp_diffs = []\n",
    "# Loop over all subjects\n",
    "for sub in pd_subjects:\n",
    "    # Get all delta timestamps for this sentence, across all subjects\n",
    "    pd_timestamp_diffs.extend(df.loc[(df.sentence_id == 57) & (df.participant_id == sub)].timestamp.diff().values)\n",
    "    \n",
    "control_timestamp_diffs = []\n",
    "# Loop over all subjects\n",
    "for sub in controls:\n",
    "    # Get all delta timestamps for this sentence, across all subjects\n",
    "    control_timestamp_diffs.extend(df.loc[(df.sentence_id == 57) & (df.participant_id == sub)].timestamp.diff().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaNs\n",
    "pd_cleaned_list = [x for x in pd_timestamp_diffs if str(x) != 'nan']\n",
    "control_cleaned_list = [x for x in control_timestamp_diffs if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PD\n",
    "\n",
    "# fixed bin size\n",
    "bins = np.arange(0, 10000, 50) # fixed bin size\n",
    "# plt.xlim([min(data)-5, max(data)+5])\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "plt.hist(pd_cleaned_list, bins=bins, alpha=0.5)\n",
    "# plt.vlines(np.quantile(cleaned_list,0.95),0,900,'r')\n",
    "# plt.vlines(np.mean(cleaned_list),0,900,'b')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control\n",
    "\n",
    "# fixed bin size\n",
    "bins = np.arange(0, 10000, 50) # fixed bin size\n",
    "# plt.xlim([min(data)-5, max(data)+5])\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "plt.hist(control_cleaned_list, bins=bins, alpha=0.5)\n",
    "# plt.vlines(np.quantile(cleaned_list,0.95),0,900,'r')\n",
    "# plt.vlines(np.mean(cleaned_list),0,900,'b')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mechanical turk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(data_root /'preprocessed_MechanicalTurkCombinedEnglishData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../data/MJFF/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IKI extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = create_mjff_iki_training_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyboard inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.charCNN.data_utils_tf import create_mjff_data_objects, us_standard_layout_keyboard, english_keys_to_2d_coordinates\n",
    "import keras.backend as K\n",
    "from keras import callbacks\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array, int64, ones, hstack, pad, einsum, dstack\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import cast, float32, one_hot\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_information = \"char_time_space\"\n",
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\"\n",
    "data_string = \"EnglishData-preprocessed.csv\"\n",
    "if which_information == \"char_time_space\":\n",
    "    # Get relevant long-format data\n",
    "    which_information = \"char_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv(DATA_ROOT / which_information / data_string, header=0)  # MJFF data\n",
    "subject_documents, subjects_diagnoses, alphabet = create_mjff_data_objects(df)\n",
    "\n",
    "# Store alphabet size\n",
    "alphabet_size = len(alphabet)\n",
    "\n",
    "print('Total number of characters:', alphabet_size)\n",
    "alphabet_indices = dict((c, i) for i, c in enumerate(alphabet))\n",
    "\n",
    "if which_information == \"char_time\" or which_information == \"char_time_space\":\n",
    "    # Rounds (up) to nearest thousand\n",
    "    max_sentence_length = round(df.Preprocessed_typed_sentence.apply(lambda x: len(x)).max(), -3)\n",
    "if which_information == \"char\":\n",
    "    # Rounds (up) to nearest hundred\n",
    "    max_sentence_length = round(df.Preprocessed_typed_sentence.apply(lambda x: len(x)).max(), -2)\n",
    "\n",
    "# Make training data array\n",
    "all_sentences = [item for sublist in subject_documents for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise tokenizer which maps characters to integers\n",
    "tk = Tokenizer(num_words=None, char_level=True)\n",
    "\n",
    "# Fit to text: convert all chars to ints\n",
    "tk.fit_on_texts(all_sentences)\n",
    "\n",
    "# Update alphabet\n",
    "tk.word_index = alphabet_indices\n",
    "\n",
    "# Get integer sequences: converts sequences of chars to sequences of ints\n",
    "int_sequences = tk.texts_to_sequences(all_sentences)\n",
    "\n",
    "# Pad sequences so that they all have the same length and then one-hot encode\n",
    "X = to_categorical(pad_sequences(int_sequences, maxlen=max_sentence_length, padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_information  =  'char_time_space'\n",
    "if which_information == 'char_time_space':\n",
    "    # Load relevant keyboard\n",
    "    keyboard = us_standard_layout_keyboard()  # OBS: nested list\n",
    "    # Check that all chars are in fact in our \"keyboard\" -- if not, we cannot map a coordinate\n",
    "    assert alphabet.issubset(set(list(itertools.chain.from_iterable(keyboard))))\n",
    "    space = [english_keys_to_2d_coordinates(sentence, keyboard) for sentence in all_sentences]\n",
    "    space_padded = [pad(s, [(0, max_sentence_length - len(s)), (0, 0)], mode='constant') for s in space]\n",
    "    # Append coordinates to one-hot encoded sentences\n",
    "    X = einsum('ijk->kij', dstack([hstack((x, s)) for (x, s) in zip(X, space_padded)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document (participant) -level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three options (at time of writing):\n",
    "\n",
    "1. Submit each sentence to model and extract classification probability for each sentence, agglomorate at the end, and the conduct a classification on the vector of all 15 probabilities.\n",
    "2. Calculate the expected value of all encoded (15) sentences and then pass this to the model and take the classification.\n",
    "3. Vertically stack all embedded sentences, and let the convolution run over this (very long) array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.charCNN.data_utils_tf import create_training_data_keras, create_mjff_data_objects\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_information = \"char_time\"\n",
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\"\n",
    "data_string = \"EnglishData-preprocessed.csv\"\n",
    "df = read_csv(DATA_ROOT / which_information / data_string, header=0)  # MJFF data\n",
    "# subject_documents, subjects_diagnoses, alphabet = create_mjff_data_objects(df)\n",
    "# X_train, X_test, y_train, y_test, max_sentence_length, alphabet_size = create_training_data_keras(DATA_ROOT, which_information, data_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in subject_documents[:3]:\n",
    "    print(doc[:2])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to find modelling tools for later use\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from haberrspd.preprocess import (clean_mrc,\n",
    "                                  sentence_level_pause_correction,\n",
    "                                  backspace_corrector, \n",
    "                                  flatten,\n",
    "                                  calculate_edit_distance_between_response_and_target_MRC)\n",
    "from haberrspd.charCNN.data_utils_tf import us_english_keyboard_mrc\n",
    "from numpy import concatenate\n",
    "from typing import Tuple\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "from itertools import compress, count, groupby\n",
    "from operator import itemgetter\n",
    "import copy\n",
    "\n",
    "# Monster machine\n",
    "data_root = Path('../data/MRC/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nd/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_root / \"CombinedTypingDataNov21-duplicateeventsremoved.csv\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some entries (indexed by the pair subject and sentence) have multiple copies of the same typed sentence stored onto the same index. This section removes those duplicates and stores a new file which has one unique typed sentence per index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.preprocess import remove_superfluous_reponse_id_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file CombinedTypingDataSept27.csv as df\n",
    "\n",
    "# Run\n",
    "# remove_superfluous_reponse_id_rows(df)\n",
    "\n",
    "# Save\n",
    "# df.to_csv(data_root / \"CombinedTypingDataNov21-duplicateeventsremoved.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removal of sentences with 'high' Levenshtein distance...\n",
      "\n",
      "Size of dataframe before row pruning: (790643, 12)\n",
      "Size of dataframe after row pruning: (790148, 12)\n",
      "\n",
      "Removal of sentences with left/right arrows keys...\n",
      "\n",
      "Size of dataframe before row pruning: (790148, 12)\n",
      "Size of dataframe after row pruning: (758744, 12)\n"
     ]
    }
   ],
   "source": [
    "out = clean_mrc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle(Path('../data/MRC/') / 'cleaned_mrc.pkl')\n",
    "# In-place dropping of keyup rows\n",
    "df.drop(df.index[(df.type == \"keyup\")], inplace=True)\n",
    "# Reset index so that we can sort it properly in the next step\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_IKI_R_calc = defaultdict(list)\n",
    "ii = 0\n",
    "for i in range(len(df))[1:]:\n",
    "    if (df.loc[i-1, \"sentence_id\"] == 1) == (df.loc[i, \"sentence_id\"] == 1) & (df.loc[i-1,\"participant_id\"] == df.loc[i,\"participant_id\"]):\n",
    "        # Same sentence ID and same participant ID\n",
    "        all_IKI_R_calc[df.loc[i,\"participant_id\"]].append(df.loc[i, \"timestamp\"] - df.loc[i-1, \"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_IKI_R_calc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {1: (2064.0, 20810),\n",
       "             2: (2000.0, 18762),\n",
       "             3: (1790.6949999739882, 22887),\n",
       "             4: (1823.900000017995, 19049),\n",
       "             5: (1680.0000000519503, 18513),\n",
       "             6: (1719.809999922989, 28298),\n",
       "             7: (1572.100000000035, 27566),\n",
       "             8: (1718.0899999809917, 29777),\n",
       "             9: (1919.82499999, 29700),\n",
       "             10: (1656.0, 23966),\n",
       "             11: (1806.8749999989523, 25706),\n",
       "             12: (1615.9450000000652, 31414),\n",
       "             13: (1687.0, 30449),\n",
       "             14: (1538.3649999999907, 24306),\n",
       "             15: (1600.5, 29108)})"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat = defaultdict(dict)\n",
    "# Loop over all subjects which have typed this sentence\n",
    "for sent in df.sentence_id.unique():\n",
    "    tmp = []\n",
    "    for subject in df.loc[(df.sentence_id == sent)].participant_id.unique():\n",
    "        # Get all delta timestamps for this sentence_level_pause_correction, across all subjects\n",
    "        bajs = df.loc[(df.sentence_id == sent) & (df.participant_id == subject)].timestamp.diff()[1:]\n",
    "        # Append to get statistics over all participants\n",
    "        tmp.extend(bajs)\n",
    "    stat[sent] = (np.percentile(np.array(tmp),99,interpolation='lower'),len(tmp))\n",
    "\n",
    "stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.array(tmp),bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: (1306.4601992895189, 312.5746008904562),\n",
       " 2: (1364.8994170364967, 328.6095374940683),\n",
       " 3: (1182.4900805574543, 318.0734980952354),\n",
       " 4: (1262.980299303649, 321.35385545801665),\n",
       " 5: (1772.3117371819153, 413.46921513134305),\n",
       " 6: (1306.229145699725, 312.6193342933887),\n",
       " 7: (1079.251618715184, 293.5182280849008),\n",
       " 8: (1149.9267595651233, 299.9284354804538),\n",
       " 9: (1239.7947729204416, 317.64031188000985),\n",
       " 10: (1180.9260877541903, 299.4039406664668),\n",
       " 11: (1205.0930181731587, 297.0645406615338),\n",
       " 12: (1100.975547571192, 296.35147347976505),\n",
       " 13: (1169.4416477479342, 296.679986942157),\n",
       " 14: (1004.799480000814, 288.0282207162526),\n",
       " 15: (1025.509363768798, 291.13882435869743)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = sentence_level_pause_correction(df)\n",
    "out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removal of sentences with 'high' Levenshtein distance...\n",
      "\n",
      "Size of dataframe before row pruning: (790643, 12)\n",
      "Size of dataframe after row pruning: (790148, 12)\n",
      "\n",
      "Removal of sentences with left/right arrows keys...\n",
      "\n",
      "Size of dataframe before row pruning: (790148, 12)\n",
      "Size of dataframe after row pruning: (758744, 12)\n"
     ]
    }
   ],
   "source": [
    "out = clean_mrc(df)\n",
    "# out.head()\n",
    "# df = copy.copy(out)\n",
    "# Pickle processed data\n",
    "out.to_pickle(data_root / 'processed_mcr.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle stored file\n",
    "df = pd.read_pickle(data_root / 'processed_mcr.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol to process the MRC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A form of `create_char_compression_time_mjff_data` [to get the temporal data] <-- this needs to be set so that we get a list of \n",
    "2. Second use `create_dataframe_from_processed_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.preprocess import (combine_contiguous_shift_keydowns_without_matching_keyup, \n",
    "                                  assess_repeating_key_compression_pattern, \n",
    "                                  remove_solitary_key_presses,\n",
    "                                  sentence_level_pause_correction,\n",
    "                                  make_character_compression_time_sentence_mrc)\n",
    "from collections import defaultdict\n",
    "from itertools import count \n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# Baseline\n",
    "from haberrspd.preprocess_baseline import calculate_iki_and_ed_baseline, reference_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IKI and ED baseline calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(Path('../data/MRC/') / 'processed_mcr.pkl')\n",
    "out = sentence_level_pause_correction(df)\n",
    "output = open(data_root / 'sentence_level_pause_correct_mrc.pkl', 'wb')\n",
    "pickle.dump(out, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(data_root / 'sentence_level_pause_correct_mrc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MJFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"../data/\")\n",
    "df = pd.read_csv(data_root / 'MJFF' / 'raw' / 'EnglishData-duplicateeventsremoved.csv')\n",
    "\n",
    "out = sentence_level_pause_correction(df)\n",
    "\n",
    "output = open(data_root / 'sentence_level_pause_correct_mjff.pkl', 'wb')\n",
    "# pickle.dump(mydict, output)\n",
    "pickle.dump(out, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression times attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentences_from_raw_typing_mrc(df: pd.DataFrame, \n",
    "                                          make_long_format=True,\n",
    "                                          time_redux_fact=10) -> Tuple[dict, list]:\n",
    "\n",
    "    fail = 0\n",
    "    success = 0\n",
    "    corrected_sentences = defaultdict(dict)\n",
    "    broken_sentences = defaultdict(dict)\n",
    "    char_compression_sentences = defaultdict(dict)\n",
    "    for subj_idx in df.participant_id.unique():\n",
    "        # Not all subjects have typed all sentences hence we have to do it this way\n",
    "        print(\"\\n\\t>>>This is subject: %i.\" % subj_idx)\n",
    "        for sent_idx in df.loc[(df.participant_id == subj_idx)].sentence_id.unique():\n",
    "            print(sent_idx, end=\" \")\n",
    "            \n",
    "            # Locate df segment to extract\n",
    "            coordinates = (df.participant_id == subj_idx) & (df.sentence_id == sent_idx)\n",
    "\n",
    "            # Store temporary dataframe because why not\n",
    "            df_view = df.loc[coordinates, (\"key\", \"timestamp\", \"type\")].reset_index(drop=True).copy()  # Reset index\n",
    "\n",
    "            # Action order:\n",
    "            #     2. Remove contiguous shifts\n",
    "            #     0. Sort dataset\n",
    "            #     1. Implement backspaces\n",
    "            #     (3. Remove solitary keys)\n",
    "\n",
    "            # Removes contiguous shift presses\n",
    "            df_view = combine_contiguous_shift_keydowns_without_matching_keyup(df_view)\n",
    "            \n",
    "            # Get correctly ordered sentences and total compression times\n",
    "            df_view = move_keys_to_temporal_monotonically_increasing_order(df_view)\n",
    "\n",
    "            # Method to 'implement' the users' backspace actions\n",
    "            new_backspace_implementer_mrc(df_view)\n",
    "\n",
    "            if [i for i in Counter(df_view.key).most_common() if i[1] % 2 != 0]:\n",
    "                # Remove solitary key-presses which do not have a matching keyup or keydown\n",
    "                remove_solitary_key_presses(df_view) # TODO: not sure if we need this.\n",
    "\n",
    "            # Check what we managed to achieve\n",
    "            if assess_repeating_key_compression_pattern(df_view.type.tolist()):\n",
    "\n",
    "                # Condition succeeds: data-collection is fixed\n",
    "                corrected_sentences[subj_idx][sent_idx] = df_view\n",
    "                success += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Condition fails: data-collection is broken\n",
    "                broken_sentences[subj_idx][sent_idx] = df_view\n",
    "                fail += 1\n",
    "                print(\"\\n\\t[broken sentence] Participant: {}, Sentence: {}\".format(subj_idx, sent_idx))\n",
    "                \n",
    "        print()\n",
    "\n",
    "#     for subj_idx in corrected_sentences.keys():\n",
    "#         # Not all subjects have typed all sentences hence we have to do it this way\n",
    "#         for sent_idx in corrected_sentences[subj_idx].keys():\n",
    "#             if make_long_format:\n",
    "#                 # Final long-format sentences stored here\n",
    "#                 char_compression_sentences[subj_idx][sent_idx] = \"\".join(\n",
    "#                     make_character_compression_time_sentence_mrc(\n",
    "#                         corrected_sentences[subj_idx][sent_idx], time_redux_fact=time_redux_fact\n",
    "#                     )\n",
    "#                 )\n",
    "#             else:\n",
    "#                 # We do not use the time-dimension and look only at the spatial component\n",
    "#                 # Final long-format sentences stored here\n",
    "#                 char_compression_sentences[subj_idx][sent_idx] = \"\".join(\n",
    "#                     corrected_sentences[subj_idx][sent_idx].key[::2]\n",
    "#                 )  # [::2] takes into account that we only want one of the keydown-keyup pair.\n",
    "\n",
    "    print(\"Percentage failed: {}\".format(round(100 * (fail / (success + fail)), 2)))\n",
    "    print(fail, success)\n",
    "\n",
    "    return char_compression_sentences, broken_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t>>>This is subject: 1010.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1011.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 \n",
      "\t[broken sentence] Participant: 1011, Sentence: 12\n",
      "13 14 \n",
      "\t[broken sentence] Participant: 1011, Sentence: 14\n",
      "\n",
      "\n",
      "\t>>>This is subject: 1012.\n",
      "1 2 5 3 \n",
      "\t[broken sentence] Participant: 1012, Sentence: 3\n",
      "4 6 \n",
      "\t[broken sentence] Participant: 1012, Sentence: 6\n",
      "7 8 9 10 11 \n",
      "\t[broken sentence] Participant: 1012, Sentence: 11\n",
      "12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1014.\n",
      "1 2 5 3 4 6 \n",
      "\t[broken sentence] Participant: 1014, Sentence: 6\n",
      "7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1013.\n",
      "1 2 5 3 4 6 7 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1015.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1019.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1016.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1018.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1020.\n",
      "1 2 5 3 4 6 7 8 9 10 12 \n",
      "\t[broken sentence] Participant: 1020, Sentence: 12\n",
      "13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1022.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1021.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1024.\n",
      "1 \n",
      "\t[broken sentence] Participant: 1024, Sentence: 1\n",
      "5 3 \n",
      "\t[broken sentence] Participant: 1024, Sentence: 3\n",
      "4 6 7 \n",
      "\t[broken sentence] Participant: 1024, Sentence: 7\n",
      "8 9 \n",
      "\t[broken sentence] Participant: 1024, Sentence: 9\n",
      "10 11 12 \n",
      "\t[broken sentence] Participant: 1024, Sentence: 12\n",
      "13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1023.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1026.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1025.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1027.\n",
      "2 5 3 4 6 7 8 9 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1030.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1031.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1033.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1034.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1035.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1037.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1038.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1039.\n",
      "1 5 6 7 8 9 10 14 \n",
      "\n",
      "\t>>>This is subject: 1040.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1041.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1042.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1043.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1044.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 15 \n",
      "\n",
      "\t>>>This is subject: 1045.\n",
      "2 5 3 4 6 7 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1046.\n",
      "1 2 5 3 4 6 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1047.\n",
      "1 2 5 3 4 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1048.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1049.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1051.\n",
      "1 2 5 3 4 6 7 8 9 10 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1052.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1053.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1054.\n",
      "1 2 5 3 4 6 7 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1055.\n",
      "1 2 5 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1056.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1057.\n",
      "1 2 5 3 \n",
      "\n",
      "\t>>>This is subject: 1059.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1063.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1064.\n",
      "2 5 3 6 7 8 10 11 12 14 15 \n",
      "\n",
      "\t>>>This is subject: 1065.\n",
      "1 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1066.\n",
      "2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1067.\n",
      "1 2 5 3 4 6 7 8 9 \n",
      "\t[broken sentence] Participant: 1067, Sentence: 9\n",
      "10 11 12 13 14 15 \n",
      "\t[broken sentence] Participant: 1067, Sentence: 15\n",
      "\n",
      "\n",
      "\t>>>This is subject: 1068.\n",
      "1 2 5 \n",
      "\t[broken sentence] Participant: 1068, Sentence: 5\n",
      "3 4 \n",
      "\t[broken sentence] Participant: 1068, Sentence: 4\n",
      "6 7 8 9 10 11 12 \n",
      "\t[broken sentence] Participant: 1068, Sentence: 12\n",
      "13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1069.\n",
      "1 2 5 3 \n",
      "\t[broken sentence] Participant: 1069, Sentence: 3\n",
      "4 6 7 8 \n",
      "\t[broken sentence] Participant: 1069, Sentence: 8\n",
      "9 10 11 12 13 \n",
      "\t[broken sentence] Participant: 1069, Sentence: 13\n",
      "14 \n",
      "\t[broken sentence] Participant: 1069, Sentence: 14\n",
      "15 \n",
      "\t[broken sentence] Participant: 1069, Sentence: 15\n",
      "\n",
      "\n",
      "\t>>>This is subject: 1071.\n",
      "1 2 \n",
      "\t[broken sentence] Participant: 1071, Sentence: 2\n",
      "5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\t[broken sentence] Participant: 1071, Sentence: 15\n",
      "\n",
      "\n",
      "\t>>>This is subject: 1070.\n",
      "1 2 5 3 \n",
      "\t[broken sentence] Participant: 1070, Sentence: 3\n",
      "4 6 \n",
      "\t[broken sentence] Participant: 1070, Sentence: 6\n",
      "7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1072.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 15 \n",
      "\n",
      "\t>>>This is subject: 1073.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1074.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1075.\n",
      "1 2 5 3 \n",
      "\t[broken sentence] Participant: 1075, Sentence: 3\n",
      "4 6 8 9 \n",
      "\t[broken sentence] Participant: 1075, Sentence: 9\n",
      "10 11 12 \n",
      "\t[broken sentence] Participant: 1075, Sentence: 12\n",
      "13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1077.\n",
      "1 2 5 \n",
      "\t[broken sentence] Participant: 1077, Sentence: 5\n",
      "3 4 \n",
      "\t[broken sentence] Participant: 1077, Sentence: 4\n",
      "6 7 8 9 10 \n",
      "\t[broken sentence] Participant: 1077, Sentence: 10\n",
      "11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1078.\n",
      "1 2 5 3 4 6 8 9 \n",
      "\t[broken sentence] Participant: 1078, Sentence: 9\n",
      "10 \n",
      "\t[broken sentence] Participant: 1078, Sentence: 10\n",
      "11 12 \n",
      "\t[broken sentence] Participant: 1078, Sentence: 12\n",
      "13 14 \n",
      "\t[broken sentence] Participant: 1078, Sentence: 14\n",
      "15 \n",
      "\t[broken sentence] Participant: 1078, Sentence: 15\n",
      "\n",
      "\n",
      "\t>>>This is subject: 1076.\n",
      "1 2 5 \n",
      "\t[broken sentence] Participant: 1076, Sentence: 5\n",
      "3 4 \n",
      "\t[broken sentence] Participant: 1076, Sentence: 4\n",
      "6 \n",
      "\t[broken sentence] Participant: 1076, Sentence: 6\n",
      "7 8 \n",
      "\t[broken sentence] Participant: 1076, Sentence: 8\n",
      "9 10 \n",
      "\t[broken sentence] Participant: 1076, Sentence: 10\n",
      "11 12 13 14 \n",
      "\t[broken sentence] Participant: 1076, Sentence: 14\n",
      "15 \n",
      "\t[broken sentence] Participant: 1076, Sentence: 15\n",
      "\n",
      "\n",
      "\t>>>This is subject: 1079.\n",
      "1 2 5 3 \n",
      "\t[broken sentence] Participant: 1079, Sentence: 3\n",
      "4 6 \n",
      "\t[broken sentence] Participant: 1079, Sentence: 6\n",
      "7 8 9 \n",
      "\t[broken sentence] Participant: 1079, Sentence: 9\n",
      "10 \n",
      "\t[broken sentence] Participant: 1079, Sentence: 10\n",
      "11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1081.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1080.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1082.\n",
      "5 6 9 \n",
      "\n",
      "\t>>>This is subject: 1084.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 \n",
      "\n",
      "\t>>>This is subject: 1083.\n",
      "1 2 5 3 4 6 7 8 9 10 11 12 13 14 15 "
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "    key  timestamp     type\n129   α   728770.0    keyup\n130   α   729425.0  keydown\n131   α   729597.0    keyup",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-60ce327acb2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_sentences_from_raw_typing_mrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-a8091c09a754>\u001b[0m in \u001b[0;36mcreate_sentences_from_raw_typing_mrc\u001b[0;34m(df, make_long_format, time_redux_fact)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Method to 'implement' the users' backspace actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mnew_backspace_implementer_mrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-6a956da927c5>\u001b[0m in \u001b[0;36mnew_backspace_implementer_mrc\u001b[0;34m(df, backspace_char)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbackspace_char\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'keydown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mlength_backspace_keydowns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mlength_backspace_keydowns\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0;31m# Indices to manipulate including g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mfirst_half\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_range_extend_mrc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_first_backspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength_backspace_keydowns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m:     key  timestamp     type\n129   α   728770.0    keyup\n130   α   729425.0  keydown\n131   α   729597.0    keyup"
     ]
    }
   ],
   "source": [
    "new = create_sentences_from_raw_typing_mrc(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1010, 1011, 1012, 1014, 1013, 1015, 1019, 1016, 1018, 1020, 1022,\n",
       "       1021, 1024, 1023, 1026, 1025, 1027, 1030, 1031, 1033, 1034, 1035,\n",
       "       1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047,\n",
       "       1048, 1049, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1059, 1063,\n",
       "       1064, 1065, 1066, 1067, 1068, 1069, 1071, 1070, 1072, 1073, 1074,\n",
       "       1075, 1077, 1078, 1076, 1079, 1081, 1080, 1082, 1084, 1083, 1085,\n",
       "       1086, 1087, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098,\n",
       "       1099, 1100, 1104, 1108, 1106, 1109, 1110, 1111, 1113, 1117, 1119,\n",
       "       1120, 1121, 1122, 1123, 1124, 1128, 1126, 1125, 1129, 1127, 1131,\n",
       "       1130, 1137, 1133, 1134, 1138, 1135, 1140, 1141, 1139, 1142, 1143,\n",
       "       1144, 1146, 1145, 1149, 1148, 1150, 1151, 1152, 1153, 1154, 1156,\n",
       "       1157, 1158, 1159, 1160, 1161, 1162, 1163, 1165, 1166, 1167,   11,\n",
       "         12,   13,   17,   18,   19,   20,   25,   27,   28,   29,   30,\n",
       "         31,   32,   33,   26,   34,   36,   37,   38,   40,   41,   42,\n",
       "         43,   45,   46,   47,   48,   49,   50,   51,   53,   54,   55,\n",
       "         56,   57,   58,   59,   60,   61,   62,   63,   64,   67,   68,\n",
       "         69,   75,   76,   77,   78,   79,   81,   82,   83,   84,   85,\n",
       "         86,   87,   88,   89,   90,   91,   92,   93,   95,   96,   97,\n",
       "         98,  100,  101,  103,  106,  107,  108,  109,  110,  111,  112,\n",
       "        114,  117,  118,  119])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.participant_id.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the sorting mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_solitary_key_presses(df, verbose=False):\n",
    "\n",
    "    suspect_keys = []\n",
    "    for key, value in Counter(df.key.tolist()).items():\n",
    "        if value % 2 != 0:\n",
    "            # Find all keys which appear an unequal number of times\n",
    "            suspect_keys.append(key)\n",
    "\n",
    "    # Do not remove \"correction identifier key\" i.e. €, backspace\n",
    "    suspect_keys = [key for key in suspect_keys if key not in {\"€\", \"α\"}]\n",
    "\n",
    "    if verbose:\n",
    "        print(suspect_keys)\n",
    "\n",
    "    # Find all instances of suspect keys in df\n",
    "    if len(suspect_keys) != 0:\n",
    "        indices_to_keep = []\n",
    "        all_idxs = []\n",
    "        for key in suspect_keys:\n",
    "            idxs = df.loc[df.key == key].index\n",
    "            all_idxs.extend(idxs)\n",
    "            # If there is more than one such key\n",
    "            for pair in list(zip(idxs, idxs[1:]))[::2]:\n",
    "                if pair[1] - pair[0] == 1:\n",
    "                    indices_to_keep.extend(pair)\n",
    "\n",
    "        # Take set difference to find what's left\n",
    "        indices_to_remove = list(set(all_idxs) - set(indices_to_keep))\n",
    "\n",
    "        # In-place operation, no need to return anything. Cannot reset index at this point.\n",
    "        df.drop(df.index[indices_to_remove], inplace=True)\n",
    "        # Reset index so that we can sort it properly in the next step\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "def find_all_backspace_groups(df, backspace_char = \"α\"):\n",
    "    ids = df.index[(df.key == backspace_char)].tolist()\n",
    "    groups = []\n",
    "    for k, g in groupby(enumerate(sorted(ids)), lambda ix: ix[1] - ix[0]):\n",
    "        groups.append(list(map(itemgetter(1), g)))\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def lookup(v, \n",
    "           d={}, \n",
    "           c=count()):\n",
    "    if v in d:\n",
    "        return d.pop(v)\n",
    "    else:\n",
    "        d[v] = next(c)\n",
    "    return d[v]\n",
    "\n",
    "\n",
    "def reorder_key_timestamp_columns_mrc(df: pd.DataFrame):\n",
    "    # Use lookup function to extract the next row-order\n",
    "    df[\"new_row_order\"] = df.key.map(lookup)\n",
    "    return df.sort_values(by=\"new_row_order\", kind=\"mergesort\").drop(\"new_row_order\", axis=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def move_keys_to_temporal_monotonically_increasing_order(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # 1. Remove singular characters [in-place operation]\n",
    "    remove_solitary_key_presses(df) # Does not operate on backspaces or indicators\n",
    "    \n",
    "    # 2. hide/mask blocks of backspaces and don't re-order these, and then insert them again after the re-order\n",
    "    blocks = [i for i in find_all_backspace_groups(df, \"α\") if len(i) > 2]\n",
    "    if len(blocks) != 0:\n",
    "        proper_sorted = []\n",
    "        if len(blocks) == 1:\n",
    "            proper_sorted.append(reorder_key_timestamp_columns_mrc(df.iloc[0:blocks[0][0]]))\n",
    "            proper_sorted.append(df.iloc[blocks[0]])\n",
    "            proper_sorted.append(reorder_key_timestamp_columns_mrc(df.iloc[blocks[0][-1]+1:]))\n",
    "        else:\n",
    "            i = 0\n",
    "            for block in blocks:\n",
    "                # Character blocks to be re-ordered\n",
    "                proper_sorted.append(reorder_key_timestamp_columns_mrc(df.iloc[i:block[0]]))\n",
    "                # Backspaces blocks to be left as is\n",
    "                proper_sorted.append(df.iloc[block])\n",
    "                # Store the coordinate of last index in the backspace block\n",
    "                i = block[-1]+1\n",
    "            # Append the text block\n",
    "            proper_sorted.append(reorder_key_timestamp_columns_mrc(df.iloc[i:]))\n",
    "            \n",
    "        # Recombine all blocks and return\n",
    "        return pd.concat(proper_sorted, ignore_index=True)\n",
    "        \n",
    "    else:\n",
    "        # No contiguous blocks, so sort as usual\n",
    "        return reorder_key_timestamp_columns_mrc(df)\n",
    "\n",
    "def indicate_single_and_double_backspaces(df):\n",
    "    # 0) Remove any singular backspaces that appear bc. data-reading problems\n",
    "    remove = []\n",
    "    groups = find_all_backspace_groups(df)\n",
    "    \n",
    "    # Only remove ones which are actually only of list length 1\n",
    "    for g in groups:\n",
    "        # Data-reading error\n",
    "        if len(g) == 1:\n",
    "            remove.extend(g)\n",
    "        # We replace these inline so we don't have to do it later\n",
    "        elif len(g) == 2:\n",
    "            # Place indicators [keydown]\n",
    "            df.loc[g[0], \"key\"] = \"€\"\n",
    "            # Place indicators [keyup]\n",
    "            df.loc[g[1], \"key\"] = \"€\"\n",
    "            \n",
    "######### Neil [15/11/19] : we do not need this right now but will keep it anyway\n",
    "#         else:\n",
    "#             # This line checks if a backspace (keydown,keyup) is included in a contiguous sequence\n",
    "#             t = df.loc[g,'type'].tolist()\n",
    "#             for i, pair in enumerate(list(zip(t, t[1:]))):\n",
    "#                 if df.loc[g[i-1],'type'] != 'keydown':\n",
    "#                     if pair == ('keydown', 'keyup') or pair == ('keyup', 'keydown'):\n",
    "#                         # Place indicators [keydown]\n",
    "#                         df.loc[g[i], \"key\"] = \"€\"\n",
    "#                         # Place indicators [keyup]\n",
    "#                         df.loc[g[i+1], \"key\"] = \"€\"\n",
    "\n",
    "    if remove:\n",
    "        # In-place droppping of rows with only one backspace\n",
    "        df.drop(df.index[remove], inplace=True)\n",
    "        # Reset index so that we can sort it properly in the next step\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        \n",
    "def remove_backspace_keyup(df, backspace_char):\n",
    "    \n",
    "    idxs_up = df.index[(df.key == backspace_char) & (df.type == \"keyup\")].tolist()\n",
    "    # Copy these rows for later use\n",
    "    df_keyup = df.iloc[idxs_up].copy(deep=True)\n",
    "    # In-place dropping of these rows\n",
    "    df.drop(df.index[idxs_up], inplace=True)\n",
    "    # Reset index so that we can sort it properly in the next step\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df_keyup\n",
    "\n",
    "def find_remaining_backspace_keydown(df, backspace_char):\n",
    "    \n",
    "    idxs = df.index[(df.key == backspace_char) & (df.type == \"keydown\")].tolist()\n",
    "    contiguous_groups = []\n",
    "    for k, g in groupby(enumerate(sorted(idxs)), lambda ix: ix[1] - ix[0]):\n",
    "        contiguous_groups.append(list(map(itemgetter(1), g)))\n",
    "        \n",
    "    return contiguous_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NEW FUNCTIONS\n",
    "\n",
    "def new_indicate_single_and_double_backspaces(df):\n",
    "    \n",
    "    # 0) Remove any singular backspaces that appear bc. data-reading problems\n",
    "    remove = []\n",
    "    groups = find_all_backspace_groups(df)\n",
    "    \n",
    "    # Only remove ones which are actually only of list length 1\n",
    "    for g in groups:\n",
    "        # Data-reading error\n",
    "        if len(g) == 1:\n",
    "            remove.extend(g)\n",
    "        # We replace these inline so we don't have to do it later\n",
    "        elif len(g) == 2:\n",
    "            # Place indicators [keydown]\n",
    "            df.loc[g[0], \"key\"] = \"€\"\n",
    "            # Place indicators [keyup]\n",
    "            df.loc[g[1], \"key\"] = \"€\"\n",
    "    \n",
    "    # Return the indices to remove so we can add to this list during processing\n",
    "    return remove \n",
    "\n",
    "def find_timestamp_in_index_range(df_view, key_type, which_extreme):\n",
    "    \n",
    "    # 1. We want the smallest timestamp for any df_view when looking at keydown\n",
    "    # 2. We want the largest timestamp for any df_view when looking at keyup\n",
    "    \n",
    "    # df_view is an already sub-indexed view of the larger matrix\n",
    "    \n",
    "    assert key_type in ['keydown','keyup']\n",
    "    assert which_extreme in ['min','max']\n",
    "    assert df_view.shape[0] < 100  # It is highly unlikely that someone has pressed backspace this much\n",
    "    \n",
    "    if which_extreme == 'min':\n",
    "        assert key_type == 'keydown', key_type\n",
    "        return df_view.loc[(df_view.type == key_type), 'timestamp'].min()\n",
    "    else:\n",
    "        assert key_type == 'keyup', key_type\n",
    "        return df_view.loc[(df_view.type == key_type), 'timestamp'].max()\n",
    "    \n",
    "    \n",
    "def new_range_extend_mrc(start, length) -> list:\n",
    "    \"\"\"\n",
    "    start: is the top index of what is being removed\n",
    "    length: is the total length of what is being removed (backwards),\n",
    "            it is the length of backspace+keydown sequence but only n-1 of them\n",
    "    \"\"\"    \n",
    "    # Need to assert that this is given a sequentially ordered array\n",
    "    out = list(range(start - 2 * length, start - length)) + list(range(start - length, start)) + list(range(start,start-length))\n",
    "    assert np.diff(out).sum() == len(out) - 1, (start, length, out)\n",
    "    return out\n",
    "\n",
    "def get_coordinate_of_first_backspace_keydown_in_view(df_view, backspace_char = 'α'):\n",
    "    # Get the index of the first backspace+keydown\n",
    "    idx = df_view.loc[(df_view.type == 'keydown') & (df_view.key == backspace_char)].index[0]\n",
    "    if df_view.loc[idx].type == 'keydown':\n",
    "        # This means that the backspace sequence correctly starts with a keydown\n",
    "        return idx\n",
    "    elif df_view.loc[idx].type == 'keyup':\n",
    "        # This means that the backspace sequence is unsorted and we simply take the first index\n",
    "        return df_view.loc[(df_view.key == backspace_char)].index[0]\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "def new_backspace_implementer_mrc(df, backspace_char = 'α'):\n",
    "    \n",
    "    # XXX This function assumes that df has already been sorted properly using the new method.\n",
    "    \n",
    "    indices_to_remove = new_indicate_single_and_double_backspaces(df)\n",
    "    \n",
    "    # Find all larger groups \n",
    "    big_contiguous_groups = find_all_backspace_groups(df)\n",
    "    \n",
    "    # Filter, in case single backspace indices are still present\n",
    "    big_contiguous_groups = [i for i in big_contiguous_groups if i[0] not in indices_to_remove]\n",
    "    \n",
    "    if big_contiguous_groups:\n",
    "    \n",
    "        \"\"\"\n",
    "        How it works:\n",
    "        \n",
    "        1. Get the coordinate of the first backspace in the sequence (index 0)\n",
    "        2. Get the total count of number of backspace+keydowns in the sequence (this will account for hold-down sequence and tapping sequences)\n",
    "           this takes into account that we only want the n-1 backspaces\n",
    "        \"\"\"\n",
    "        \n",
    "        # Loop over all found groups\n",
    "        for g in big_contiguous_groups:\n",
    "            \n",
    "            # For simplicity we save this as an array view\n",
    "            df_view = df.loc[g]\n",
    "            \n",
    "            # Index of the first backspace\n",
    "            idx_first_backspace = get_coordinate_of_first_backspace_keydown_in_view(df_view)\n",
    "            \n",
    "            if idx_first_backspace == 0:\n",
    "                # Special case when a sentence starts with backspaces\n",
    "\n",
    "                assert idx_first_backspace == g[0], (idx_first_backspace, g[0])\n",
    "                \n",
    "                # Assign global start indicator\n",
    "                assert df_view.loc[df_view.index[0]].type == 'keydown'\n",
    "                start_timestamp = find_timestamp_in_index_range(df_view,'keydown','min')\n",
    "                df.loc[g[0], (\"key\", \"timestamp\", \"type\")] = [\"€\", start_timestamp, \"keydown\"]\n",
    "                # Assign global end indicator\n",
    "                assert df_view.loc[df_view.index[-1]].type == 'keyup'\n",
    "                end_timestamp = find_timestamp_in_index_range(df_view,'keyup','max')\n",
    "                df.loc[g[-1], (\"key\", \"timestamp\", \"type\")] = [\"€\", end_timestamp, \"keyup\"]                \n",
    "                \n",
    "                # Append the indices that we remove at the very end\n",
    "                indices_to_remove.extend(g[1:-1])\n",
    "                \n",
    "            else:            \n",
    "                # Select only the n-1 backspaces [assumption: backspace keyups are masked at this stage]\n",
    "                tmp = df_view.loc[(df_view.key == backspace_char) & (df_view.type == 'keydown')]\n",
    "                length_backspace_keydowns = len(tmp) -1\n",
    "                assert length_backspace_keydowns != 0, df_view\n",
    "                # Indices to manipulate including g\n",
    "                first_half = new_range_extend_mrc(idx_first_backspace, length_backspace_keydowns)\n",
    "                if any(i < 0 for i in first_half):\n",
    "                    first_half = list(filter(lambda x: x >= 0, first_half))\n",
    "                # First half and second half of indices to remove\n",
    "                gg = sorted(first_half + g)\n",
    "                # Assign global start indicator\n",
    "                start_timestamp = find_timestamp_in_index_range(df_view,'keydown','min')\n",
    "                df.loc[gg[0], (\"key\", \"timestamp\", \"type\")] = [\"€\", start_timestamp, \"keydown\"]\n",
    "                # Assign global end indicator\n",
    "                end_timestamp = find_timestamp_in_index_range(df_view,'keyup','max')\n",
    "                df.loc[gg[-1], (\"key\", \"timestamp\", \"type\")] = [\"€\", end_timestamp, \"keyup\"]\n",
    "            \n",
    "                # Append the indices that we remove at the very end\n",
    "                indices_to_remove.extend(gg[1:-1])\n",
    "        \n",
    "        # Remove all duplicate indices, drop the unique ones and reset index\n",
    "        df.drop(df.index[list(set(indices_to_remove))], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    elif indices_to_remove:\n",
    "        # No contiguous groups to consider, only data-reading backspaces\n",
    "    \n",
    "        # Remove all duplicate indices and reset index\n",
    "        df.drop(df.index[list(set(indices_to_remove))], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # Check that the indicators appear in the right places (they must appear in pairs)\n",
    "    indicator_indices = df.index[(df.key == \"€\")].tolist()\n",
    "    for pair in list(zip(indicator_indices, indicator_indices[1:]))[::2]:\n",
    "        assert pair[1] - pair[0] == 1, indicator_indices\n",
    "        \n",
    "    # Final check\n",
    "    assert backspace_char not in df.key.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    tmp0 = df.loc[(df.participant_id == 1010) & (df.sentence_id == 12), (\"key\", \"timestamp\", \"type\")].reset_index(drop=True)  # Reset index\n",
    "    #print(tmp0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[95, 96], [284, 285], [287, 288]]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all_backspace_groups(tmp0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp0 = move_keys_to_temporal_monotonically_increasing_order(tmp0)\n",
    "new_backspace_implementer_mrc(tmp0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb, traceback, sys\n",
    "\n",
    "try:\n",
    "    annoying_fuck = backspace_implementer_mrc(test_df)\n",
    "except:\n",
    "    extype, value, tb = sys.exc_info()\n",
    "    traceback.print_exc()\n",
    "    pdb.post_mortem(tb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
