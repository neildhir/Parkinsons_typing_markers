{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.4\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "\n",
    "# Set path to find modelling tools for later use\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(),\"..\"))\n",
    "\n",
    "\n",
    "from haberrspd.preprocess import preprocessMJFF\n",
    "                         \n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "from collections import Counter, defaultdict\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "from scipy.stats import (gamma, lognorm, gengamma)\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "\n",
    "# Plot stuff\n",
    "import seaborn as sns\n",
    "from scipy.constants import golden\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# Depending on where I am, set the path\n",
    "import socket\n",
    "if socket.gethostname() == 'pax':\n",
    "    # Monster machine\n",
    "    data_root = '../data/MJFF/' # My local path\n",
    "    data_root = Path(data_root)\n",
    "else:\n",
    "    # Laptop\n",
    "    data_root = '/home/nd/data/liverpool/MJFF' # My local path\n",
    "    data_root = Path(data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character + Timing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = preprocessMJFF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = proc('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(\"../data/MJFF/preproc/EnglishSpanishData-preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = proc('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(\"../data/MJFF/preproc/SpanishData-preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = proc('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(\"../data/MJFF/preproc/EnglishData-preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[out['Preprocessed_typed_sentence'].apply(lambda x: len(x) > 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(data_root / 'SpanishData-duplicateeventsremoved.csv')\n",
    "df = pd.read_csv(data_root / 'SpanishData.csv')\n",
    "df_meta = pd.read_csv(data_root / \"SpanishParticipantKey.csv\",\n",
    "                      index_col=0,\n",
    "                      header=0,\n",
    "                      names=['participant_id', 'diagnosis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character ONLY data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = preprocessMJFF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = proc('english',include_time=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[out['Preprocessed_typed_sentence'].apply(lambda x: len(x) < 30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(\"../data/MJFF/preproc/char/EnglishSpanishData-preprocessed.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = set(df_meta.loc[df_meta.diagnosis == 0].participant_id)\n",
    "pd_subjects = set(df_meta.loc[df_meta.diagnosis == 1].participant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_timestamp_diffs = []\n",
    "# Loop over all subjects\n",
    "for sub in pd_subjects:\n",
    "    # Get all delta timestamps for this sentence, across all subjects\n",
    "    pd_timestamp_diffs.extend(df.loc[(df.sentence_id == 57) & (df.participant_id == sub)].timestamp.diff().values)\n",
    "    \n",
    "control_timestamp_diffs = []\n",
    "# Loop over all subjects\n",
    "for sub in controls:\n",
    "    # Get all delta timestamps for this sentence, across all subjects\n",
    "    control_timestamp_diffs.extend(df.loc[(df.sentence_id == 57) & (df.participant_id == sub)].timestamp.diff().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaNs\n",
    "pd_cleaned_list = [x for x in pd_timestamp_diffs if str(x) != 'nan']\n",
    "control_cleaned_list = [x for x in control_timestamp_diffs if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PD\n",
    "\n",
    "# fixed bin size\n",
    "bins = np.arange(0, 10000, 50) # fixed bin size\n",
    "# plt.xlim([min(data)-5, max(data)+5])\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "plt.hist(pd_cleaned_list, bins=bins, alpha=0.5)\n",
    "# plt.vlines(np.quantile(cleaned_list,0.95),0,900,'r')\n",
    "# plt.vlines(np.mean(cleaned_list),0,900,'b')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control\n",
    "\n",
    "# fixed bin size\n",
    "bins = np.arange(0, 10000, 50) # fixed bin size\n",
    "# plt.xlim([min(data)-5, max(data)+5])\n",
    "fig = plt.figure(figsize=(14,6))\n",
    "plt.hist(control_cleaned_list, bins=bins, alpha=0.5)\n",
    "# plt.vlines(np.quantile(cleaned_list,0.95),0,900,'r')\n",
    "# plt.vlines(np.mean(cleaned_list),0,900,'b')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mechanical turk data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(data_root /'preprocessed_MechanicalTurkCombinedEnglishData.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ../data/MJFF/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IKI extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = create_mjff_iki_training_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyboard inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.charCNN.data_utils_tf import create_mjff_data_objects, us_standard_layout_keyboard, english_keys_to_2d_coordinates\n",
    "import keras.backend as K\n",
    "from keras import callbacks\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from numpy import array, int64, ones, hstack, pad, einsum, dstack\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import cast, float32, one_hot\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_information = \"char_time_space\"\n",
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\"\n",
    "data_string = \"EnglishData-preprocessed.csv\"\n",
    "if which_information == \"char_time_space\":\n",
    "    # Get relevant long-format data\n",
    "    which_information = \"char_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_csv(DATA_ROOT / which_information / data_string, header=0)  # MJFF data\n",
    "subject_documents, subjects_diagnoses, alphabet = create_mjff_data_objects(df)\n",
    "\n",
    "# Store alphabet size\n",
    "alphabet_size = len(alphabet)\n",
    "\n",
    "print('Total number of characters:', alphabet_size)\n",
    "alphabet_indices = dict((c, i) for i, c in enumerate(alphabet))\n",
    "\n",
    "if which_information == \"char_time\" or which_information == \"char_time_space\":\n",
    "    # Rounds (up) to nearest thousand\n",
    "    max_sentence_length = round(df.Preprocessed_typed_sentence.apply(lambda x: len(x)).max(), -3)\n",
    "if which_information == \"char\":\n",
    "    # Rounds (up) to nearest hundred\n",
    "    max_sentence_length = round(df.Preprocessed_typed_sentence.apply(lambda x: len(x)).max(), -2)\n",
    "\n",
    "# Make training data array\n",
    "all_sentences = [item for sublist in subject_documents for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise tokenizer which maps characters to integers\n",
    "tk = Tokenizer(num_words=None, char_level=True)\n",
    "\n",
    "# Fit to text: convert all chars to ints\n",
    "tk.fit_on_texts(all_sentences)\n",
    "\n",
    "# Update alphabet\n",
    "tk.word_index = alphabet_indices\n",
    "\n",
    "# Get integer sequences: converts sequences of chars to sequences of ints\n",
    "int_sequences = tk.texts_to_sequences(all_sentences)\n",
    "\n",
    "# Pad sequences so that they all have the same length and then one-hot encode\n",
    "X = to_categorical(pad_sequences(int_sequences, maxlen=max_sentence_length, padding='post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_information  =  'char_time_space'\n",
    "if which_information == 'char_time_space':\n",
    "    # Load relevant keyboard\n",
    "    keyboard = us_standard_layout_keyboard()  # OBS: nested list\n",
    "    # Check that all chars are in fact in our \"keyboard\" -- if not, we cannot map a coordinate\n",
    "    assert alphabet.issubset(set(list(itertools.chain.from_iterable(keyboard))))\n",
    "    space = [english_keys_to_2d_coordinates(sentence, keyboard) for sentence in all_sentences]\n",
    "    space_padded = [pad(s, [(0, max_sentence_length - len(s)), (0, 0)], mode='constant') for s in space]\n",
    "    # Append coordinates to one-hot encoded sentences\n",
    "    X = einsum('ijk->kij', dstack([hstack((x, s)) for (x, s) in zip(X, space_padded)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document (participant) -level classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three options (at time of writing):\n",
    "\n",
    "1. Submit each sentence to model and extract classification probability for each sentence, agglomorate at the end, and the conduct a classification on the vector of all 15 probabilities.\n",
    "2. Calculate the expected value of all encoded (15) sentences and then pass this to the model and take the classification.\n",
    "3. Vertically stack all embedded sentences, and let the convolution run over this (very long) array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.charCNN.data_utils_tf import create_training_data_keras, create_mjff_data_objects\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_information = \"char_time\"\n",
    "DATA_ROOT = Path(\"../data/\") / \"MJFF\" / \"preproc\"\n",
    "data_string = \"EnglishData-preprocessed.csv\"\n",
    "df = read_csv(DATA_ROOT / which_information / data_string, header=0)  # MJFF data\n",
    "# subject_documents, subjects_diagnoses, alphabet = create_mjff_data_objects(df)\n",
    "# X_train, X_test, y_train, y_test, max_sentence_length, alphabet_size = create_training_data_keras(DATA_ROOT, which_information, data_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in subject_documents[:3]:\n",
    "    print(doc[:2])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Homogenise so that Spacebar is a blank character\n",
    "0. Delete rubbish characters (i.e. remove the rows)\n",
    "2. What to do with stuff like Shift\n",
    "3. Figure out what to do when multiple characters are depressed simultaneously\n",
    "4. Make lowercase all characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from haberrspd.preprocess import clean_MRC, backspace_corrector, make_character_compression_time_sentence\n",
    "from haberrspd.charCNN.data_utils_tf import us_english_keyboard_mrc\n",
    "from numpy import concatenate\n",
    "from typing import Tuple\n",
    "\n",
    "from itertools import compress, count\n",
    "from operator import itemgetter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Monster machine\n",
    "data_root = '../data/' # My local path\n",
    "data_root = Path(data_root)\n",
    "full_path = data_root / \"MRC\" / \"mrc_raw.csv\"\n",
    "\n",
    "df = read_csv(full_path, header=0)  # MRC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removal of sentences with 'high' Levenshtein distance...\n",
      "\n",
      "Size of dataframe before row pruning: (814320, 13)\n",
      "Size of dataframe after row pruning: (813662, 13)\n",
      "\n",
      "Removal of sentences with left/right arrows keys...\n",
      "\n",
      "Size of dataframe before row pruning: (813662, 13)\n",
      "Size of dataframe after row pruning: (781533, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>type</th>\n",
       "      <th>location</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>response_id</th>\n",
       "      <th>response_content</th>\n",
       "      <th>response_created</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence_content</th>\n",
       "      <th>LPI</th>\n",
       "      <th>STAGE</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>shift</td>\n",
       "      <td>keydown</td>\n",
       "      <td>1</td>\n",
       "      <td>25885.055</td>\n",
       "      <td>116</td>\n",
       "      <td>However,religions other than Islam, use a diff...</td>\n",
       "      <td>2019-07-01T12:31:07.218Z</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>However, religions other than Islam, use a dif...</td>\n",
       "      <td>-4</td>\n",
       "      <td>NODIAGNOSIS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>h</td>\n",
       "      <td>keydown</td>\n",
       "      <td>0</td>\n",
       "      <td>26086.840</td>\n",
       "      <td>116</td>\n",
       "      <td>However,religions other than Islam, use a diff...</td>\n",
       "      <td>2019-07-01T12:31:07.218Z</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>However, religions other than Islam, use a dif...</td>\n",
       "      <td>-4</td>\n",
       "      <td>NODIAGNOSIS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>shift</td>\n",
       "      <td>keyup</td>\n",
       "      <td>1</td>\n",
       "      <td>26181.975</td>\n",
       "      <td>116</td>\n",
       "      <td>However,religions other than Islam, use a diff...</td>\n",
       "      <td>2019-07-01T12:31:07.218Z</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>However, religions other than Islam, use a dif...</td>\n",
       "      <td>-4</td>\n",
       "      <td>NODIAGNOSIS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>h</td>\n",
       "      <td>keyup</td>\n",
       "      <td>0</td>\n",
       "      <td>26193.745</td>\n",
       "      <td>116</td>\n",
       "      <td>However,religions other than Islam, use a diff...</td>\n",
       "      <td>2019-07-01T12:31:07.218Z</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>However, religions other than Islam, use a dif...</td>\n",
       "      <td>-4</td>\n",
       "      <td>NODIAGNOSIS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>o</td>\n",
       "      <td>keydown</td>\n",
       "      <td>0</td>\n",
       "      <td>26321.480</td>\n",
       "      <td>116</td>\n",
       "      <td>However,religions other than Islam, use a diff...</td>\n",
       "      <td>2019-07-01T12:31:07.218Z</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>However, religions other than Islam, use a dif...</td>\n",
       "      <td>-4</td>\n",
       "      <td>NODIAGNOSIS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     key     type  location  timestamp  response_id  \\\n",
       "0  shift  keydown         1  25885.055          116   \n",
       "1      h  keydown         0  26086.840          116   \n",
       "2  shift    keyup         1  26181.975          116   \n",
       "3      h    keyup         0  26193.745          116   \n",
       "4      o  keydown         0  26321.480          116   \n",
       "\n",
       "                                    response_content  \\\n",
       "0  However,religions other than Islam, use a diff...   \n",
       "1  However,religions other than Islam, use a diff...   \n",
       "2  However,religions other than Islam, use a diff...   \n",
       "3  However,religions other than Islam, use a diff...   \n",
       "4  However,religions other than Islam, use a diff...   \n",
       "\n",
       "           response_created  participant_id  sentence_id  \\\n",
       "0  2019-07-01T12:31:07.218Z            1000            1   \n",
       "1  2019-07-01T12:31:07.218Z            1000            1   \n",
       "2  2019-07-01T12:31:07.218Z            1000            1   \n",
       "3  2019-07-01T12:31:07.218Z            1000            1   \n",
       "4  2019-07-01T12:31:07.218Z            1000            1   \n",
       "\n",
       "                                    sentence_content  LPI        STAGE  \\\n",
       "0  However, religions other than Islam, use a dif...   -4  NODIAGNOSIS   \n",
       "1  However, religions other than Islam, use a dif...   -4  NODIAGNOSIS   \n",
       "2  However, religions other than Islam, use a dif...   -4  NODIAGNOSIS   \n",
       "3  However, religions other than Islam, use a dif...   -4  NODIAGNOSIS   \n",
       "4  However, religions other than Islam, use a dif...   -4  NODIAGNOSIS   \n",
       "\n",
       "   diagnosis  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean_MRC(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol to process the MRC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A form of `create_char_compression_time_mjff_data` [to get the temporal data] <-- this needs to be set so that we get a list of \n",
    "2. Second use `create_dataframe_from_processed_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(v, d={}, c=count()):\n",
    "    if v in d:\n",
    "        return d.pop(v)\n",
    "    else:\n",
    "        d[v] = next(c)\n",
    "        return d[v]\n",
    "    \n",
    "def increasing(L):\n",
    "    return all(x<=y for x, y in zip(L, L[1:]))\n",
    "\n",
    "def reorder_key_timestamp_columns_mrc(df):\n",
    "    \n",
    "    # Check that the column is of even length\n",
    "    assert len(df) % 2 == 0, \"The length is {}.\".format(len(df))\n",
    "    \n",
    "    df['new_row_order'] = df.key.map(lookup)\n",
    "    \n",
    "    return df.sort_values(by='new_row_order', kind='mergesort').drop('new_row_order', axis=1).reset_index(drop=True)\n",
    "    \n",
    "def calculate_total_key_compression_time(df):\n",
    "    return [(x-y) for x,y in zip(df.timestamp[1::2], df.timestamp[0::2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_compression_time_mrc_data(df: pd.DataFrame, time_redux_fact=10) -> Tuple[dict, list]:\n",
    "\n",
    "    assert set([\"participant_id\", \"key\", \"timestamp\", \"sentence_id\"]).issubset(df.columns)\n",
    "\n",
    "    # All sentences will be stored here, indexed by their type\n",
    "    char_compression_sentences = defaultdict(dict)\n",
    "    \n",
    "    # Get the unique number of subjects\n",
    "    subjects = sorted(set(df.participant_id))  # NOTE: set() is weakly random\n",
    "\n",
    "    # Loop over subjects\n",
    "    for subj_idx in subjects:\n",
    "        # Not all subjects have typed all sentences hence we have to do it this way\n",
    "        for sent_idx in df.loc[(df.participant_id == subj_idx)].sentence_id.unique():\n",
    "            \n",
    "            # Locate df segment to extract\n",
    "            coordinates = (df.participant_id == subj_idx) & (df.sentence_id == sent_idx)\n",
    "            \n",
    "            print(\"Participant: {}, Sentence: {}\".format(subj_idx, sent_idx))\n",
    "            \n",
    "            # Get correctly ordered sentences and total compression times\n",
    "            df_tmp = reorder_key_timestamp_columns_mrc(df.loc[coordinates, ('key','timestamp')])\n",
    "            \n",
    "            # \"correct\" the sentence by operating on user backspaces \n",
    "            corrected_char_sentence, removed_chars_indx = backspace_corrector(df_tmp.key.tolist())\n",
    "            \n",
    "            compression_times = calculate_total_key_compression_time(df_tmp.drop(df_tmp.index[removed_chars_indx]))\n",
    "            \n",
    "            assert len(compression_times) == len(corrected_char_sentence[::2]), \"Error at ({},{}).\".format(subj_idx,sent_idx)\n",
    "            assert any(x < 0 for x in compression_times) is False, \"Error at ({},{}).\".format(subj_idx,sent_idx) # Check no negative timings\n",
    "\n",
    "            # Make long-format version of each typed, corrected, sentence\n",
    "            # Note that we remove the last character to make the calculation correct.\n",
    "            char_compression_sentences[subj_idx][sent_idx] = make_character_compression_time_sentence(compression_times,\n",
    "                                                                                                      corrected_char_sentence[::2], \n",
    "                                                                                                      time_redux_fact)\n",
    "    return char_compression_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.loc[df.participant_id.isin([1024,1025,1026,1027])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_small.loc[df_small.participant_id == 1025].sentence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>shift</td>\n",
       "      <td>111201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>shift</td>\n",
       "      <td>111466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>111373.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>t</td>\n",
       "      <td>111466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>h</td>\n",
       "      <td>111671.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>e</td>\n",
       "      <td>141498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>r</td>\n",
       "      <td>141466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>r</td>\n",
       "      <td>141606.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>141716.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>&lt;unk&gt;</td>\n",
       "      <td>141873.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       key  timestamp\n",
       "0    shift   111201.0\n",
       "1    shift   111466.0\n",
       "2        t   111373.0\n",
       "3        t   111466.0\n",
       "4        h   111671.0\n",
       "..     ...        ...\n",
       "231      e   141498.0\n",
       "232      r   141466.0\n",
       "233      r   141606.0\n",
       "234  <unk>   141716.0\n",
       "235  <unk>   141873.0\n",
       "\n",
       "[236 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reorder_key_timestamp_columns_mrc(df_small.loc[(df_small.participant_id == 1025) & (df_small.sentence_id == 3), ('key','timestamp')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df_small.loc[df_small.participant_id == 1025].sentence_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 1, LEN: 230\n",
      "ID: 2, LEN: 131\n",
      "ID: 3, LEN: 236\n",
      "ID: 4, LEN: 206\n",
      "ID: 5, LEN: 143\n",
      "ID: 6, LEN: 329\n",
      "ID: 7, LEN: 250\n",
      "ID: 8, LEN: 272\n",
      "ID: 9, LEN: 290\n",
      "ID: 10, LEN: 190\n",
      "ID: 11, LEN: 274\n",
      "ID: 12, LEN: 358\n",
      "ID: 13, LEN: 306\n",
      "ID: 14, LEN: 498\n",
      "ID: 15, LEN: 265\n"
     ]
    }
   ],
   "source": [
    "for i in {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}:\n",
    "    print(\"ID: {}, LEN: {}\".format(i,len(df_small.loc[(df_small.participant_id == 1025) & (df_small.sentence_id == i), \"key\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = create_char_compression_time_mrc_data(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = (df.participant_id == 1000) & (df.sentence_id == 3)\n",
    "dft = df.loc[coordinates, ('key','timestamp')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>shift</td>\n",
       "      <td>125183.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>t</td>\n",
       "      <td>125529.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>t</td>\n",
       "      <td>125626.555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>shift</td>\n",
       "      <td>125632.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>h</td>\n",
       "      <td>125755.395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       key   timestamp\n",
       "512  shift  125183.195\n",
       "513      t  125529.335\n",
       "514      t  125626.555\n",
       "515  shift  125632.485\n",
       "516      h  125755.395"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = reorder_key_timestamp_columns_mrc(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = (df.participant_id == 1025) & (df.sentence_id == 5)\n",
    "dft = df.loc[coordinates, ('key','timestamp')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shift',\n",
       " 'b',\n",
       " 'shift',\n",
       " 'b',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'o',\n",
       " 'k',\n",
       " 'k',\n",
       " 's',\n",
       " 's',\n",
       " ' ',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " 'l',\n",
       " 'c',\n",
       " 'l',\n",
       " 'u',\n",
       " 'u',\n",
       " 'd',\n",
       " 'd',\n",
       " 'e',\n",
       " 'e',\n",
       " ' ',\n",
       " ' ',\n",
       " 'shift',\n",
       " 'p',\n",
       " 'shift',\n",
       " 'p',\n",
       " 'e',\n",
       " 'n',\n",
       " 'e',\n",
       " 'n',\n",
       " 'g',\n",
       " 'g',\n",
       " 'u',\n",
       " 'u',\n",
       " 'i',\n",
       " 'i',\n",
       " 'n',\n",
       " 'n',\n",
       " ' ',\n",
       " ' ',\n",
       " 'a',\n",
       " 'shift',\n",
       " 'a',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'shift',\n",
       " 'backspace',\n",
       " 'backspace',\n",
       " 'shift',\n",
       " 'i',\n",
       " 'i',\n",
       " 'shift',\n",
       " 's',\n",
       " 's',\n",
       " 'l',\n",
       " 'l',\n",
       " 'a',\n",
       " 'n',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " 'd',\n",
       " ',',\n",
       " ',',\n",
       " ' ',\n",
       " ' ',\n",
       " 'a',\n",
       " 'a',\n",
       " ' ',\n",
       " ' ',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 't',\n",
       " 'i',\n",
       " 'r',\n",
       " 'r',\n",
       " 'e',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " ' ',\n",
       " 'n',\n",
       " 'o',\n",
       " ' ',\n",
       " 'n',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'e',\n",
       " ' ',\n",
       " ' ',\n",
       " 'shift',\n",
       " 'd',\n",
       " 'shift',\n",
       " 'd',\n",
       " 'r',\n",
       " 'r',\n",
       " 'e',\n",
       " 'e',\n",
       " 'y',\n",
       " 'y',\n",
       " 'f',\n",
       " 'u',\n",
       " 'f',\n",
       " 'u',\n",
       " 's',\n",
       " 's',\n",
       " ' ',\n",
       " ' ',\n",
       " 'a',\n",
       " 'a',\n",
       " 'f',\n",
       " 'f',\n",
       " 'f',\n",
       " 'f',\n",
       " 'a',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'r',\n",
       " 'r',\n",
       " '<unk>',\n",
       " '<unk>']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft.key.tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
