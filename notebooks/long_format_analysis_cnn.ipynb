{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/neil/cloud/haberrspd\n"
     ]
    }
   ],
   "source": [
    "cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "from haberrspd.__init__ import *\n",
    "from haberrspd.__init_paths import data_root\n",
    "\n",
    "# goto the correct folder just for practicality's sake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convolutional Neural Networks for Sentence Classification** adapted for use with characters embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "import haberrspd.charCNN.mydatasets as mydatasets\n",
    "import torch # To set the device\n",
    "import datetime\n",
    "import os\n",
    "from haberrspd.charCNN.model import CNN_Text\n",
    "import haberrspd.charCNN.train as train\n",
    "from haberrspd.pdnet_flair import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config(\n",
    "    # Arguments\n",
    "    batch_size = 64,\n",
    "    lr=0.001,\n",
    "    epochs=10,\n",
    "    kernel_sizes = [3,4,5],\n",
    "    kernel_num = 100,\n",
    "    embed_dim=128,\n",
    "    dropout=0.5,\n",
    "    cuda = torch.cuda.is_available(),\n",
    "    snapshot = None,\n",
    "    static=False,\n",
    "    predict=None,\n",
    "    test=False,\n",
    "    log_interval=1,\n",
    "    test_interval=100,\n",
    "    early_stop=1000,\n",
    "    save_best=True,\n",
    "    save_interval=500,\n",
    "    save_dir = 'snapshot'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading\n",
      "extracting\n"
     ]
    }
   ],
   "source": [
    "# In this example, words are embedded\n",
    "def mr(text_field, label_field, **kargs):\n",
    "    train_data, dev_data = mydatasets.MR.splits(text_field, label_field)\n",
    "    text_field.build_vocab(train_data, dev_data)\n",
    "    label_field.build_vocab(train_data, dev_data)\n",
    "    train_iter, dev_iter = data.Iterator.splits(\n",
    "                                (train_data, dev_data), \n",
    "                                batch_sizes=(args.batch_size, len(dev_data)),\n",
    "                                **kargs)\n",
    "    return train_iter, dev_iter\n",
    "\n",
    "# load data\n",
    "text_field = data.Field(lower=True)\n",
    "label_field = data.Field(sequential=False)\n",
    "# Data passed to model here \n",
    "train_iter, dev_iter = mr(text_field, \n",
    "                          label_field, \n",
    "                          device=torch.device('cuda'), \n",
    "                          repeat=False)\n",
    "\n",
    "# update args and print\n",
    "args.set(\"embed_num\",len(text_field.vocab))\n",
    "args.set(\"class_num\",len(label_field.vocab) - 1)\n",
    "args.set(\"save_dir\", os.path.join(args.save_dir, \n",
    "                                  datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, each character is embedded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-example here (task: infer if positive/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his',\n",
       " 'best',\n",
       " 'film',\n",
       " 'remains',\n",
       " 'his',\n",
       " 'shortest',\n",
       " ',',\n",
       " 'the',\n",
       " 'hole',\n",
       " ',',\n",
       " 'which',\n",
       " 'makes',\n",
       " 'many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'points',\n",
       " 'that',\n",
       " 'this',\n",
       " 'film',\n",
       " 'does',\n",
       " 'but',\n",
       " 'feels',\n",
       " 'less',\n",
       " 'repetitive',\n",
       " '']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter.dataset.examples[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'lr': 0.001,\n",
       " 'epochs': 10,\n",
       " 'kernel_sizes': [3, 4, 5],\n",
       " 'kernel_num': 100,\n",
       " 'embed_dim': 128,\n",
       " 'dropout': 0.5,\n",
       " 'cuda': True,\n",
       " 'snapshot': None,\n",
       " 'static': False,\n",
       " 'predict': None,\n",
       " 'test': False,\n",
       " 'log_interval': 1,\n",
       " 'test_interval': 100,\n",
       " 'early_stop': 1000,\n",
       " 'save_best': True,\n",
       " 'save_interval': 500,\n",
       " 'save_dir': 'snapshot/2019-06-05_15-00-47',\n",
       " 'embed_num': 21114,\n",
       " 'class_num': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "cnn = CNN_Text(args)\n",
    "if args.cuda:\n",
    "    cnn = cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch[100] - loss: 0.694505  acc: 57.0000%(37/64)\n",
      "Evaluation - loss: 0.670083  acc: 55.0000%(593/1066) \n",
      "\n",
      "Batch[101] - loss: 0.678370  acc: 59.0000%(38/64)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[200] - loss: 0.465207  acc: 87.0000%(56/64)\n",
      "Evaluation - loss: 0.618228  acc: 64.0000%(689/1066) \n",
      "\n",
      "Batch[300] - loss: 0.485058  acc: 78.0000%(47/60)\n",
      "Evaluation - loss: 0.621354  acc: 65.0000%(702/1066) \n",
      "\n",
      "Batch[400] - loss: 0.305611  acc: 90.0000%(58/64)\n",
      "Evaluation - loss: 0.599315  acc: 69.0000%(740/1066) \n",
      "\n",
      "Batch[500] - loss: 0.102855  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.618641  acc: 68.0000%(725/1066) \n",
      "\n",
      "Batch[600] - loss: 0.068792  acc: 100.0000%(60/60)\n",
      "Evaluation - loss: 0.625644  acc: 69.0000%(741/1066) \n",
      "\n",
      "Batch[700] - loss: 0.045885  acc: 98.0000%(63/64)\n",
      "Evaluation - loss: 0.668458  acc: 69.0000%(743/1066) \n",
      "\n",
      "Batch[800] - loss: 0.012878  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.686670  acc: 70.0000%(754/1066) \n",
      "\n",
      "Batch[900] - loss: 0.014656  acc: 100.0000%(60/60)\n",
      "Evaluation - loss: 0.712205  acc: 70.0000%(749/1066) \n",
      "\n",
      "Batch[1000] - loss: 0.008831  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.725757  acc: 70.0000%(756/1066) \n",
      "\n",
      "Batch[1100] - loss: 0.004772  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.751856  acc: 70.0000%(747/1066) \n",
      "\n",
      "Batch[1200] - loss: 0.005250  acc: 100.0000%(60/60)\n",
      "Evaluation - loss: 0.766666  acc: 70.0000%(751/1066) \n",
      "\n",
      "Batch[1300] - loss: 0.003259  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.779324  acc: 70.0000%(753/1066) \n",
      "\n",
      "Batch[1400] - loss: 0.003238  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.792569  acc: 70.0000%(753/1066) \n",
      "\n",
      "Batch[1500] - loss: 0.002870  acc: 100.0000%(60/60)\n",
      "Evaluation - loss: 0.801711  acc: 70.0000%(750/1066) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train or predict\n",
    "if args.predict is not None:\n",
    "    label = train.predict(args.predict, cnn, text_field, label_field, args.cuda)\n",
    "    print('\\n[Text]  {}\\n[Label] {}\\n'.format(args.predict, label))\n",
    "elif args.test:\n",
    "    try:\n",
    "        train.eval(test_iter, cnn, args) \n",
    "    except Exception as e:\n",
    "        print(\"\\nSorry. The test dataset doesn't exist.\\n\")\n",
    "else:\n",
    "    print()\n",
    "    try:\n",
    "        train.train(train_iter, dev_iter, cnn, args)\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\n' + '-' * 89)\n",
    "        print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Character-level Convolutional Networks for Text Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.pdnet_flair import Config, make_train_test_dev\n",
    "from haberrspd.charCNN.auxiliary import make_MJFF_data_loader\n",
    "from haberrspd.charCNN.train import run\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For deploy version, all of this will be placed in the parser\n",
    "args = Config(\n",
    "    # Model arguments\n",
    "    val_path='haberrspd/charCNN/data/dev.csv',\n",
    "    test_path='haberrspd/charCNN/data/test.csv',\n",
    "    train_path='haberrspd/charCNN/data/train.csv',\n",
    "    alphabet_path='haberrspd/charCNN/alphabet.json',# Where our alphabet lives\n",
    "    load_very_long_sentences=False, # Temporary fix to remove unreasonably long sequences\n",
    "    max_sample_length=8192, # The maximum length allowed for each training example\n",
    "    num_workers=1,\n",
    "    batch_size = 32,\n",
    "    lr=0.001, # Learning rate\n",
    "    epochs=100, # Set epochs here\n",
    "    max_norm=400,\n",
    "    optimizer='Adam',\n",
    "    class_weight=None,\n",
    "    dynamic_lr=False, # Dynamic learning rate, used for all but Adam\n",
    "    milestones = [5,10,15],\n",
    "    decay_factor = 0.5, # Rate by which we reduce the learning rate\n",
    "    kernel_sizes = [3,4,5],\n",
    "    kernel_num = 100,\n",
    "    embed_dim=128,\n",
    "    shuffle=False,\n",
    "    dropout=0.5,\n",
    "    cuda=True, # We have a GPU so let's use it\n",
    "    verbose=False,\n",
    "    continue_from='', # If we already trained a model we can continue from the stored one\n",
    "    checkpoint=False,\n",
    "    checkpoint_per_batch=10000,\n",
    "    save_folder='haberrspd/charCNN/run_results',\n",
    "    log_config=False,\n",
    "    log_result=False,\n",
    "    log_interval=1,\n",
    "    val_interval=200,\n",
    "    save_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from haberrspd/charCNN/data/train.csv\n",
      "Loading data from haberrspd/charCNN/data/dev.csv\n",
      "\n",
      "Number of training samples: 834\n",
      "\tLabel 0:           583\n",
      "\tLabel 1:           251\n",
      "\n",
      "Number of developing samples: 110\n",
      "\tLabel 0:            67\n",
      "\tLabel 1:            43\n",
      "\n",
      " Directory for saving results, already exists.\n",
      "\n",
      "Configuration:\n",
      "\tAlphabet path:          haberrspd/charCNN/alphabet.json\n",
      "\tBatch size:             64\n",
      "\tCheckpoint:             False\n",
      "\tCheckpoint per batch:   10000\n",
      "\tClass weight:           None\n",
      "\tContinue from:          \n",
      "\tCuda:                   True\n",
      "\tDecay factor:           0.5\n",
      "\tDropout:                0.5\n",
      "\tDynamic lr:             False\n",
      "\tEmbed dim:              128\n",
      "\tEpochs:                 100\n",
      "\tKernel num:             100\n",
      "\tKernel sizes:           [3, 4, 5]\n",
      "\tLoad very long sentences:False\n",
      "\tLog config:             False\n",
      "\tLog interval:           1\n",
      "\tLog result:             False\n",
      "\tLr:                     0.001\n",
      "\tMax norm:               400\n",
      "\tMax sample length:      8192\n",
      "\tMilestones:             [5, 10, 15]\n",
      "\tNum features:           70\n",
      "\tNum workers:            1\n",
      "\tOptimizer:              Adam\n",
      "\tSave folder:            haberrspd/charCNN/run_results\n",
      "\tSave interval:          1\n",
      "\tShuffle:                False\n",
      "\tTest path:              haberrspd/charCNN/data/test.csv\n",
      "\tTrain path:             haberrspd/charCNN/data/train.csv\n",
      "\tVal interval:           200\n",
      "\tVal path:               haberrspd/charCNN/data/dev.csv\n",
      "\tVerbose:                False\n",
      "CharCNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(70, 64, kernel_size=(32,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=8, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(16,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=130496, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5)\n",
      "  )\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (log_softmax): LogSoftmax()\n",
      ")\n",
      "\n",
      "Number of trainable model parameters: 67024514\n",
      "\n",
      "Epoch[1] Batch[1] - loss: 0.705399  lr: 0.00100  acc: 31.000% (20/64)\n",
      "Epoch[1] Batch[2] - loss: 3.948757  lr: 0.00100  acc: 68.000% (44/64)\n",
      "Epoch[1] Batch[3] - loss: 0.853415  lr: 0.00100  acc: 70.000% (45/64)\n",
      "Epoch[1] Batch[4] - loss: 0.790501  lr: 0.00100  acc: 31.000% (20/64)\n",
      "Epoch[1] Batch[5] - loss: 0.864967  lr: 0.00100  acc: 25.000% (16/64)\n",
      "Epoch[1] Batch[6] - loss: 0.814826  lr: 0.00100  acc: 31.000% (20/64)\n",
      "Epoch[1] Batch[7] - loss: 0.676103  lr: 0.00100  acc: 67.000% (43/64)\n",
      "Epoch[1] Batch[8] - loss: 0.634153  lr: 0.00100  acc: 68.000% (44/64)\n",
      "Epoch[1] Batch[9] - loss: 0.632376  lr: 0.00100  acc: 67.000% (43/64)\n",
      "Epoch[1] Batch[10] - loss: 0.644599  lr: 0.00100  acc: 64.000% (41/64)\n",
      "Epoch[1] Batch[11] - loss: 0.584497  lr: 0.00100  acc: 71.000% (46/64)\n",
      "Epoch[1] Batch[12] - loss: 0.610854  lr: 0.00100  acc: 67.000% (43/64)\n",
      "Epoch[1] Batch[13] - loss: 0.636763  lr: 0.00100  acc: 64.000% (41/64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 0.634460  lr: 0.00100  acc: 64.000% (41/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 64.1\u001b[0m% (41/64)      Recall: \u001b[32m100.0\u001b[0m% (41/41)      F-Score: \u001b[32m 78.1\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m  0.0\u001b[0m% (0/0)        Recall: \u001b[32m  0.0\u001b[0m% (0/23)       F-Score:   N/A\u001b[0m\n",
      "\n",
      "\n",
      "=> found better validated model, saving to haberrspd/charCNN/run_results/CharCNN_best.pth.tar\n",
      "\n",
      "\n",
      "Epoch[2] Batch[1] - loss: 0.569909  lr: 0.00100  acc: 71.000% (46/64)\n",
      "Epoch[2] Batch[2] - loss: 0.630582  lr: 0.00100  acc: 65.000% (42/64)\n",
      "Epoch[2] Batch[3] - loss: 0.574955  lr: 0.00100  acc: 70.000% (45/64)\n",
      "Epoch[2] Batch[4] - loss: 0.605607  lr: 0.00100  acc: 70.000% (45/64)\n",
      "Epoch[2] Batch[5] - loss: 0.710331  lr: 0.00100  acc: 60.000% (39/64)\n",
      "Epoch[2] Batch[6] - loss: 0.643850  lr: 0.00100  acc: 65.000% (42/64)\n",
      "Epoch[2] Batch[7] - loss: 0.593653  lr: 0.00100  acc: 67.000% (43/64)\n",
      "Epoch[2] Batch[8] - loss: 0.575147  lr: 0.00100  acc: 71.000% (46/64)\n",
      "Epoch[2] Batch[9] - loss: 0.618886  lr: 0.00100  acc: 65.000% (42/64)\n",
      "Epoch[2] Batch[10] - loss: 0.576641  lr: 0.00100  acc: 70.000% (45/64)\n",
      "Epoch[2] Batch[11] - loss: 0.530850  lr: 0.00100  acc: 75.000% (48/64)\n",
      "Epoch[2] Batch[12] - loss: 0.508706  lr: 0.00100  acc: 78.000% (50/64)\n",
      "Epoch[2] Batch[13] - loss: 0.538591  lr: 0.00100  acc: 76.000% (49/64)\n",
      "\n",
      "Evaluation - loss: 0.605712  lr: 0.00100  acc: 68.000% (44/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 68.8\u001b[0m% (44/64)      Recall: \u001b[32m100.0\u001b[0m% (44/44)      F-Score: \u001b[32m 81.5\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m  0.0\u001b[0m% (0/0)        Recall: \u001b[32m  0.0\u001b[0m% (0/20)       F-Score:   N/A\u001b[0m\n",
      "\n",
      "\n",
      "=> found better validated model, saving to haberrspd/charCNN/run_results/CharCNN_best.pth.tar\n",
      "\n",
      "\n",
      "Epoch[3] Batch[1] - loss: 0.582682  lr: 0.00100  acc: 68.000% (44/64)\n",
      "Epoch[3] Batch[2] - loss: 0.677572  lr: 0.00100  acc: 57.000% (37/64)\n",
      "Epoch[3] Batch[3] - loss: 0.708938  lr: 0.00100  acc: 57.000% (37/64)\n",
      "Epoch[3] Batch[4] - loss: 0.573248  lr: 0.00100  acc: 73.000% (47/64)\n",
      "Epoch[3] Batch[5] - loss: 0.486932  lr: 0.00100  acc: 81.000% (52/64)\n",
      "Epoch[3] Batch[6] - loss: 0.576192  lr: 0.00100  acc: 71.000% (46/64)\n",
      "Epoch[3] Batch[7] - loss: 0.561548  lr: 0.00100  acc: 70.000% (45/64)\n",
      "Epoch[3] Batch[8] - loss: 0.620036  lr: 0.00100  acc: 67.000% (43/64)\n",
      "Epoch[3] Batch[9] - loss: 0.496227  lr: 0.00100  acc: 79.000% (51/64)\n",
      "Epoch[3] Batch[10] - loss: 0.511088  lr: 0.00100  acc: 75.000% (48/64)\n",
      "Epoch[3] Batch[11] - loss: 0.489649  lr: 0.00100  acc: 76.000% (49/64)\n",
      "Epoch[3] Batch[12] - loss: 0.534940  lr: 0.00100  acc: 70.000% (45/64)\n",
      "Epoch[3] Batch[13] - loss: 0.465365  lr: 0.00100  acc: 82.000% (53/64)\n",
      "\n",
      "Evaluation - loss: 0.695571  lr: 0.00100  acc: 65.000% (42/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 62.7\u001b[0m% (37/59)      Recall: \u001b[32m100.0\u001b[0m% (37/37)      F-Score: \u001b[32m 77.1\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m100.0\u001b[0m% (5/5)        Recall: \u001b[32m 18.5\u001b[0m% (5/27)       F-Score: \u001b[32m 31.2\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[4] Batch[1] - loss: 0.533714  lr: 0.00100  acc: 75.000% (48/64)\n",
      "Epoch[4] Batch[2] - loss: 0.509367  lr: 0.00100  acc: 76.000% (49/64)\n",
      "Epoch[4] Batch[3] - loss: 0.580766  lr: 0.00100  acc: 70.000% (45/64)\n",
      "Epoch[4] Batch[4] - loss: 0.556788  lr: 0.00100  acc: 70.000% (45/64)\n",
      "Epoch[4] Batch[5] - loss: 0.530447  lr: 0.00100  acc: 78.000% (50/64)\n",
      "Epoch[4] Batch[6] - loss: 0.498941  lr: 0.00100  acc: 78.000% (50/64)\n",
      "Epoch[4] Batch[7] - loss: 0.511264  lr: 0.00100  acc: 79.000% (51/64)\n",
      "Epoch[4] Batch[8] - loss: 0.487352  lr: 0.00100  acc: 79.000% (51/64)\n",
      "Epoch[4] Batch[9] - loss: 0.472634  lr: 0.00100  acc: 81.000% (52/64)\n",
      "Epoch[4] Batch[10] - loss: 0.453812  lr: 0.00100  acc: 84.000% (54/64)\n",
      "Epoch[4] Batch[11] - loss: 0.511714  lr: 0.00100  acc: 73.000% (47/64)\n",
      "Epoch[4] Batch[12] - loss: 0.484318  lr: 0.00100  acc: 81.000% (52/64)\n",
      "Epoch[4] Batch[13] - loss: 0.504536  lr: 0.00100  acc: 75.000% (48/64)\n",
      "\n",
      "Evaluation - loss: 0.625949  lr: 0.00100  acc: 64.000% (41/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 66.0\u001b[0m% (31/47)      Recall: \u001b[32m 81.6\u001b[0m% (31/38)      F-Score: \u001b[32m 72.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 58.8\u001b[0m% (10/17)      Recall: \u001b[32m 38.5\u001b[0m% (10/26)      F-Score: \u001b[32m 46.5\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[5] Batch[1] - loss: 0.519576  lr: 0.00100  acc: 76.000% (49/64)\n",
      "Epoch[5] Batch[2] - loss: 0.556874  lr: 0.00100  acc: 73.000% (47/64)\n",
      "Epoch[5] Batch[3] - loss: 0.461562  lr: 0.00100  acc: 76.000% (49/64)\n",
      "Epoch[5] Batch[4] - loss: 0.444315  lr: 0.00100  acc: 85.000% (55/64)\n",
      "Epoch[5] Batch[5] - loss: 0.514790  lr: 0.00100  acc: 75.000% (48/64)\n",
      "Epoch[5] Batch[6] - loss: 0.389669  lr: 0.00100  acc: 84.000% (54/64)\n",
      "Epoch[5] Batch[7] - loss: 0.384725  lr: 0.00100  acc: 84.000% (54/64)\n",
      "Epoch[5] Batch[8] - loss: 0.324796  lr: 0.00100  acc: 92.000% (59/64)\n",
      "Epoch[5] Batch[9] - loss: 0.434890  lr: 0.00100  acc: 81.000% (52/64)\n",
      "Epoch[5] Batch[10] - loss: 0.338038  lr: 0.00100  acc: 84.000% (54/64)\n",
      "Epoch[5] Batch[11] - loss: 0.330088  lr: 0.00100  acc: 84.000% (54/64)\n",
      "Epoch[5] Batch[12] - loss: 0.414707  lr: 0.00100  acc: 79.000% (51/64)\n",
      "Epoch[5] Batch[13] - loss: 0.392191  lr: 0.00100  acc: 79.000% (51/64)\n",
      "\n",
      "Evaluation - loss: 0.591885  lr: 0.00100  acc: 76.000% (49/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 77.4\u001b[0m% (41/53)      Recall: \u001b[32m 93.2\u001b[0m% (41/44)      F-Score: \u001b[32m 84.5\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 72.7\u001b[0m% (8/11)       Recall: \u001b[32m 40.0\u001b[0m% (8/20)       F-Score: \u001b[32m 51.6\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "=> found better validated model, saving to haberrspd/charCNN/run_results/CharCNN_best.pth.tar\n",
      "\n",
      "\n",
      "Epoch[6] Batch[1] - loss: 0.329063  lr: 0.00100  acc: 85.000% (55/64)\n",
      "Epoch[6] Batch[2] - loss: 0.217652  lr: 0.00100  acc: 90.000% (58/64)\n",
      "Epoch[6] Batch[3] - loss: 0.263541  lr: 0.00100  acc: 87.000% (56/64)\n",
      "Epoch[6] Batch[4] - loss: 0.352606  lr: 0.00100  acc: 84.000% (54/64)\n",
      "Epoch[6] Batch[5] - loss: 0.340780  lr: 0.00100  acc: 82.000% (53/64)\n",
      "Epoch[6] Batch[6] - loss: 0.287080  lr: 0.00100  acc: 85.000% (55/64)\n",
      "Epoch[6] Batch[7] - loss: 0.224450  lr: 0.00100  acc: 95.000% (61/64)\n",
      "Epoch[6] Batch[8] - loss: 0.233915  lr: 0.00100  acc: 89.000% (57/64)\n",
      "Epoch[6] Batch[9] - loss: 0.283745  lr: 0.00100  acc: 87.000% (56/64)\n",
      "Epoch[6] Batch[10] - loss: 0.250701  lr: 0.00100  acc: 89.000% (57/64)\n",
      "Epoch[6] Batch[11] - loss: 0.289301  lr: 0.00100  acc: 87.000% (56/64)\n",
      "Epoch[6] Batch[12] - loss: 0.316964  lr: 0.00100  acc: 84.000% (54/64)\n",
      "Epoch[6] Batch[13] - loss: 0.193087  lr: 0.00100  acc: 96.000% (62/64)\n",
      "\n",
      "Evaluation - loss: 0.954663  lr: 0.00100  acc: 62.000% (40/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 64.7\u001b[0m% (33/51)      Recall: \u001b[32m 84.6\u001b[0m% (33/39)      F-Score: \u001b[32m 73.3\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 53.8\u001b[0m% (7/13)       Recall: \u001b[32m 28.0\u001b[0m% (7/25)       F-Score: \u001b[32m 36.8\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[7] Batch[1] - loss: 0.193600  lr: 0.00100  acc: 95.000% (61/64)\n",
      "Epoch[7] Batch[2] - loss: 0.208985  lr: 0.00100  acc: 95.000% (61/64)\n",
      "Epoch[7] Batch[3] - loss: 0.188700  lr: 0.00100  acc: 93.000% (60/64)\n",
      "Epoch[7] Batch[4] - loss: 0.116603  lr: 0.00100  acc: 95.000% (61/64)\n",
      "Epoch[7] Batch[5] - loss: 0.200464  lr: 0.00100  acc: 89.000% (57/64)\n",
      "Epoch[7] Batch[6] - loss: 0.103109  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[7] Batch[7] - loss: 0.166896  lr: 0.00100  acc: 95.000% (61/64)\n",
      "Epoch[7] Batch[8] - loss: 0.103451  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[7] Batch[9] - loss: 0.077337  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[7] Batch[10] - loss: 0.124661  lr: 0.00100  acc: 96.000% (62/64)\n",
      "Epoch[7] Batch[11] - loss: 0.096281  lr: 0.00100  acc: 96.000% (62/64)\n",
      "Epoch[7] Batch[12] - loss: 0.159343  lr: 0.00100  acc: 93.000% (60/64)\n",
      "Epoch[7] Batch[13] - loss: 0.207553  lr: 0.00100  acc: 92.000% (59/64)\n",
      "\n",
      "Evaluation - loss: 1.174894  lr: 0.00100  acc: 68.000% (44/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 72.0\u001b[0m% (36/50)      Recall: \u001b[32m 85.7\u001b[0m% (36/42)      F-Score: \u001b[32m 78.3\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 57.1\u001b[0m% (8/14)       Recall: \u001b[32m 36.4\u001b[0m% (8/22)       F-Score: \u001b[32m 44.4\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[8] Batch[1] - loss: 0.057951  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[8] Batch[2] - loss: 0.055556  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[8] Batch[3] - loss: 0.039897  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[8] Batch[4] - loss: 0.141627  lr: 0.00100  acc: 95.000% (61/64)\n",
      "Epoch[8] Batch[5] - loss: 0.038337  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[8] Batch[6] - loss: 0.041172  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[8] Batch[7] - loss: 0.088768  lr: 0.00100  acc: 96.000% (62/64)\n",
      "Epoch[8] Batch[8] - loss: 0.056978  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[8] Batch[9] - loss: 0.064587  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[8] Batch[10] - loss: 0.031846  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[8] Batch[11] - loss: 0.087102  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[8] Batch[12] - loss: 0.109484  lr: 0.00100  acc: 95.000% (61/64)\n",
      "Epoch[8] Batch[13] - loss: 0.060021  lr: 0.00100  acc: 98.000% (63/64)\n",
      "\n",
      "Evaluation - loss: 1.716031  lr: 0.00100  acc: 62.000% (40/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 62.7\u001b[0m% (32/51)      Recall: \u001b[32m 86.5\u001b[0m% (32/37)      F-Score: \u001b[32m 72.7\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 61.5\u001b[0m% (8/13)       Recall: \u001b[32m 29.6\u001b[0m% (8/27)       F-Score: \u001b[32m 40.0\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[9] Batch[1] - loss: 0.014003  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[9] Batch[2] - loss: 0.034546  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[9] Batch[3] - loss: 0.043089  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[9] Batch[4] - loss: 0.017783  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[9] Batch[5] - loss: 0.060693  lr: 0.00100  acc: 96.000% (62/64)\n",
      "Epoch[9] Batch[6] - loss: 0.050144  lr: 0.00100  acc: 96.000% (62/64)\n",
      "Epoch[9] Batch[7] - loss: 0.024363  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[9] Batch[8] - loss: 0.023680  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[9] Batch[9] - loss: 0.013988  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[9] Batch[10] - loss: 0.013491  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[9] Batch[11] - loss: 0.043068  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[9] Batch[12] - loss: 0.012541  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[9] Batch[13] - loss: 0.044673  lr: 0.00100  acc: 98.000% (63/64)\n",
      "\n",
      "Evaluation - loss: 1.581485  lr: 0.00100  acc: 68.000% (44/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 68.9\u001b[0m% (31/45)      Recall: \u001b[32m 83.8\u001b[0m% (31/37)      F-Score: \u001b[32m 75.6\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 68.4\u001b[0m% (13/19)      Recall: \u001b[32m 48.1\u001b[0m% (13/27)      F-Score: \u001b[32m 56.5\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[10] Batch[1] - loss: 0.014307  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[10] Batch[2] - loss: 0.016555  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[10] Batch[3] - loss: 0.014069  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[10] Batch[4] - loss: 0.011966  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[10] Batch[5] - loss: 0.041667  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[10] Batch[6] - loss: 0.026776  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[10] Batch[7] - loss: 0.007327  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[10] Batch[8] - loss: 0.021739  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[10] Batch[9] - loss: 0.014520  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[10] Batch[10] - loss: 0.028200  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[10] Batch[11] - loss: 0.010054  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[10] Batch[12] - loss: 0.016528  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[10] Batch[13] - loss: 0.010283  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 1.846001  lr: 0.00100  acc: 65.000% (42/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 67.3\u001b[0m% (33/49)      Recall: \u001b[32m 84.6\u001b[0m% (33/39)      F-Score: \u001b[32m 75.0\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 60.0\u001b[0m% (9/15)       Recall: \u001b[32m 36.0\u001b[0m% (9/25)       F-Score: \u001b[32m 45.0\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[11] Batch[1] - loss: 0.012866  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[2] - loss: 0.008412  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[3] - loss: 0.011870  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[4] - loss: 0.007309  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[5] - loss: 0.003650  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[6] - loss: 0.009648  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[7] - loss: 0.006049  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[8] - loss: 0.008032  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[9] - loss: 0.012189  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[10] - loss: 0.002996  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[11] - loss: 0.017209  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[12] - loss: 0.009933  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[11] Batch[13] - loss: 0.008039  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.340681  lr: 0.00100  acc: 64.000% (41/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 64.4\u001b[0m% (29/45)      Recall: \u001b[32m 80.6\u001b[0m% (29/36)      F-Score: \u001b[32m 71.6\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 63.2\u001b[0m% (12/19)      Recall: \u001b[32m 42.9\u001b[0m% (12/28)      F-Score: \u001b[32m 51.1\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[12] Batch[1] - loss: 0.016894  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[12] Batch[2] - loss: 0.021842  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[12] Batch[3] - loss: 0.004795  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[12] Batch[4] - loss: 0.011721  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[12] Batch[5] - loss: 0.006720  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[12] Batch[6] - loss: 0.005959  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[12] Batch[7] - loss: 0.002105  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[12] Batch[8] - loss: 0.005144  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[12] Batch[9] - loss: 0.009732  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[12] Batch[10] - loss: 0.002049  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[12] Batch[11] - loss: 0.001679  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[12] Batch[12] - loss: 0.025765  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[12] Batch[13] - loss: 0.006250  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.088461  lr: 0.00100  acc: 67.000% (43/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 70.2\u001b[0m% (33/47)      Recall: \u001b[32m 82.5\u001b[0m% (33/40)      F-Score: \u001b[32m 75.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 58.8\u001b[0m% (10/17)      Recall: \u001b[32m 41.7\u001b[0m% (10/24)      F-Score: \u001b[32m 48.8\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[13] Batch[1] - loss: 0.002646  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[2] - loss: 0.001363  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[3] - loss: 0.003949  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[4] - loss: 0.005389  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[5] - loss: 0.004384  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[6] - loss: 0.002893  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[7] - loss: 0.001169  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[8] - loss: 0.001912  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[9] - loss: 0.002568  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[10] - loss: 0.001523  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[11] - loss: 0.003622  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[12] - loss: 0.004019  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[13] Batch[13] - loss: 0.001212  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.582283  lr: 0.00100  acc: 60.000% (39/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 65.3\u001b[0m% (32/49)      Recall: \u001b[32m 80.0\u001b[0m% (32/40)      F-Score: \u001b[32m 71.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 46.7\u001b[0m% (7/15)       Recall: \u001b[32m 29.2\u001b[0m% (7/24)       F-Score: \u001b[32m 35.9\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[14] Batch[1] - loss: 0.000697  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[2] - loss: 0.015006  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[3] - loss: 0.002191  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[4] - loss: 0.003179  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[5] - loss: 0.001386  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[6] - loss: 0.002306  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[7] - loss: 0.002940  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[8] - loss: 0.002554  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[9] - loss: 0.005070  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[10] - loss: 0.001387  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[11] - loss: 0.002530  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[12] - loss: 0.001645  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[14] Batch[13] - loss: 0.001228  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.766119  lr: 0.00100  acc: 65.000% (42/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 62.7\u001b[0m% (32/51)      Recall: \u001b[32m 91.4\u001b[0m% (32/35)      F-Score: \u001b[32m 74.4\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 76.9\u001b[0m% (10/13)      Recall: \u001b[32m 34.5\u001b[0m% (10/29)      F-Score: \u001b[32m 47.6\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[15] Batch[1] - loss: 0.001913  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[2] - loss: 0.008773  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[3] - loss: 0.007963  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[4] - loss: 0.002270  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[5] - loss: 0.003462  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[6] - loss: 0.005186  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[7] - loss: 0.008837  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[8] - loss: 0.005363  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[9] - loss: 0.001353  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[10] - loss: 0.001197  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[11] - loss: 0.008874  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[12] - loss: 0.005367  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[15] Batch[13] - loss: 0.001140  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.064883  lr: 0.00100  acc: 70.000% (45/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 71.7\u001b[0m% (33/46)      Recall: \u001b[32m 84.6\u001b[0m% (33/39)      F-Score: \u001b[32m 77.6\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 66.7\u001b[0m% (12/18)      Recall: \u001b[32m 48.0\u001b[0m% (12/25)      F-Score: \u001b[32m 55.8\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[16] Batch[1] - loss: 0.007107  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[2] - loss: 0.000763  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[3] - loss: 0.001548  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[4] - loss: 0.002212  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[5] - loss: 0.001920  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[6] - loss: 0.002941  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[7] - loss: 0.009281  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[8] - loss: 0.000333  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[9] - loss: 0.001135  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[10] - loss: 0.012230  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[11] - loss: 0.000640  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[12] - loss: 0.000628  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[16] Batch[13] - loss: 0.005917  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.490709  lr: 0.00100  acc: 65.000% (42/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 66.0\u001b[0m% (33/50)      Recall: \u001b[32m 86.8\u001b[0m% (33/38)      F-Score: \u001b[32m 75.0\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 64.3\u001b[0m% (9/14)       Recall: \u001b[32m 34.6\u001b[0m% (9/26)       F-Score: \u001b[32m 45.0\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[17] Batch[1] - loss: 0.008139  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[2] - loss: 0.000529  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[3] - loss: 0.000755  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[4] - loss: 0.003773  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[5] - loss: 0.001949  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[6] - loss: 0.000479  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[7] - loss: 0.004751  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[8] - loss: 0.005503  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[9] - loss: 0.004053  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[10] - loss: 0.000786  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[11] - loss: 0.004100  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[12] - loss: 0.005166  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[17] Batch[13] - loss: 0.000763  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.650161  lr: 0.00100  acc: 68.000% (44/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 75.0\u001b[0m% (36/48)      Recall: \u001b[32m 81.8\u001b[0m% (36/44)      F-Score: \u001b[32m 78.3\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 50.0\u001b[0m% (8/16)       Recall: \u001b[32m 40.0\u001b[0m% (8/20)       F-Score: \u001b[32m 44.4\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[18] Batch[1] - loss: 0.004947  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[2] - loss: 0.001642  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[3] - loss: 0.001565  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[4] - loss: 0.001654  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[5] - loss: 0.001564  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[6] - loss: 0.000812  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[7] - loss: 0.001197  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[8] - loss: 0.001568  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[9] - loss: 0.002878  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[10] - loss: 0.000601  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[11] - loss: 0.000975  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[12] - loss: 0.003537  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[18] Batch[13] - loss: 0.000851  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 1.979625  lr: 0.00100  acc: 70.000% (45/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 68.8\u001b[0m% (33/48)      Recall: \u001b[32m 89.2\u001b[0m% (33/37)      F-Score: \u001b[32m 77.6\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 75.0\u001b[0m% (12/16)      Recall: \u001b[32m 44.4\u001b[0m% (12/27)      F-Score: \u001b[32m 55.8\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[19] Batch[1] - loss: 0.000680  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[2] - loss: 0.003227  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[3] - loss: 0.000940  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[4] - loss: 0.000400  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[5] - loss: 0.002370  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[6] - loss: 0.001948  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[7] - loss: 0.000626  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[8] - loss: 0.000115  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[9] - loss: 0.000690  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[10] - loss: 0.001141  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[11] - loss: 0.000255  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[12] - loss: 0.001521  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[19] Batch[13] - loss: 0.000238  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.662431  lr: 0.00100  acc: 67.000% (43/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 68.6\u001b[0m% (35/51)      Recall: \u001b[32m 87.5\u001b[0m% (35/40)      F-Score: \u001b[32m 76.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 61.5\u001b[0m% (8/13)       Recall: \u001b[32m 33.3\u001b[0m% (8/24)       F-Score: \u001b[32m 43.2\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[20] Batch[1] - loss: 0.003599  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[2] - loss: 0.002097  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[3] - loss: 0.000189  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[4] - loss: 0.000220  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[5] - loss: 0.000557  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[6] - loss: 0.000600  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[7] - loss: 0.000211  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[8] - loss: 0.000676  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[9] - loss: 0.000309  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[10] - loss: 0.001168  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[11] - loss: 0.000179  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[12] - loss: 0.000803  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[20] Batch[13] - loss: 0.001163  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.834430  lr: 0.00100  acc: 62.000% (40/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 66.0\u001b[0m% (31/47)      Recall: \u001b[32m 79.5\u001b[0m% (31/39)      F-Score: \u001b[32m 72.1\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 52.9\u001b[0m% (9/17)       Recall: \u001b[32m 36.0\u001b[0m% (9/25)       F-Score: \u001b[32m 42.9\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[21] Batch[1] - loss: 0.001800  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[2] - loss: 0.001810  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[3] - loss: 0.001160  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[4] - loss: 0.000431  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[5] - loss: 0.000653  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[6] - loss: 0.000467  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[7] - loss: 0.001387  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[8] - loss: 0.000952  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[9] - loss: 0.000675  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[10] - loss: 0.001391  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[11] - loss: 0.000369  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[12] - loss: 0.000256  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[21] Batch[13] - loss: 0.000155  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.739967  lr: 0.00100  acc: 67.000% (43/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 68.6\u001b[0m% (35/51)      Recall: \u001b[32m 87.5\u001b[0m% (35/40)      F-Score: \u001b[32m 76.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 61.5\u001b[0m% (8/13)       Recall: \u001b[32m 33.3\u001b[0m% (8/24)       F-Score: \u001b[32m 43.2\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[22] Batch[1] - loss: 0.000446  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[2] - loss: 0.000387  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[3] - loss: 0.015941  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[22] Batch[4] - loss: 0.000554  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[5] - loss: 0.002914  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[6] - loss: 0.000736  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[7] - loss: 0.008620  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[8] - loss: 0.000597  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[9] - loss: 0.000637  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[10] - loss: 0.000489  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[11] - loss: 0.000598  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[12] - loss: 0.000751  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[22] Batch[13] - loss: 0.000416  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.388442  lr: 0.00100  acc: 68.000% (44/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 74.4\u001b[0m% (32/43)      Recall: \u001b[32m 78.0\u001b[0m% (32/41)      F-Score: \u001b[32m 76.2\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 57.1\u001b[0m% (12/21)      Recall: \u001b[32m 52.2\u001b[0m% (12/23)      F-Score: \u001b[32m 54.5\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[23] Batch[1] - loss: 0.000348  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[2] - loss: 0.001004  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[3] - loss: 0.000614  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[4] - loss: 0.001594  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[5] - loss: 0.000974  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[6] - loss: 0.000327  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[7] - loss: 0.002300  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[8] - loss: 0.000333  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[9] - loss: 0.001303  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[10] - loss: 0.002296  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[11] - loss: 0.000155  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[12] - loss: 0.000154  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[23] Batch[13] - loss: 0.000167  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.929245  lr: 0.00100  acc: 59.000% (38/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 65.2\u001b[0m% (30/46)      Recall: \u001b[32m 75.0\u001b[0m% (30/40)      F-Score: \u001b[32m 69.8\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 44.4\u001b[0m% (8/18)       Recall: \u001b[32m 33.3\u001b[0m% (8/24)       F-Score: \u001b[32m 38.1\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[24] Batch[1] - loss: 0.000616  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[2] - loss: 0.000300  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[3] - loss: 0.000299  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[4] - loss: 0.000272  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[5] - loss: 0.001697  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[6] - loss: 0.000550  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[7] - loss: 0.003848  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[8] - loss: 0.000333  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[9] - loss: 0.000243  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[10] - loss: 0.000403  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[11] - loss: 0.001576  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[12] - loss: 0.000405  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[24] Batch[13] - loss: 0.000594  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.638166  lr: 0.00100  acc: 68.000% (44/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 68.0\u001b[0m% (34/50)      Recall: \u001b[32m 89.5\u001b[0m% (34/38)      F-Score: \u001b[32m 77.3\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 71.4\u001b[0m% (10/14)      Recall: \u001b[32m 38.5\u001b[0m% (10/26)      F-Score: \u001b[32m 50.0\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[25] Batch[1] - loss: 0.000101  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[2] - loss: 0.010039  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[3] - loss: 0.000091  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[4] - loss: 0.002104  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[5] - loss: 0.000200  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[6] - loss: 0.000467  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[7] - loss: 0.000924  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[8] - loss: 0.000417  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[9] - loss: 0.001772  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[10] - loss: 0.000495  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[11] - loss: 0.003474  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[12] - loss: 0.000519  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[25] Batch[13] - loss: 0.000290  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.464581  lr: 0.00100  acc: 71.000% (46/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 76.7\u001b[0m% (33/43)      Recall: \u001b[32m 80.5\u001b[0m% (33/41)      F-Score: \u001b[32m 78.6\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 61.9\u001b[0m% (13/21)      Recall: \u001b[32m 56.5\u001b[0m% (13/23)      F-Score: \u001b[32m 59.1\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[26] Batch[1] - loss: 0.001270  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[2] - loss: 0.001872  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[3] - loss: 0.001541  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[4] - loss: 0.000340  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[5] - loss: 0.000162  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[6] - loss: 0.000300  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[7] - loss: 0.000135  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[8] - loss: 0.000441  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[9] - loss: 0.000569  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[10] - loss: 0.000669  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[11] - loss: 0.000191  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[12] - loss: 0.003451  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[26] Batch[13] - loss: 0.001169  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.486440  lr: 0.00100  acc: 71.000% (46/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 73.6\u001b[0m% (39/53)      Recall: \u001b[32m 90.7\u001b[0m% (39/43)      F-Score: \u001b[32m 81.2\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 63.6\u001b[0m% (7/11)       Recall: \u001b[32m 33.3\u001b[0m% (7/21)       F-Score: \u001b[32m 43.8\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[27] Batch[1] - loss: 0.000355  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[2] - loss: 0.000239  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[3] - loss: 0.000308  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[4] - loss: 0.000203  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[5] - loss: 0.000238  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[6] - loss: 0.000520  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[7] - loss: 0.000325  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[8] - loss: 0.000203  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[9] - loss: 0.001473  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[10] - loss: 0.000363  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[11] - loss: 0.000170  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[12] - loss: 0.001913  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[27] Batch[13] - loss: 0.000123  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.495324  lr: 0.00100  acc: 60.000% (39/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 64.6\u001b[0m% (31/48)      Recall: \u001b[32m 79.5\u001b[0m% (31/39)      F-Score: \u001b[32m 71.3\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 50.0\u001b[0m% (8/16)       Recall: \u001b[32m 32.0\u001b[0m% (8/25)       F-Score: \u001b[32m 39.0\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[28] Batch[1] - loss: 0.000076  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[2] - loss: 0.000301  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[3] - loss: 0.000243  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[4] - loss: 0.000418  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[5] - loss: 0.000373  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[6] - loss: 0.000317  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[7] - loss: 0.003416  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[8] - loss: 0.000209  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[9] - loss: 0.000340  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[10] - loss: 0.000094  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[11] - loss: 0.003189  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[12] - loss: 0.000266  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[28] Batch[13] - loss: 0.000072  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 1.925454  lr: 0.00100  acc: 78.000% (50/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 80.4\u001b[0m% (37/46)      Recall: \u001b[32m 88.1\u001b[0m% (37/42)      F-Score: \u001b[32m 84.1\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 72.2\u001b[0m% (13/18)      Recall: \u001b[32m 59.1\u001b[0m% (13/22)      F-Score: \u001b[32m 65.0\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "=> found better validated model, saving to haberrspd/charCNN/run_results/CharCNN_best.pth.tar\n",
      "\n",
      "\n",
      "Epoch[29] Batch[1] - loss: 0.001825  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[2] - loss: 0.001827  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[3] - loss: 0.000093  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[4] - loss: 0.000096  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[5] - loss: 0.000128  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[6] - loss: 0.000139  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[7] - loss: 0.000108  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[8] - loss: 0.000217  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[9] - loss: 0.000057  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[10] - loss: 0.002114  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[11] - loss: 0.000097  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[12] - loss: 0.000327  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[29] Batch[13] - loss: 0.000100  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.511997  lr: 0.00100  acc: 67.000% (43/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 73.3\u001b[0m% (33/45)      Recall: \u001b[32m 78.6\u001b[0m% (33/42)      F-Score: \u001b[32m 75.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 52.6\u001b[0m% (10/19)      Recall: \u001b[32m 45.5\u001b[0m% (10/22)      F-Score: \u001b[32m 48.8\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[30] Batch[1] - loss: 0.000092  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[2] - loss: 0.000059  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[3] - loss: 0.000221  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[4] - loss: 0.000082  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[5] - loss: 0.000148  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[6] - loss: 0.001799  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[7] - loss: 0.000523  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[8] - loss: 0.000355  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[9] - loss: 0.000081  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[10] - loss: 0.000473  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[11] - loss: 0.000172  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[12] - loss: 0.000138  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[30] Batch[13] - loss: 0.000108  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.119711  lr: 0.00100  acc: 65.000% (42/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 70.2\u001b[0m% (33/47)      Recall: \u001b[32m 80.5\u001b[0m% (33/41)      F-Score: \u001b[32m 75.0\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 52.9\u001b[0m% (9/17)       Recall: \u001b[32m 39.1\u001b[0m% (9/23)       F-Score: \u001b[32m 45.0\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[31] Batch[1] - loss: 0.000130  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[2] - loss: 0.000659  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[3] - loss: 0.000077  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[4] - loss: 0.001791  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[5] - loss: 0.000150  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[6] - loss: 0.000231  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[7] - loss: 0.000306  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[8] - loss: 0.000094  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[9] - loss: 0.000069  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[10] - loss: 0.000120  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[11] - loss: 0.001218  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[12] - loss: 0.000305  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[31] Batch[13] - loss: 0.000141  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.714282  lr: 0.00100  acc: 67.000% (43/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 70.2\u001b[0m% (33/47)      Recall: \u001b[32m 82.5\u001b[0m% (33/40)      F-Score: \u001b[32m 75.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 58.8\u001b[0m% (10/17)      Recall: \u001b[32m 41.7\u001b[0m% (10/24)      F-Score: \u001b[32m 48.8\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[32] Batch[1] - loss: 0.000067  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[2] - loss: 0.002047  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[3] - loss: 0.000082  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[4] - loss: 0.000098  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[5] - loss: 0.000837  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[6] - loss: 0.000154  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[7] - loss: 0.000101  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[8] - loss: 0.000081  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[9] - loss: 0.000242  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[10] - loss: 0.000166  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[11] - loss: 0.000625  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[12] - loss: 0.000052  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[32] Batch[13] - loss: 0.000510  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.721965  lr: 0.00100  acc: 67.000% (43/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 64.6\u001b[0m% (31/48)      Recall: \u001b[32m 88.6\u001b[0m% (31/35)      F-Score: \u001b[32m 74.7\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 75.0\u001b[0m% (12/16)      Recall: \u001b[32m 41.4\u001b[0m% (12/29)      F-Score: \u001b[32m 53.3\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[33] Batch[1] - loss: 0.000812  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[2] - loss: 0.000245  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[3] - loss: 0.000442  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[4] - loss: 0.000049  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[5] - loss: 0.000058  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[6] - loss: 0.000191  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[7] - loss: 0.000048  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[8] - loss: 0.000252  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[9] - loss: 0.000055  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[10] - loss: 0.000105  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[11] - loss: 0.000061  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[12] - loss: 0.000123  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[33] Batch[13] - loss: 0.000134  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.745963  lr: 0.00100  acc: 64.000% (41/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 66.7\u001b[0m% (32/48)      Recall: \u001b[32m 82.1\u001b[0m% (32/39)      F-Score: \u001b[32m 73.6\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 56.2\u001b[0m% (9/16)       Recall: \u001b[32m 36.0\u001b[0m% (9/25)       F-Score: \u001b[32m 43.9\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[34] Batch[1] - loss: 0.001981  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[2] - loss: 0.000315  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[3] - loss: 0.000150  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[4] - loss: 0.000227  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[5] - loss: 0.000186  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[6] - loss: 0.000343  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[7] - loss: 0.000066  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[8] - loss: 0.000069  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[9] - loss: 0.000066  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[10] - loss: 0.000051  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[11] - loss: 0.000060  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[12] - loss: 0.000045  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[34] Batch[13] - loss: 0.000349  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.445120  lr: 0.00100  acc: 65.000% (42/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 66.7\u001b[0m% (30/45)      Recall: \u001b[32m 81.1\u001b[0m% (30/37)      F-Score: \u001b[32m 73.2\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 63.2\u001b[0m% (12/19)      Recall: \u001b[32m 44.4\u001b[0m% (12/27)      F-Score: \u001b[32m 52.2\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[35] Batch[1] - loss: 0.000072  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[2] - loss: 0.000103  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[3] - loss: 0.000270  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[4] - loss: 0.000059  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[5] - loss: 0.000043  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[6] - loss: 0.000148  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[7] - loss: 0.002592  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[8] - loss: 0.005362  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[9] - loss: 0.000101  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[10] - loss: 0.000027  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[11] - loss: 0.000165  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[12] - loss: 0.000378  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[35] Batch[13] - loss: 0.000913  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.449030  lr: 0.00100  acc: 62.000% (40/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 68.1\u001b[0m% (32/47)      Recall: \u001b[32m 78.0\u001b[0m% (32/41)      F-Score: \u001b[32m 72.7\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 47.1\u001b[0m% (8/17)       Recall: \u001b[32m 34.8\u001b[0m% (8/23)       F-Score: \u001b[32m 40.0\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[36] Batch[1] - loss: 0.000066  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[2] - loss: 0.000088  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[3] - loss: 0.000220  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[4] - loss: 0.001755  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[5] - loss: 0.000099  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[6] - loss: 0.000106  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[7] - loss: 0.000051  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[8] - loss: 0.005355  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[9] - loss: 0.000127  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[10] - loss: 0.000228  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[11] - loss: 0.000201  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[12] - loss: 0.000781  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[36] Batch[13] - loss: 0.000067  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 2.846512  lr: 0.00100  acc: 65.000% (42/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 64.4\u001b[0m% (29/45)      Recall: \u001b[32m 82.9\u001b[0m% (29/35)      F-Score: \u001b[32m 72.5\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 68.4\u001b[0m% (13/19)      Recall: \u001b[32m 44.8\u001b[0m% (13/29)      F-Score: \u001b[32m 54.2\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[37] Batch[1] - loss: 0.000097  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[2] - loss: 0.000153  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[3] - loss: 0.000147  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[4] - loss: 0.000113  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[5] - loss: 0.000235  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[6] - loss: 0.000123  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[7] - loss: 0.000203  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[8] - loss: 0.000122  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[9] - loss: 0.000239  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[10] - loss: 0.000156  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[11] - loss: 0.001994  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[12] - loss: 0.000316  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[37] Batch[13] - loss: 0.000037  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.921016  lr: 0.00100  acc: 60.000% (39/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 64.4\u001b[0m% (29/45)      Recall: \u001b[32m 76.3\u001b[0m% (29/38)      F-Score: \u001b[32m 69.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 52.6\u001b[0m% (10/19)      Recall: \u001b[32m 38.5\u001b[0m% (10/26)      F-Score: \u001b[32m 44.4\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[38] Batch[1] - loss: 0.000358  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[2] - loss: 0.000101  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[3] - loss: 0.001761  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[4] - loss: 0.000129  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[5] - loss: 0.000909  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[6] - loss: 0.017597  lr: 0.00100  acc: 98.000% (63/64)\n",
      "Epoch[38] Batch[7] - loss: 0.000149  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[8] - loss: 0.000106  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[9] - loss: 0.000034  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[10] - loss: 0.000162  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[11] - loss: 0.000111  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[12] - loss: 0.000226  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[38] Batch[13] - loss: 0.000798  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.049891  lr: 0.00100  acc: 67.000% (43/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 71.1\u001b[0m% (32/45)      Recall: \u001b[32m 80.0\u001b[0m% (32/40)      F-Score: \u001b[32m 75.3\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 57.9\u001b[0m% (11/19)      Recall: \u001b[32m 45.8\u001b[0m% (11/24)      F-Score: \u001b[32m 51.2\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[39] Batch[1] - loss: 0.000730  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[2] - loss: 0.000731  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[3] - loss: 0.000175  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[4] - loss: 0.000111  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[5] - loss: 0.000156  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[6] - loss: 0.000242  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[7] - loss: 0.000256  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[8] - loss: 0.000274  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[9] - loss: 0.000304  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[10] - loss: 0.000365  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[11] - loss: 0.000320  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[12] - loss: 0.000073  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[39] Batch[13] - loss: 0.000200  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.963616  lr: 0.00100  acc: 60.000% (39/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 66.7\u001b[0m% (32/48)      Recall: \u001b[32m 78.0\u001b[0m% (32/41)      F-Score: \u001b[32m 71.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 43.8\u001b[0m% (7/16)       Recall: \u001b[32m 30.4\u001b[0m% (7/23)       F-Score: \u001b[32m 35.9\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[40] Batch[1] - loss: 0.001367  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[2] - loss: 0.000575  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[3] - loss: 0.000916  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[4] - loss: 0.000156  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[5] - loss: 0.000045  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[6] - loss: 0.000094  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[7] - loss: 0.000863  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[8] - loss: 0.000263  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[9] - loss: 0.000144  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[10] - loss: 0.000054  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[11] - loss: 0.000190  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[12] - loss: 0.000161  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[40] Batch[13] - loss: 0.000041  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.448443  lr: 0.00100  acc: 65.000% (42/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 66.0\u001b[0m% (31/47)      Recall: \u001b[32m 83.8\u001b[0m% (31/37)      F-Score: \u001b[32m 73.8\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 64.7\u001b[0m% (11/17)      Recall: \u001b[32m 40.7\u001b[0m% (11/27)      F-Score: \u001b[32m 50.0\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[41] Batch[1] - loss: 0.000068  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[2] - loss: 0.000172  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[3] - loss: 0.000099  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[4] - loss: 0.000558  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[5] - loss: 0.000109  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[6] - loss: 0.000749  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[7] - loss: 0.000044  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[8] - loss: 0.000608  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[9] - loss: 0.000122  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[10] - loss: 0.000149  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[11] - loss: 0.000112  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[12] - loss: 0.000134  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[41] Batch[13] - loss: 0.000328  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.496446  lr: 0.00100  acc: 67.000% (43/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 68.6\u001b[0m% (35/51)      Recall: \u001b[32m 87.5\u001b[0m% (35/40)      F-Score: \u001b[32m 76.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 61.5\u001b[0m% (8/13)       Recall: \u001b[32m 33.3\u001b[0m% (8/24)       F-Score: \u001b[32m 43.2\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[42] Batch[1] - loss: 0.000092  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[2] - loss: 0.000592  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[3] - loss: 0.000077  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[4] - loss: 0.000013  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[5] - loss: 0.000029  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[6] - loss: 0.000097  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[7] - loss: 0.000039  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[8] - loss: 0.000483  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[9] - loss: 0.000034  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[10] - loss: 0.000475  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[11] - loss: 0.000097  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[12] - loss: 0.000522  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[42] Batch[13] - loss: 0.000074  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.845570  lr: 0.00100  acc: 60.000% (39/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 63.8\u001b[0m% (30/47)      Recall: \u001b[32m 78.9\u001b[0m% (30/38)      F-Score: \u001b[32m 70.6\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 52.9\u001b[0m% (9/17)       Recall: \u001b[32m 34.6\u001b[0m% (9/26)       F-Score: \u001b[32m 41.9\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[43] Batch[1] - loss: 0.000184  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[2] - loss: 0.000112  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[3] - loss: 0.000099  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[4] - loss: 0.000323  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[5] - loss: 0.000615  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[6] - loss: 0.000174  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[7] - loss: 0.000120  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[8] - loss: 0.001720  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[9] - loss: 0.001246  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[10] - loss: 0.000047  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[11] - loss: 0.000346  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[12] - loss: 0.000153  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[43] Batch[13] - loss: 0.000286  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.465947  lr: 0.00100  acc: 60.000% (39/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 65.3\u001b[0m% (32/49)      Recall: \u001b[32m 80.0\u001b[0m% (32/40)      F-Score: \u001b[32m 71.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 46.7\u001b[0m% (7/15)       Recall: \u001b[32m 29.2\u001b[0m% (7/24)       F-Score: \u001b[32m 35.9\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[44] Batch[1] - loss: 0.000450  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[2] - loss: 0.000093  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[3] - loss: 0.000109  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[4] - loss: 0.000341  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[5] - loss: 0.000061  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[6] - loss: 0.000262  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[7] - loss: 0.000035  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[8] - loss: 0.000030  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[9] - loss: 0.000082  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[10] - loss: 0.000022  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[11] - loss: 0.006391  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[12] - loss: 0.000367  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[44] Batch[13] - loss: 0.000035  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.185978  lr: 0.00100  acc: 64.000% (41/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 64.6\u001b[0m% (31/48)      Recall: \u001b[32m 83.8\u001b[0m% (31/37)      F-Score: \u001b[32m 72.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 62.5\u001b[0m% (10/16)      Recall: \u001b[32m 37.0\u001b[0m% (10/27)      F-Score: \u001b[32m 46.5\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[45] Batch[1] - loss: 0.003371  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[2] - loss: 0.000443  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[3] - loss: 0.000065  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[4] - loss: 0.000098  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[5] - loss: 0.001240  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[6] - loss: 0.000099  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[7] - loss: 0.000416  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[8] - loss: 0.000061  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[9] - loss: 0.003595  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[10] - loss: 0.000056  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[11] - loss: 0.000099  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[12] - loss: 0.000894  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[45] Batch[13] - loss: 0.000073  lr: 0.00100  acc: 100.000% (64/64)\n",
      "\n",
      "Evaluation - loss: 3.363508  lr: 0.00100  acc: 64.000% (41/64) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 70.5\u001b[0m% (31/44)      Recall: \u001b[32m 75.6\u001b[0m% (31/41)      F-Score: \u001b[32m 72.9\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 50.0\u001b[0m% (10/20)      Recall: \u001b[32m 43.5\u001b[0m% (10/23)      F-Score: \u001b[32m 46.5\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[46] Batch[1] - loss: 0.000149  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[46] Batch[2] - loss: 0.000071  lr: 0.00100  acc: 100.000% (64/64)\n",
      "Epoch[46] Batch[3] - loss: 0.000109  lr: 0.00100  acc: 100.000% (64/64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b28569b7bbef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cloud/haberrspd/haberrspd/charCNN/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    262\u001b[0m           \u001b[0mdev_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m           args)\n\u001b[0m",
      "\u001b[0;32m~/cloud/haberrspd/haberrspd/charCNN/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, dev_loader, model, args)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_lr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'Adam'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_batch\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available() is True\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 64, 11])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 70\n",
    "x = torch.ones(2**13, batch_size, 50) # Input    \n",
    "m = nn.Sequential(\n",
    "    nn.Conv1d(batch_size, 64, kernel_size=16, stride=1), # Function\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=3,stride=3)\n",
    ")\n",
    "out = m(x)\n",
    "print(out.size())\n",
    "# print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 448])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.view(out.size(0),-1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
