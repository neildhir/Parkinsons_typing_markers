{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/neil/cloud/habitual_errors_NLP\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline\n",
    "from haberrspd.__init__ import *\n",
    "from haberrspd.__init_paths import data_root\n",
    "\n",
    "# goto the correct folder just for practicality's sake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convolutional Neural Networks for Sentence Classification** adapted for use with characters embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "import haberrspd.charCNN.mydatasets as mydatasets\n",
    "import torch # To set the device\n",
    "import datetime\n",
    "import os\n",
    "from haberrspd.charCNN.model import CNN_Text\n",
    "import haberrspd.charCNN.train as train\n",
    "from haberrspd.pdnet_flair import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Config(\n",
    "    # Arguments\n",
    "    batch_size = 64,\n",
    "    lr=0.001,\n",
    "    epochs=10,\n",
    "    kernel_sizes = [3,4,5],\n",
    "    kernel_num = 100,\n",
    "    embed_dim=128,\n",
    "    dropout=0.5,\n",
    "    cuda = torch.cuda.is_available(),\n",
    "    snapshot = None,\n",
    "    static=False,\n",
    "    predict=None,\n",
    "    test=False,\n",
    "    log_interval=1,\n",
    "    test_interval=100,\n",
    "    early_stop=1000,\n",
    "    save_best=True,\n",
    "    save_interval=500,\n",
    "    save_dir = 'snapshot'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading\n",
      "extracting\n"
     ]
    }
   ],
   "source": [
    "# In this example, words are embedded\n",
    "def mr(text_field, label_field, **kargs):\n",
    "    train_data, dev_data = mydatasets.MR.splits(text_field, label_field)\n",
    "    text_field.build_vocab(train_data, dev_data)\n",
    "    label_field.build_vocab(train_data, dev_data)\n",
    "    train_iter, dev_iter = data.Iterator.splits(\n",
    "                                (train_data, dev_data), \n",
    "                                batch_sizes=(args.batch_size, len(dev_data)),\n",
    "                                **kargs)\n",
    "    return train_iter, dev_iter\n",
    "\n",
    "# load data\n",
    "text_field = data.Field(lower=True)\n",
    "label_field = data.Field(sequential=False)\n",
    "# Data passed to model here \n",
    "train_iter, dev_iter = mr(text_field, \n",
    "                          label_field, \n",
    "                          device=torch.device('cuda'), \n",
    "                          repeat=False)\n",
    "\n",
    "# update args and print\n",
    "args.set(\"embed_num\",len(text_field.vocab))\n",
    "args.set(\"class_num\",len(label_field.vocab) - 1)\n",
    "args.set(\"save_dir\", os.path.join(args.save_dir, \n",
    "                                  datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, each character is embedded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-example here (task: infer if positive/negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his',\n",
       " 'best',\n",
       " 'film',\n",
       " 'remains',\n",
       " 'his',\n",
       " 'shortest',\n",
       " ',',\n",
       " 'the',\n",
       " 'hole',\n",
       " ',',\n",
       " 'which',\n",
       " 'makes',\n",
       " 'many',\n",
       " 'of',\n",
       " 'the',\n",
       " 'points',\n",
       " 'that',\n",
       " 'this',\n",
       " 'film',\n",
       " 'does',\n",
       " 'but',\n",
       " 'feels',\n",
       " 'less',\n",
       " 'repetitive',\n",
       " '']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter.dataset.examples[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'lr': 0.001,\n",
       " 'epochs': 10,\n",
       " 'kernel_sizes': [3, 4, 5],\n",
       " 'kernel_num': 100,\n",
       " 'embed_dim': 128,\n",
       " 'dropout': 0.5,\n",
       " 'cuda': True,\n",
       " 'snapshot': None,\n",
       " 'static': False,\n",
       " 'predict': None,\n",
       " 'test': False,\n",
       " 'log_interval': 1,\n",
       " 'test_interval': 100,\n",
       " 'early_stop': 1000,\n",
       " 'save_best': True,\n",
       " 'save_interval': 500,\n",
       " 'save_dir': 'snapshot/2019-06-05_15-00-47',\n",
       " 'embed_num': 21114,\n",
       " 'class_num': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "cnn = CNN_Text(args)\n",
    "if args.cuda:\n",
    "    cnn = cnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch[100] - loss: 0.694505  acc: 57.0000%(37/64)\n",
      "Evaluation - loss: 0.670083  acc: 55.0000%(593/1066) \n",
      "\n",
      "Batch[101] - loss: 0.678370  acc: 59.0000%(38/64)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch[200] - loss: 0.465207  acc: 87.0000%(56/64)\n",
      "Evaluation - loss: 0.618228  acc: 64.0000%(689/1066) \n",
      "\n",
      "Batch[300] - loss: 0.485058  acc: 78.0000%(47/60)\n",
      "Evaluation - loss: 0.621354  acc: 65.0000%(702/1066) \n",
      "\n",
      "Batch[400] - loss: 0.305611  acc: 90.0000%(58/64)\n",
      "Evaluation - loss: 0.599315  acc: 69.0000%(740/1066) \n",
      "\n",
      "Batch[500] - loss: 0.102855  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.618641  acc: 68.0000%(725/1066) \n",
      "\n",
      "Batch[600] - loss: 0.068792  acc: 100.0000%(60/60)\n",
      "Evaluation - loss: 0.625644  acc: 69.0000%(741/1066) \n",
      "\n",
      "Batch[700] - loss: 0.045885  acc: 98.0000%(63/64)\n",
      "Evaluation - loss: 0.668458  acc: 69.0000%(743/1066) \n",
      "\n",
      "Batch[800] - loss: 0.012878  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.686670  acc: 70.0000%(754/1066) \n",
      "\n",
      "Batch[900] - loss: 0.014656  acc: 100.0000%(60/60)\n",
      "Evaluation - loss: 0.712205  acc: 70.0000%(749/1066) \n",
      "\n",
      "Batch[1000] - loss: 0.008831  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.725757  acc: 70.0000%(756/1066) \n",
      "\n",
      "Batch[1100] - loss: 0.004772  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.751856  acc: 70.0000%(747/1066) \n",
      "\n",
      "Batch[1200] - loss: 0.005250  acc: 100.0000%(60/60)\n",
      "Evaluation - loss: 0.766666  acc: 70.0000%(751/1066) \n",
      "\n",
      "Batch[1300] - loss: 0.003259  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.779324  acc: 70.0000%(753/1066) \n",
      "\n",
      "Batch[1400] - loss: 0.003238  acc: 100.0000%(64/64)\n",
      "Evaluation - loss: 0.792569  acc: 70.0000%(753/1066) \n",
      "\n",
      "Batch[1500] - loss: 0.002870  acc: 100.0000%(60/60)\n",
      "Evaluation - loss: 0.801711  acc: 70.0000%(750/1066) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train or predict\n",
    "if args.predict is not None:\n",
    "    label = train.predict(args.predict, cnn, text_field, label_field, args.cuda)\n",
    "    print('\\n[Text]  {}\\n[Label] {}\\n'.format(args.predict, label))\n",
    "elif args.test:\n",
    "    try:\n",
    "        train.eval(test_iter, cnn, args) \n",
    "    except Exception as e:\n",
    "        print(\"\\nSorry. The test dataset doesn't exist.\\n\")\n",
    "else:\n",
    "    print()\n",
    "    try:\n",
    "        train.train(train_iter, dev_iter, cnn, args)\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\n' + '-' * 89)\n",
    "        print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Character-level Convolutional Networks for Text Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haberrspd.pdnet_flair import Config, make_train_test_dev\n",
    "from haberrspd.charCNN.auxiliary import make_MJFF_data_loader\n",
    "from haberrspd.charCNN.train import run\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For deploy version, all of this will be placed in the parser\n",
    "args = Config(\n",
    "    # Model arguments\n",
    "    val_path='haberrspd/charCNN/data/dev.csv',\n",
    "    test_path='haberrspd/charCNN/data/test.csv',\n",
    "    train_path='haberrspd/charCNN/data/train.csv',\n",
    "    alphabet_path='haberrspd/charCNN/alphabet.json',# Where our alphabet lives\n",
    "    load_very_long_sentences=False, # Temporary fix to remove unreasonably long sequences\n",
    "    max_sample_length=8192, # The maximum length allowed for each training example\n",
    "    num_workers=1,\n",
    "    batch_size = 32,\n",
    "    lr=0.001, # Learning rate\n",
    "    epochs=5, # Set epochs here\n",
    "    max_norm=400,\n",
    "    optimizer='Adam',\n",
    "    class_weight=None,\n",
    "    dynamic_lr=False, # Dynamic learning rate, used for all but Adam\n",
    "    milestones = [5,10,15],\n",
    "    decay_factor = 0.5, # Rate by which we reduce the learning rate\n",
    "    kernel_sizes = [3,4,5],\n",
    "    kernel_num = 100,\n",
    "    embed_dim=128,\n",
    "    shuffle=False,\n",
    "    dropout=0.5,\n",
    "    cuda=True, # We have a GPU so let's use it\n",
    "    verbose=False,\n",
    "    continue_from='', # If we already trained a model we can continue from the stored one\n",
    "    checkpoint=False,\n",
    "    checkpoint_per_batch=10000,\n",
    "    save_folder='haberrspd/charCNN/run_results',\n",
    "    log_config=False,\n",
    "    log_result=False,\n",
    "    log_interval=2,\n",
    "    val_interval=200,\n",
    "    save_interval=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from haberrspd/charCNN/data/train.csv\n",
      "Loading data from haberrspd/charCNN/data/dev.csv\n",
      "\n",
      "Number of training samples: 834\n",
      "\tLabel 0:           583\n",
      "\tLabel 1:           251\n",
      "\n",
      "Number of developing samples: 110\n",
      "\tLabel 0:            67\n",
      "\tLabel 1:            43\n",
      "\n",
      " Directory for saving results, already exists.\n",
      "\n",
      "Configuration:\n",
      "\tAlphabet path:          haberrspd/charCNN/alphabet.json\n",
      "\tBatch size:             32\n",
      "\tCheckpoint:             False\n",
      "\tCheckpoint per batch:   10000\n",
      "\tClass weight:           None\n",
      "\tContinue from:          \n",
      "\tCuda:                   True\n",
      "\tDecay factor:           0.5\n",
      "\tDropout:                0.5\n",
      "\tDynamic lr:             False\n",
      "\tEmbed dim:              128\n",
      "\tEpochs:                 5\n",
      "\tKernel num:             100\n",
      "\tKernel sizes:           [3, 4, 5]\n",
      "\tLoad very long sentences:False\n",
      "\tLog config:             False\n",
      "\tLog interval:           1\n",
      "\tLog result:             False\n",
      "\tLr:                     0.001\n",
      "\tMax norm:               400\n",
      "\tMax sample length:      8192\n",
      "\tMilestones:             [5, 10, 15]\n",
      "\tNum features:           70\n",
      "\tNum workers:            1\n",
      "\tOptimizer:              Adam\n",
      "\tSave folder:            haberrspd/charCNN/run_results\n",
      "\tSave interval:          1\n",
      "\tShuffle:                False\n",
      "\tTest path:              haberrspd/charCNN/data/test.csv\n",
      "\tTrain path:             haberrspd/charCNN/data/train.csv\n",
      "\tVal interval:           200\n",
      "\tVal path:               haberrspd/charCNN/data/dev.csv\n",
      "\tVerbose:                False\n",
      "CharCNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv1d(70, 64, kernel_size=(16,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=8, stride=8, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv1d(64, 64, kernel_size=(16,), stride=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=65408, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5)\n",
      "  )\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (log_softmax): LogSoftmax()\n",
      ")\n",
      "\n",
      "Number of trainable model parameters: 33627778\n",
      "\n",
      "Epoch[1] Batch[1] - loss: 0.704848  lr: 0.00100  acc: 28.000% (9/32)\n",
      "Epoch[1] Batch[2] - loss: 2.021280  lr: 0.00100  acc: 62.000% (20/32)\n",
      "Epoch[1] Batch[3] - loss: 0.590399  lr: 0.00100  acc: 71.000% (23/32)\n",
      "Epoch[1] Batch[4] - loss: 0.784711  lr: 0.00100  acc: 40.000% (13/32)\n",
      "Epoch[1] Batch[5] - loss: 0.685432  lr: 0.00100  acc: 50.000% (16/32)\n",
      "Epoch[1] Batch[6] - loss: 0.629689  lr: 0.00100  acc: 65.000% (21/32)\n",
      "Epoch[1] Batch[7] - loss: 0.580474  lr: 0.00100  acc: 75.000% (24/32)\n",
      "Epoch[1] Batch[8] - loss: 0.648839  lr: 0.00100  acc: 68.000% (22/32)\n",
      "Epoch[1] Batch[9] - loss: 0.447860  lr: 0.00100  acc: 81.000% (26/32)\n",
      "Epoch[1] Batch[10] - loss: 0.734397  lr: 0.00100  acc: 59.000% (19/32)\n",
      "Epoch[1] Batch[11] - loss: 0.596693  lr: 0.00100  acc: 71.000% (23/32)\n",
      "Epoch[1] Batch[12] - loss: 0.398192  lr: 0.00100  acc: 84.000% (27/32)\n",
      "Epoch[1] Batch[13] - loss: 0.652447  lr: 0.00100  acc: 65.000% (21/32)\n",
      "Epoch[1] Batch[14] - loss: 0.565360  lr: 0.00100  acc: 68.000% (22/32)\n",
      "Epoch[1] Batch[15] - loss: 0.551698  lr: 0.00100  acc: 68.000% (22/32)\n",
      "Epoch[1] Batch[16] - loss: 0.557877  lr: 0.00100  acc: 78.000% (25/32)\n",
      "Epoch[1] Batch[17] - loss: 0.632253  lr: 0.00100  acc: 71.000% (23/32)\n",
      "Epoch[1] Batch[18] - loss: 0.650915  lr: 0.00100  acc: 65.000% (21/32)\n",
      "Epoch[1] Batch[19] - loss: 0.597049  lr: 0.00100  acc: 75.000% (24/32)\n",
      "Epoch[1] Batch[20] - loss: 0.633379  lr: 0.00100  acc: 62.000% (20/32)\n",
      "Epoch[1] Batch[21] - loss: 0.571602  lr: 0.00100  acc: 65.000% (21/32)\n",
      "Epoch[1] Batch[22] - loss: 0.608563  lr: 0.00100  acc: 75.000% (24/32)\n",
      "Epoch[1] Batch[23] - loss: 0.596363  lr: 0.00100  acc: 71.000% (23/32)\n",
      "Epoch[1] Batch[24] - loss: 0.607241  lr: 0.00100  acc: 65.000% (21/32)\n",
      "Epoch[1] Batch[25] - loss: 0.512789  lr: 0.00100  acc: 81.000% (26/32)\n",
      "Epoch[1] Batch[26] - loss: 0.502281  lr: 0.00100  acc: 81.000% (26/32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neil/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 0.603630  lr: 0.00100  acc: 67.000% (65/96) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 66.3\u001b[0m% (57/86)      Recall: \u001b[32m 96.6\u001b[0m% (57/59)      F-Score: \u001b[32m 78.6\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 80.0\u001b[0m% (8/10)       Recall: \u001b[32m 21.6\u001b[0m% (8/37)       F-Score: \u001b[32m 34.0\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "=> found better validated model, saving to haberrspd/charCNN/run_results/CharCNN_best.pth.tar\n",
      "\n",
      "\n",
      "Epoch[2] Batch[1] - loss: 0.557843  lr: 0.00100  acc: 81.000% (26/32)\n",
      "Epoch[2] Batch[2] - loss: 0.469927  lr: 0.00100  acc: 81.000% (26/32)\n",
      "Epoch[2] Batch[3] - loss: 0.519208  lr: 0.00100  acc: 68.000% (22/32)\n",
      "Epoch[2] Batch[4] - loss: 0.478063  lr: 0.00100  acc: 78.000% (25/32)\n",
      "Epoch[2] Batch[5] - loss: 0.482974  lr: 0.00100  acc: 78.000% (25/32)\n",
      "Epoch[2] Batch[6] - loss: 0.478311  lr: 0.00100  acc: 81.000% (26/32)\n",
      "Epoch[2] Batch[7] - loss: 0.430139  lr: 0.00100  acc: 84.000% (27/32)\n",
      "Epoch[2] Batch[8] - loss: 0.405333  lr: 0.00100  acc: 84.000% (27/32)\n",
      "Epoch[2] Batch[9] - loss: 0.394905  lr: 0.00100  acc: 78.000% (25/32)\n",
      "Epoch[2] Batch[10] - loss: 0.383437  lr: 0.00100  acc: 84.000% (27/32)\n",
      "Epoch[2] Batch[11] - loss: 0.584459  lr: 0.00100  acc: 71.000% (23/32)\n",
      "Epoch[2] Batch[12] - loss: 0.596472  lr: 0.00100  acc: 75.000% (24/32)\n",
      "Epoch[2] Batch[13] - loss: 0.420737  lr: 0.00100  acc: 78.000% (25/32)\n",
      "Epoch[2] Batch[14] - loss: 0.545849  lr: 0.00100  acc: 68.000% (22/32)\n",
      "Epoch[2] Batch[15] - loss: 0.618076  lr: 0.00100  acc: 71.000% (23/32)\n",
      "Epoch[2] Batch[16] - loss: 0.504759  lr: 0.00100  acc: 75.000% (24/32)\n",
      "Epoch[2] Batch[17] - loss: 0.494358  lr: 0.00100  acc: 75.000% (24/32)\n",
      "Epoch[2] Batch[18] - loss: 0.431743  lr: 0.00100  acc: 75.000% (24/32)\n",
      "Epoch[2] Batch[19] - loss: 0.618818  lr: 0.00100  acc: 65.000% (21/32)\n",
      "Epoch[2] Batch[20] - loss: 0.598773  lr: 0.00100  acc: 62.000% (20/32)\n",
      "Epoch[2] Batch[21] - loss: 0.563627  lr: 0.00100  acc: 62.000% (20/32)\n",
      "Epoch[2] Batch[22] - loss: 0.455011  lr: 0.00100  acc: 78.000% (25/32)\n",
      "Epoch[2] Batch[23] - loss: 0.535035  lr: 0.00100  acc: 78.000% (25/32)\n",
      "Epoch[2] Batch[24] - loss: 0.512412  lr: 0.00100  acc: 68.000% (22/32)\n",
      "Epoch[2] Batch[25] - loss: 0.530390  lr: 0.00100  acc: 71.000% (23/32)\n",
      "Epoch[2] Batch[26] - loss: 0.470671  lr: 0.00100  acc: 78.000% (25/32)\n",
      "\n",
      "Evaluation - loss: 0.627396  lr: 0.00100  acc: 66.000% (64/96) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 66.7\u001b[0m% (56/84)      Recall: \u001b[32m 93.3\u001b[0m% (56/60)      F-Score: \u001b[32m 77.8\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 66.7\u001b[0m% (8/12)       Recall: \u001b[32m 22.2\u001b[0m% (8/36)       F-Score: \u001b[32m 33.3\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[3] Batch[1] - loss: 0.400485  lr: 0.00100  acc: 81.000% (26/32)\n",
      "Epoch[3] Batch[2] - loss: 0.355425  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[3] Batch[3] - loss: 0.276528  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[3] Batch[4] - loss: 0.392639  lr: 0.00100  acc: 78.000% (25/32)\n",
      "Epoch[3] Batch[5] - loss: 0.366624  lr: 0.00100  acc: 84.000% (27/32)\n",
      "Epoch[3] Batch[6] - loss: 0.238834  lr: 0.00100  acc: 93.000% (30/32)\n",
      "Epoch[3] Batch[7] - loss: 0.364770  lr: 0.00100  acc: 84.000% (27/32)\n",
      "Epoch[3] Batch[8] - loss: 0.458929  lr: 0.00100  acc: 71.000% (23/32)\n",
      "Epoch[3] Batch[9] - loss: 0.287049  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[3] Batch[10] - loss: 0.396410  lr: 0.00100  acc: 81.000% (26/32)\n",
      "Epoch[3] Batch[11] - loss: 0.343896  lr: 0.00100  acc: 81.000% (26/32)\n",
      "Epoch[3] Batch[12] - loss: 0.262069  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[3] Batch[13] - loss: 0.294199  lr: 0.00100  acc: 84.000% (27/32)\n",
      "Epoch[3] Batch[14] - loss: 0.364557  lr: 0.00100  acc: 78.000% (25/32)\n",
      "Epoch[3] Batch[15] - loss: 0.309585  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[3] Batch[16] - loss: 0.246593  lr: 0.00100  acc: 93.000% (30/32)\n",
      "Epoch[3] Batch[17] - loss: 0.367808  lr: 0.00100  acc: 87.000% (28/32)\n",
      "Epoch[3] Batch[18] - loss: 0.397669  lr: 0.00100  acc: 78.000% (25/32)\n",
      "Epoch[3] Batch[19] - loss: 0.316847  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[3] Batch[20] - loss: 0.340578  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[3] Batch[21] - loss: 0.213635  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[3] Batch[22] - loss: 0.237093  lr: 0.00100  acc: 84.000% (27/32)\n",
      "Epoch[3] Batch[23] - loss: 0.516450  lr: 0.00100  acc: 68.000% (22/32)\n",
      "Epoch[3] Batch[24] - loss: 0.293993  lr: 0.00100  acc: 81.000% (26/32)\n",
      "Epoch[3] Batch[25] - loss: 0.327387  lr: 0.00100  acc: 81.000% (26/32)\n",
      "Epoch[3] Batch[26] - loss: 0.339911  lr: 0.00100  acc: 87.000% (28/32)\n",
      "\n",
      "Evaluation - loss: 0.695080  lr: 0.00100  acc: 66.000% (64/96) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 69.0\u001b[0m% (49/71)      Recall: \u001b[32m 83.1\u001b[0m% (49/59)      F-Score: \u001b[32m 75.4\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 60.0\u001b[0m% (15/25)      Recall: \u001b[32m 40.5\u001b[0m% (15/37)      F-Score: \u001b[32m 48.4\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[4] Batch[1] - loss: 0.135531  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[4] Batch[2] - loss: 0.202633  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[4] Batch[3] - loss: 0.166117  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[4] Batch[4] - loss: 0.164717  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[4] Batch[5] - loss: 0.186332  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[4] Batch[6] - loss: 0.110701  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[4] Batch[7] - loss: 0.150113  lr: 0.00100  acc: 93.000% (30/32)\n",
      "Epoch[4] Batch[8] - loss: 0.211546  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[4] Batch[9] - loss: 0.143680  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[4] Batch[10] - loss: 0.089230  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[4] Batch[11] - loss: 0.062363  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[4] Batch[12] - loss: 0.153783  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[4] Batch[13] - loss: 0.142712  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[4] Batch[14] - loss: 0.137755  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[4] Batch[15] - loss: 0.093138  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[4] Batch[16] - loss: 0.162789  lr: 0.00100  acc: 93.000% (30/32)\n",
      "Epoch[4] Batch[17] - loss: 0.100325  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[4] Batch[18] - loss: 0.248612  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[4] Batch[19] - loss: 0.206377  lr: 0.00100  acc: 93.000% (30/32)\n",
      "Epoch[4] Batch[20] - loss: 0.158627  lr: 0.00100  acc: 93.000% (30/32)\n",
      "Epoch[4] Batch[21] - loss: 0.101339  lr: 0.00100  acc: 93.000% (30/32)\n",
      "Epoch[4] Batch[22] - loss: 0.171101  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[4] Batch[23] - loss: 0.209668  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[4] Batch[24] - loss: 0.239205  lr: 0.00100  acc: 90.000% (29/32)\n",
      "Epoch[4] Batch[25] - loss: 0.161634  lr: 0.00100  acc: 93.000% (30/32)\n",
      "Epoch[4] Batch[26] - loss: 0.075123  lr: 0.00100  acc: 96.000% (31/32)\n",
      "\n",
      "Evaluation - loss: 0.879626  lr: 0.00100  acc: 63.000% (61/96) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 66.7\u001b[0m% (50/75)      Recall: \u001b[32m 83.3\u001b[0m% (50/60)      F-Score: \u001b[32m 74.1\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 52.4\u001b[0m% (11/21)      Recall: \u001b[32m 30.6\u001b[0m% (11/36)      F-Score: \u001b[32m 38.6\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch[5] Batch[1] - loss: 0.075914  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[5] Batch[2] - loss: 0.067007  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[5] Batch[3] - loss: 0.076838  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[4] - loss: 0.072774  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[5] Batch[5] - loss: 0.041457  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[6] - loss: 0.087854  lr: 0.00100  acc: 93.000% (30/32)\n",
      "Epoch[5] Batch[7] - loss: 0.020545  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[8] - loss: 0.034275  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[9] - loss: 0.037103  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[10] - loss: 0.017050  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[11] - loss: 0.039084  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[12] - loss: 0.013983  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[13] - loss: 0.051652  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[5] Batch[14] - loss: 0.069737  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[15] - loss: 0.025975  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[16] - loss: 0.009477  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[17] - loss: 0.011222  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[18] - loss: 0.046411  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[19] - loss: 0.185964  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[5] Batch[20] - loss: 0.030523  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[21] - loss: 0.093279  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[5] Batch[22] - loss: 0.042515  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[23] - loss: 0.079614  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[5] Batch[24] - loss: 0.058853  lr: 0.00100  acc: 96.000% (31/32)\n",
      "Epoch[5] Batch[25] - loss: 0.021302  lr: 0.00100  acc: 100.000% (32/32)\n",
      "Epoch[5] Batch[26] - loss: 0.050371  lr: 0.00100  acc: 96.000% (31/32)\n",
      "\n",
      "Evaluation - loss: 1.174019  lr: 0.00100  acc: 65.000% (63/96) \n",
      "Label: \u001b[31m  0\u001b[0m\tPrec: \u001b[32m 69.9\u001b[0m% (51/73)      Recall: \u001b[32m 82.3\u001b[0m% (51/62)      F-Score: \u001b[32m 75.6\u001b[0m%\u001b[0m\n",
      "Label: \u001b[31m  1\u001b[0m\tPrec: \u001b[32m 52.2\u001b[0m% (12/23)      Recall: \u001b[32m 35.3\u001b[0m% (12/34)      F-Score: \u001b[32m 42.1\u001b[0m%\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.is_available() is True\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8192, 64, 11])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 70\n",
    "x = torch.ones(2**13, batch_size, 50) # Input    \n",
    "m = nn.Sequential(\n",
    "    nn.Conv1d(batch_size, 64, kernel_size=16, stride=1), # Function\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=3,stride=3)\n",
    ")\n",
    "out = m(x)\n",
    "print(out.size())\n",
    "# print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8192, 448])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.view(out.size(0),-1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
